{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6e25f62-77d7-4e0d-aa2b-5f5347596c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu117'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn ## nn stand for nerual network\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Check PyTorch version\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a17874a-3831-407c-b510-f225275f6832",
   "metadata": {},
   "source": [
    "## 1. Data (preparing and loading)\n",
    "\n",
    "Data can be almost anything ... in machine learning;\n",
    "* Excel speadsheet\n",
    "* Image of any kind \n",
    "* Videos \n",
    "* Audio like songs or podcasts\n",
    "* DNA\n",
    "* Text\n",
    "\n",
    "Machione learning is a game of two parts: \n",
    "1. Get data into a numberical representation ---> Convert data into a numerical\n",
    "2. Build a model to learn patterns in that numerical representation. ---> use the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9794bd-5975-4ed3-ab3b-9cb4db966354",
   "metadata": {},
   "source": [
    "### Linear regression Formula.\n",
    "to showcase this , lets's create some *known* data using the linear regression formula.\n",
    "\n",
    "we'll use a linear regression formula to make a straight line with known parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "398a7bce-51d1-46a9-beab-f5fe164e9962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0000],\n",
       "         [0.0200],\n",
       "         [0.0400],\n",
       "         [0.0600],\n",
       "         [0.0800],\n",
       "         [0.1000],\n",
       "         [0.1200],\n",
       "         [0.1400],\n",
       "         [0.1600],\n",
       "         [0.1800]]),\n",
       " tensor([[0.3000],\n",
       "         [0.3140],\n",
       "         [0.3280],\n",
       "         [0.3420],\n",
       "         [0.3560],\n",
       "         [0.3700],\n",
       "         [0.3840],\n",
       "         [0.3980],\n",
       "         [0.4120],\n",
       "         [0.4260]]),\n",
       " 50,\n",
       " 50)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create *known* parameter s\n",
    "weight = 0.7 \n",
    "bias = 0.3\n",
    "\n",
    "#Create \n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02 \n",
    "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
    "y = weight * X + bias\n",
    "# Commonly upper case in the machine learning represent a matrix or  a tensor, and lowwer case letter will represent a vector\n",
    "X[:10] , y[:10], len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ba9a9-1442-4a08-85fa-ca625dc036a8",
   "metadata": {},
   "source": [
    "# split date in to different set\n",
    "1. Training set\n",
    "2. Testing set\n",
    "3. Validation set [Optional]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cf9d834-8195-43f6-80f6-619efc068325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 40, 10, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a train/ test split\n",
    "train_split_len = int(0.8 * len(X))\n",
    "X_train, y_train = X[:train_split_len], y[:train_split_len]\n",
    "X_test, y_test = X[train_split_len:], y[train_split_len:]\n",
    "len(X_train), len(y_train), len(X_test), len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3238927d-a0c9-4f9e-b9df-060877091f7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "want_eight = 8\n",
    "test_arr = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "split_one = test_arr[:want_eight]\n",
    "split_two = test_arr[want_eight:]\n",
    "\n",
    "split_one, split_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "390e0fa3-d074-4406-84be-da5b3fa91d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(train_data = X_train, \n",
    "                    train_labels = y_train,\n",
    "                    test_data = X_test,\n",
    "                    test_labels = y_test,\n",
    "                    predictions=None):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    #plot training dat in blue\n",
    "    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
    "    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
    "\n",
    "    #Are there predictionsï¼Ÿ\n",
    "    if predictions is not None:\n",
    "        #plot the predictions if they exist\n",
    "        plt.scatter(test_data, predictions, c=\"r\", s=4, label =\"Predictions\")\n",
    "    plt.legend(prop={\"size\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b6e5a9-708b-4e76-bc79-ad0dbd110eee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "380a6f34-6b6c-4150-9b1e-5f27b7366681",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Build model with torch\n",
    "# Create linear regression model class\n",
    "class LinearRegressionModel(nn.Module): # <- () means inherhits from nn.Module\n",
    "    def __init__ (self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize model parameters ( HINT: these could be different layers from torch.nn, single parameters, hard-coded values or functions ) \n",
    "        self.weights = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))  # <-- requires_grad = True mean pytorch will track the gradients of this specific parameter for use with torch.autograd\n",
    "        self.bias = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "\n",
    "    #Forward method to define the computiation in the model [ this method require to be overwrite, and it defines what happens in the forward computiation ]\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the    [Forward defines the computation performed at every call.]\n",
    "        return self.weights * x + self.bias # this is the linear regression "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb90d78e-013b-4c0a-ba99-1550a055ff0d",
   "metadata": {},
   "source": [
    "## what our model does: \n",
    "Start with random values (weight $ bias)\n",
    "Look at training data and adjust the random values to better represent (or get closer to) the ideal values (the weight & bias values we used to create the data)\n",
    "How does it do so ? \n",
    "2 Main AlgorithmsL \n",
    "1. Gradient descent\n",
    "2. Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4fb4e6-0c43-49d0-9028-d10fee7e7353",
   "metadata": {},
   "source": [
    "----------\n",
    "Now we careate a model, see what's inside... \n",
    "\n",
    "So we can check our model parameters or what;s insde our model using `.parameters().`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea9baca0-3b8e-4eab-ba08-001c8c0ad5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.3367], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.1288], requires_grad=True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random seed \n",
    "torch.manual_seed(42)\n",
    "\n",
    "#create an instance of the model (this is a subclass of nn.Module)\n",
    "model_0 = LinearRegressionModel()\n",
    "\n",
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae311195-169c-40a8-b8a2-023b018bfe19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7, 0.3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the target is to make random seed parampter to [grow] to be the orginal weight and bias\n",
    "weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b300bc9-f28e-4cf5-a944-262171d6faab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b68c997-ede1-4b77-ad35-b0adfef3eb92",
   "metadata": {},
   "source": [
    "### Making prediction using `torch.inference_mode()`\n",
    "To Check our model's predictive power, let's see how well it predicts `y_test` base on `X_test`\n",
    "\n",
    "When we Pass the data throught our model it's going to run it through the `forward()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a2c5a94-3d10-4b18-a2af-84f3388a7044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3982],\n",
       "         [0.4049],\n",
       "         [0.4116],\n",
       "         [0.4184],\n",
       "         [0.4251],\n",
       "         [0.4318],\n",
       "         [0.4386],\n",
       "         [0.4453],\n",
       "         [0.4520],\n",
       "         [0.4588]]),\n",
       " tensor([[0.8600],\n",
       "         [0.8740],\n",
       "         [0.8880],\n",
       "         [0.9020],\n",
       "         [0.9160],\n",
       "         [0.9300],\n",
       "         [0.9440],\n",
       "         [0.9580],\n",
       "         [0.9720],\n",
       "         [0.9860]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to direct do the predict without any training?\n",
    "with torch.inference_mode():\n",
    "    y_preds = model_0(X_test)\n",
    "\n",
    "y_preds, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "250023c0-d2d6-4c29-b149-191245ac3334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzoAAAJGCAYAAACTJvC6AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABUIElEQVR4nO3dfVxUdf7//+cwXGkKrpqIyopZWW2mpenalTNFsZsfZ2xrs/qk6JZ9LcsWal2tFK2PUVsZhXbx8aPZxZa2Zc3ZbK2kwbaitdVsu1Ba8zIS1M0GowQdzu+P+TFEgDIIzMzhcb/d5jZxOOfMa/AQPHm/z/tlM03TFAAAAABYSEy4CwAAAACA1kbQAQAAAGA5BB0AAAAAlkPQAQAAAGA5BB0AAAAAlkPQAQAAAGA5BB0AAAAAlhMb7gKao6amRl9//bW6du0qm80W7nIAAAAAhIlpmjpw4ID69OmjmJimx22iIuh8/fXXSktLC3cZAAAAACLErl271K9fvyY/HxVBp2vXrpICbyYpKSnM1QAAAAAIl4qKCqWlpQUzQlOiIujUTldLSkoi6AAAAAA46i0tLEYAAAAAwHIIOgAAAAAsh6ADAAAAwHIIOgAAAAAsh6ADAAAAwHIIOgAAAAAsJyqWl26JQ4cOye/3h7sMICzi4uJkt9vDXQYAAEDYWC7oVFRUaN++faqqqgp3KUDY2Gw2JScnq3fv3kddYx4AAMCKQg4677zzjh544AGtX79eu3fv1iuvvKJx48Yd8ZiioiLl5OTos88+U1pamu666y5NmjSphSU3raKiQqWlperSpYt69uypuLg4fslDh2OapiorK7V371516tRJ3bp1C3dJAAAA7S7koFNZWakhQ4bod7/7nX7zm98cdf9t27ZpzJgxmjp1qv785z+rsLBQ119/vVJTU5WZmdmiopuyb98+denSRf369SPgoEPr1KmTqqqqtGfPHiUnJ/P9AAAAOpyQg86vf/1r/frXv272/k888YQGDBighx56SJJ06qmn6t1339XDDz/cqkHn0KFDqqqqUs+ePfmlDpCUlJSkiooK+f1+xcZabpYqAADAEbX5qmvFxcXKyMioty0zM1PFxcVNHlNVVaWKiop6j6OpXXggLi7u2AoGLKI23Bw+fDjMlQAAALS/Ng86ZWVlSklJqbctJSVFFRUV+uGHHxo9Ji8vT8nJycFHWlpas1+P0RwggO8FAADQkUVkH51Zs2bJ5/MFH7t27Qp3SQAAAACiSJtP3O/du7fKy8vrbSsvL1dSUpI6derU6DEJCQlKSEho69IAAAAAWFSbj+iMGjVKhYWF9ba99dZbGjVqVFu/NNqJzWaTw+E4pnMUFRXJZrNp7ty5rVJTW0tPT1d6enq4ywAAAEATQg463333nTZu3KiNGzdKCiwfvXHjRu3cuVNSYNrZxIkTg/tPnTpVW7du1YwZM7R582Y99thjevHFF5Wdnd067wCSAmEjlAfCz+Fw8G8BAADQRkKeuvbPf/5TTqcz+HFOTo4kKSsrS8uWLdPu3buDoUeSBgwYoFWrVik7O1uPPPKI+vXrp//7v/9r9R46HV1ubm6Dbfn5+fL5fI1+rjVt2rRJnTt3PqZzjBgxQps2bVLPnj1bqSoAAAB0ZDbTNM1wF3E0FRUVSk5Ols/nU1JSUqP7HDx4UNu2bdOAAQOUmJjYzhVGpvT0dO3YsUNR8E8cdWqnrW3fvr3F53A4HFq7dm2b/fvwPQEAAKyoOdlAitBV19B2tm/fLpvNpkmTJmnTpk267LLL1KNHD9lstuAv7a+88oquvvpqnXjiiercubOSk5N1/vnn6+WXX270nI3dozNp0iTZbDZt27ZNjz76qE455RQlJCSof//+mjdvnmpqaurt39Q9OrX3wnz33Xe69dZb1adPHyUkJOiMM87QSy+91OR7HD9+vLp3764uXbpo9OjReueddzR37lzZbDYVFRU1++vl8Xh09tlnq1OnTkpJSdGUKVO0f//+Rvf94osvNGPGDJ111lnq0aOHEhMTdfLJJ2vmzJn67rvvGnzN1q5dG/zv2sekSZOC+yxdulRut1vp6elKTExU9+7dlZmZKa/X2+z6AQAAOirapXdQW7Zs0S9/+UsNHjxYkyZN0n/+8x/Fx8dLCtxnFR8fr/POO0+pqanau3evDMPQFVdcoUcffVS33HJLs1/nD3/4g9auXav/+q//UmZmpl599VXNnTtX1dXVmj9/frPOcejQIV1yySXav3+/Lr/8cn3//fdavny5rrzySq1evVqXXHJJcN/S0lKdc8452r17t371q1/pzDPPVElJiS6++GJdeOGFIX2NnnnmGWVlZSkpKUkTJkxQt27d9NprrykjI0PV1dXBr1etlStXasmSJXI6nXI4HKqpqdEHH3yg+++/X2vXrtU777wTbGibm5urZcuWaceOHfWmFg4dOjT439OmTdOQIUOUkZGh448/XqWlpXr11VeVkZGhlStXyu12h/R+AAAAWsIoMeTd5pVzgFOuQa5wl9N8ZhTw+XymJNPn8zW5zw8//GB+/vnn5g8//NCOlUW2/v37mz/9J962bZspyZRkzpkzp9HjvvzyywbbDhw4YA4ePNhMTk42Kysr631Okjl69Oh627KyskxJ5oABA8yvv/46uH3v3r1mt27dzK5du5pVVVXB7V6v15Rk5ubmNvoe3G53vf3XrFljSjIzMzPr7X/ttdeaksz58+fX275kyZLg+/Z6vY2+7x/z+XxmUlKSedxxx5klJSXB7dXV1eYFF1xgSjL79+9f75ivvvqqXo215s2bZ0oyn3vuuXrbR48e3eDf58e2bt3aYNvXX39t9unTxzzppJOO+h74ngAAAMfKs9ljaq5M+zy7qbkyPZs94S6pWdnANE2TqWsdVO/evXXnnXc2+rkTTjihwbYuXbpo0qRJ8vl8+vDDD5v9OrNnz1Zqamrw4549e8rtduvAgQMqKSlp9nkefvjheiMoF110kfr371+vlqqqKv3lL39Rr169dNttt9U7fvLkyRo0aFCzX+/VV19VRUWFfve73+nkk08Obo+Li2tyJKpv374NRnkk6eabb5YkrVmzptmvLwUW8vip1NRUXX755fr3v/+tHTt2hHQ+AACAUHm3eWW32eU3/bLb7CraXhTukpqNoNNChiFlZweeo9GQIUMa/aVckvbs2aOcnBydeuqp6ty5c/D+kdrw8PXXXzf7dYYNG9ZgW79+/SRJ3377bbPO0a1bt0Z/6e/Xr1+9c5SUlKiqqkrDhw9v0HDWZrPpnHPOaXbdH3/8sSTp/PPPb/C5UaNGKTa24axP0zS1dOlSXXDBBerevbvsdrtsNpt69OghKbSvmyRt3bpVU6ZM0cCBA5WYmBj8dygoKGjR+QAAAELlHOAMhhy/6Zcj3RHukpqNe3RawDAkt1uy26X8fMnjkVxRNF1RklJSUhrd/s033+jss8/Wzp07de655yojI0PdunWT3W7Xxo0b5fF4VFVV1ezXaWwljNqQ4Pf7m3WO5OTkRrfHxsbWW9SgoqJCktSrV69G92/qPTfG5/M1eS673R4MLz82ffp0LVy4UGlpaXK5XEpNTQ0Grnnz5oX0dduyZYtGjBihiooKOZ1OjR07VklJSYqJiVFRUZHWrl0b0vkAAABawjXIJc9VHhVtL5Ij3RFV9+gQdFrA6w2EHL8/8FxUFH1Bp6lGlUuWLNHOnTt1zz336K677qr3ufvuu08ej6c9ymuR2lC1Z8+eRj9fXl7e7HPVhqvGzuX3+/Wf//xHffv2DW7bs2ePFi1apDPOOEPFxcX1+gqVlZVp3rx5zX5tKTBVb//+/Xr22Wd17bXX1vvc1KlTgyu2AQAAtDXXIFdUBZxaTF1rAaezLuT4/dJPVlaOal9++aUkNbqi19///vf2LickgwYNUkJCgtavX99gtMM0TRUXFzf7XEOGDJHU+HsuLi7W4cOH623bunWrTNNURkZGg+apTX3d7Ha7pMZHtpr6dzBNU++9914z3wUAAEDHRdBpAZcrMF1t+vTonLZ2JP3795ckvfvuu/W2P//883r99dfDUVKzJSQk6IorrlB5ebny8/Prfe6ZZ57R5s2bm30ut9utpKQkLV26VF988UVw+6FDhxqMdEl1X7f333+/3nS6r776SrNmzWr0Nbp37y5J2rVrV5Pn++m/w3333adPP/202e8DAACgo2LqWgu5XNYKOLUmTJig+++/X7fccou8Xq/69++vjz/+WIWFhfrNb36jlStXhrvEI8rLy9OaNWs0c+ZMrV27NthH57XXXtOvfvUrrV69WjExR8/3ycnJevTRRzVp0iSdffbZuuqqq5ScnKzXXntNnTp1qreSnFS3GtrLL7+s4cOH66KLLlJ5eblee+01XXTRRcERmh+78MIL9dJLL+nyyy/Xr3/9ayUmJmrIkCEaO3aspk6dqqeeekqXX365rrzySvXo0UMffPCBNmzYoDFjxmjVqlWt9jUDAACwIkZ0UE+/fv20du1aXXTRRVqzZo2efPJJVVdX680339TYsWPDXd5RpaWlqbi4WL/97W/1/vvvKz8/X3v27NGbb76pE088UVLjCyQ0JisrS6+88opOOukkPf3003r66ad17rnnas2aNY2uWLds2TLddttt2r9/vwoKCvTBBx8oJydHzz//fKPnnzJlimbMmKF9+/bp/vvv1+zZs/Xyyy9Lks4880y9+eabOuuss7Ry5UotXbpU3bp103vvvafhw4e38KsDAADQcdhM0zTDXcTRVFRUKDk5WT6fr8lfUg8ePKht27ZpwIABSkxMbOcKEQ3OO+88FRcXy+fzqUuXLuEup83xPQEAAH7MKDHk3eaVc4AzKhcXqNWcbCAxogML2r17d4Ntzz33nN577z1lZGR0iJADAADwY0aJIfdytwrWFci93C2jJEqbQYaAe3RgOaeffrrOPPNMnXbaacH+P0VFReratasefPDBcJcHAADQ7rzbvMGmn3abXUXbi6J6VKc5GNGB5UydOlV79uzRM888o4ULF6qkpETXXHON1q1bp8GDB4e7PAAAgHbnHOAMhhy/6Zcj3RHuktoc9+gAFsX3BAAA+DGjxFDR9iI50h1RPZrT3Ht0mLoGAAAAdACuQa6oDjihYuoaAAAAAMsh6AAAAACwHIIOAAAAAMsh6AAAAACwHIIOAAAAEEWMEkPZq7M7RNPPY0HQAQAAAKKEUWLIvdytgnUFci93E3aOgKADAAAARAnvNm+w6afdZlfR9qJwlxSxCDoAAABAlHAOcAZDjt/0y5HuCHdJEYugg3bhcDhks9nCXUazLFu2TDabTcuWLQt3KQAAAPW4Brnkucqj6SOny3OVp0M1AA0VQccibDZbSI/WNnfuXNlsNhUVFbX6uaNRUVGRbDab5s6dG+5SAACAxbgGubQgcwEh5yhiw10AWkdubm6Dbfn5+fL5fI1+rr0988wz+v7778NdBgAAADoIgo5FNDZysGzZMvl8vogYVfj5z38e7hIAAADQgTB1rQOqrq7WggULdNZZZ+m4445T165ddf7558swGi5P6PP5NGfOHJ122mnq0qWLkpKSdOKJJyorK0s7duyQFLj/Zt68eZIkp9MZnB6Xnp4ePE9j9+j8+F6YN998U+ecc446d+6sHj16KCsrS//5z38arf/JJ5/UL37xCyUmJiotLU0zZszQwYMHZbPZ5HA4mv11+OabbzR16lSlpKSoc+fOOvvss/XKK680uf/SpUvldruVnp6uxMREde/eXZmZmfJ6vfX2mzt3rpxOpyRp3rx59aYMbt++XZL0xRdfaMaMGTrrrLPUo0cPJSYm6uSTT9bMmTP13XffNfs9AAAAoHGM6HQwVVVV+tWvfqWioiINHTpU1113nQ4dOqRVq1bJ7XaroKBAN998syTJNE1lZmbqH//4h84991z96le/UkxMjHbs2CHDMDRhwgT1799fkyZNkiStXbtWWVlZwYDTrVu3ZtVkGIZWrVqlsWPH6pxzztE777yjZ555Rl9++aXefffdevvOmTNH99xzj1JSUjRlyhTFxcXpxRdf1ObNm0P6Onz//fdyOBz65JNPNGrUKI0ePVq7du3S+PHjdckllzR6zLRp0zRkyBBlZGTo+OOPV2lpqV599VVlZGRo5cqVcrvdkgKhbvv27Xr66ac1evToeuGr9muycuVKLVmyRE6nUw6HQzU1Nfrggw90//33a+3atXrnnXcUFxcX0nsCAADAj5hRwOfzmZJMn8/X5D4//PCD+fnnn5s//PBDO1YW2fr372/+9J/4jjvuMCWZs2fPNmtqaoLbKyoqzOHDh5vx8fFmaWmpaZqm+a9//cuUZI4bN67BuQ8ePGgeOHAg+HFubq4pyfR6vY3WMnr06Aa1PPXUU6YkMzY21nz33XeD2w8fPmw6HA5TkllcXBzcXlJSYtrtdrNv375meXl5vdpPO+00U5I5evToo39hflTvlClT6m1fvXq1KcmUZD711FP1Prd169YG5/n666/NPn36mCeddFK97V6v15Rk5ubmNvr6X331lVlVVdVg+7x580xJ5nPPPdes93EkfE8AABC5PJs95u//9nvTs9kT7lKiTnOygWmaJlPXWsgoMZS9OjuqutHW1NTo8ccf18CBA4NTqmp17dpVc+bMUXV1tVauXFnvuE6dOjU4V0JCgrp06dIqdV1zzTU699xzgx/b7XZlZWVJkj788MPg9hdeeEF+v1+33XabevXqVa/2u+66K6TXfOaZZxQfH6+777673vbMzExddNFFjR4zYMCABttSU1N1+eWX69///ndwKl9z9O3bV/Hx8Q22146mrVmzptnnAgAA0cUoMeRe7lbBugK5l7uj6vfJaMLUtRaovTjtNrvy/5EfNWuYl5SUaP/+/erTp0/wnpof27t3ryQFp4GdeuqpOuOMM/TCCy/oq6++0rhx4+RwODR06FDFxLReRh42bFiDbf369ZMkffvtt8FtH3/8sSTpvPPOa7D/j4PS0VRUVGjbtm067bTT1Lt37wafP//881VYWNhg+9atW5WXl6e3335bpaWlqqqqqvf5r7/+Wv37929WDaZp6qmnntKyZcv06aefyufzqaampt65AACANXm3eYMNP+02u4q2F0XF75LRhqDTAtF6cX7zzTeSpM8++0yfffZZk/tVVlZKkmJjY/X2229r7ty5evnll3XbbbdJko4//njdfPPNuvPOO2W324+5rqSkpAbbYmMDl6bf7w9uq6iokKR6ozm1UlJSmv16RzpPU+fasmWLRowYoYqKCjmdTo0dO1ZJSUmKiYlRUVGR1q5d2yD4HMn06dO1cOFCpaWlyeVyKTU1VQkJCZICCxiEci4AABBdnAOcyv9HfvD3SUe6I9wlWRJBpwWi9eKsDRSXX365XnrppWYd06NHDxUUFOjRRx/V5s2b9fbbb6ugoEC5ubmKi4vTrFmz2rLkemrr37NnT4ORk/Ly8hadpzGNnevhhx/W/v379eyzz+raa6+t97mpU6dq7dq1zX79PXv2aNGiRTrjjDNUXFyszp07Bz9XVlbW6GgbAACwDtcglzxXeVS0vUiOdEdU/ME8GnGPTgvUXpzTR06PmmlrUmAqWlJSkv75z3/q0KFDIR1rs9l06qmnatq0aXrrrbckqd5y1LUjOz8egWltQ4YMkSS99957DT73/vvvN/s8SUlJGjBggLZs2aKysrIGn//73//eYNuXX34pScGV1WqZptloPUf6emzdulWmaSojI6NeyGnqtQEAgPW4Brm0IHNB1PweGY0IOi0UjRdnbGysbrzxRu3YsUO33357o2Hn008/DY50bN++Pdj35cdqRzwSExOD27p37y5J2rVrVxtUHnDVVVcpJiZGDz30kPbt2xfcXllZqfnz54d0rgkTJqi6ulpz5sypt/3NN99s9P6c2hGkny53fd999+nTTz9tsP+Rvh6153r//ffr3Zfz1VdftesIGQAAgJUxda2DmTdvnjZs2KBHH31Uq1at0gUXXKBevXqptLRUn3zyiT7++GMVFxerV69e2rhxo37zm99oxIgRwRv3a3vHxMTEKDs7O3je2kahd9xxhz777DMlJyerW7duwVXEWsOgQYM0c+ZM3XvvvRo8eLCuvPJKxcbGauXKlRo8eLA+/fTTZi+SMGPGDK1cuVKLFy/WZ599pgsuuEC7du3Siy++qDFjxmjVqlX19p86daqeeuopXX755bryyivVo0cPffDBB9qwYUOj+59yyinq06ePli9froSEBPXr1082m0233HJLcKW2l19+WcOHD9dFF12k8vJyvfbaa7rooouCo0cAAABoOUZ0OpiEhAT97W9/05NPPqnevXvr5ZdfVn5+vt555x2lpqbq8ccf1+DBgyVJw4cP1x//+EfZbDatWrVKDz30kIqKipSRkaH33ntPLlfdaNZpp52mp556Sj179lRBQYFmz56tBx98sNXrnz9/vh577DH97Gc/0xNPPKEXX3xRV1xxhR577DFJjS9s0JjjjjtOa9eu1Q033KB///vfys/P1+bNm7VixQpdccUVDfY/88wz9eabb+qss87SypUrtXTpUnXr1k3vvfeehg8f3mB/u92ulStX6pe//KVeeOEFzZkzR7Nnz9b+/fslScuWLdNtt92m/fv3q6CgQB988IFycnL0/PPPH8NXBwAAALVspmma4S7iaCoqKpScnCyfz9fkL7IHDx7Utm3bNGDAgHpTqtAxrFmzRhdffLFmzJih+++/P9zlRAS+JwAAgBU1JxtIjOggyuzdu7fBDf7ffvtt8N6WcePGhaEqAADQUUVjE/mOgnt0EFX+/Oc/68EHH9SFF16oPn36aPfu3Vq9erX27NmjSZMmadSoUeEuEQAAdBDR2kS+oyDoIKqcc845GjZsmNasWaNvvvlGdrtdp556qmbPnq2bbrop3OUBAIAOJFqbyHcUBB1ElREjRsjj8YS7DAAAgKhtIt9REHQAAACAFqhtIl+0vUiOdAejORGGoAMAAAC0kGuQi4AToVh1DQAAAIDlEHQAAAAAWA5BBwAAAIDlEHQAAAAAWA5BBwAAAB2eUWIoe3W2jBIj3KWglRB0AAAA0KEZJYbcy90qWFcg93I3YcciCDoAAADo0LzbvMGmn3abXUXbi8JdEloBQQdtbvv27bLZbJo0aVK97Q6HQzabrc1eNz09Xenp6W12fgAAYA3OAc5gyPGbfjnSHeEuCa2AoGMxtaHix4/4+HilpaXpmmuu0b/+9a9wl9hqJk2aJJvNpu3bt4e7FAAAEMVcg1zyXOXR9JHT5bnKQwNQi4gNdwFoGwMHDtS1114rSfruu+/0wQcf6IUXXtDKlStVWFioc889N8wVSs8884y+//77Njt/YWFhm50bAABYi2uQi4BjMQQdizrxxBM1d+7cetvuuusuzZ8/X3feeaeKiorCUteP/fznP2/T8w8cOLBNzw8AAIDIxdS1DuSWW26RJH344YeSJJvNJofDodLSUk2cOFG9e/dWTExMvRD0zjvvaOzYserZs6cSEhJ00kkn6a677mp0JMbv9+v+++/XiSeeqMTERJ144onKy8tTTU1No/Uc6R4dj8ejSy65RD169FBiYqLS09M1YcIEffrpp5IC9988/fTTkqQBAwYEp+k5HI7gOZq6R6eyslK5ubk65ZRTlJiYqO7du2vMmDF67733Guw7d+5c2Ww2FRUV6fnnn9fQoUPVqVMnpaam6tZbb9UPP/zQ4JiXX35Zo0ePVq9evZSYmKg+ffooIyNDL7/8cqPvFQAAAK2PEZ0O6Mfh4j//+Y9GjRql7t2766qrrtLBgweVlJQkSXr88cc1bdo0devWTWPHjlWvXr30z3/+U/Pnz5fX65XX61V8fHzwXDfccIOWLl2qAQMGaNq0aTp48KAWLFig999/P6T6brvtNi1YsEDdu3fXuHHj1KtXL+3atUtr1qzRsGHDdPrpp+v3v/+9li1bpo8//li33nqrunXrJklHXXzg4MGDuvDCC7Vu3TqdddZZ+v3vf6/y8nKtWLFCb7zxhl544QX99re/bXDcwoULtXr1arndbl144YVavXq1Hn30Ue3bt09//vOfg/s9/vjjuummm5SamqrLLrtMPXr0UFlZmdatW6dXXnlFl19+eUhfCwAAALSQ2QILFy40+/fvbyYkJJgjRoww//GPfzS5b3V1tTlv3jzzhBNOMBMSEswzzjjD/Nvf/hbS6/l8PlOS6fP5mtznhx9+MD///HPzhx9+COncVrNt2zZTkpmZmdngc3PmzDElmU6n0zRN05RkSjInT55sHj58uN6+n332mRkbG2sOGTLE3LdvX73P5eXlmZLMBx98MLjN6/WakswhQ4aY3333XXD7V199Zfbs2dOUZGZlZdU7z+jRo82fXoJ//etfTUnm4MGDG7zuoUOHzLKysuDHWVlZpiRz27ZtjX4t+vfvb/bv37/etnnz5pmSzP/+7/82a2pqgts3bNhgxsfHm926dTMrKiqC23Nzc01JZnJysrl58+bg9u+//948+eSTzZiYGLO0tDS4/ayzzjLj4+PN8vLyBvX89P20Nb4nAACAFTUnG5imaYY8dW3FihXKyclRbm6uNmzYoCFDhigzM1N79uxpdP+77rpLTz75pAoKCvT5559r6tSpuuyyy/TRRx+1IJZFEMOQsrMDzxFoy5Ytmjt3rubOnas//OEPuuCCC3T33XcrMTFR8+fPD+4XHx+vP/3pT7Lb7fWOf/LJJ3X48GEVFBSoR48e9T43Y8YMHX/88XrhhReC25555hlJ0pw5c3TccccFt/ft21e33nprs+t+7LHHJEmPPPJIg9eNjY1VSkpKs8/VmKefflpxcXG677776o1snXnmmcrKytK3336rV199tcFxt956qwYNGhT8uFOnTrr66qtVU1Oj9evX19s3Li5OcXFxDc7x0/cDAABal1FiKHt1Ng0/IakFU9cWLFigKVOmaPLkyZKkJ554QqtWrdLSpUs1c+bMBvs/++yzuvPOO3XppZdKkm688UatWbNGDz30kJ577rljLD9MDENyuyW7XcrPlzweyRVZq3R8+eWXmjdvnqTAL94pKSm65pprNHPmTA0ePDi434ABA9SzZ88Gx3/wwQeSpDfeeKPR1cvi4uK0efPm4Mcff/yxJOn8889vsG9j25qybt06JSQkaPTo0c0+prkqKiq0detWnXrqqerXr1+DzzudTi1evFgbN27UhAkT6n1u2LBhDfavPce3334b3HbVVVdpxowZOv3003XNNdfI6XTqvPPOC04HBAAAbcMoMeRe7pbdZlf+P/JZJhqhBZ3q6mqtX79es2bNCm6LiYlRRkaGiouLGz2mqqpKiYmJ9bZ16tRJ7777bpOvU1VVpaqqquDHFRUVoZTZ9rzeQMjx+wPPRUURF3QyMzO1evXqo+7X1AjJN998I0n1Rn+OxOfzKSYmptHQFMoojM/nU9++fRUT0/rrZNReR03Vk5qaWm+/H2ssqMTGBr59/H5/cNvtt9+uHj166PHHH9dDDz2kBx98ULGxsRozZowefvhhDRgw4JjfBwAAaMi7zRts+Gm32VW0vYig08GF9Nvkvn375Pf7G/yimJKSorKyskaPyczM1IIFC/Tvf/9bNTU1euutt7Ry5Urt3r27ydfJy8tTcnJy8JGWlhZKmW3P6awLOX6/9KOVvqJNU6ue1f5iX1FRIdM0m3zUSk5OVk1Njfbt29fgXOXl5c2up1u3biorK2typbZjUfuemqqn9ho+ltEXm82m3/3ud/rwww+1d+9evfLKK/rNb34jj8ej//qv/6oXigAAQOtxDnAGQ47f9MuR7gh3SQizNl9e+pFHHtFJJ52kU045RfHx8br55ps1efLkI/7FftasWfL5fMHHrl272rrM0Lhcgelq06dH5LS11jBy5EhJdVPYjmbIkCGSpL///e8NPtfYtqaMGDFCVVVVWrt27VH3rb2vqLnhISkpSSeccIK2bNmi0tLSBp+vXVZ76NChza73SHr06KFx48ZpxYoVuvDCC/X5559ry5YtrXJuAABQn2uQS56rPJo+cjrT1iApxKDTs2dP2e32Bn8RLy8vV+/evRs95vjjj9err76qyspK7dixQ5s3b1aXLl10wgknNPk6CQkJSkpKqveIOC6XtGCBJUOOJN10002KjY3VLbfcop07dzb4/LfffltvQYnae1ruvvtuVVZWBreXlpbqkUceafbrTps2TVLg5v/a6XO1Dh8+XO/a6969uySFFISzsrJ06NAhzZo1q96I1L/+9S8tW7ZMycnJGjduXLPP91NFRUX1zitJhw4dCr6Xn07jBAAArcc1yKUFmQsIOZAU4j068fHxGjZsmAoLC4O/DNbU1KiwsFA333zzEY9NTExU3759dejQIb388su68sorW1w02t7pp5+uxx57TDfeeKMGDRqkSy+9VAMHDtSBAwe0detWrV27VpMmTdITTzwhKXAj/+TJk/XUU09p8ODBuuyyy1RVVaUVK1bol7/8pV577bVmve6ll16q22+/XQ8++KBOOukkXXbZZerVq5dKS0tVWFio22+/Xb///e8lSRdeeKEefPBB3XDDDbr88st13HHHqX///g0WEvixGTNmaNWqVXr22We1adMmXXTRRdqzZ49WrFihw4cPa/HixeratWuLv27jxo1TUlKSfvnLX6p///46dOiQ3nrrLX3++ee64oor1L9//xafGwAAAM0X8qprOTk5ysrK0vDhwzVixAjl5+ersrIyuArbxIkT1bdvX+Xl5UmS/vGPf6i0tFRDhw5VaWmp5s6dq5qaGs2YMaN13wla3ZQpUzR06FAtWLBA77zzjv76178qOTlZP//5z5Wdna2srKx6+y9evFgnn3yyFi9erIULF6pfv37KycnRlVde2eygI0kPPPCARo0apYULF+qll17SwYMHlZqaqgsvvFAXX3xxcL9f//rX+tOf/qTFixfroYce0qFDhzR69OgjBp3ExES9/fbbuv/++7VixQo9/PDD6ty5s0aPHq077rhD5513XuhfqB/Jy8vT6tWrtW7dOv31r3/Vcccdp4EDB+rxxx/Xddddd0znBgAAQPPZzJ/Os2mGhQsX6oEHHlBZWZmGDh2qRx99NHhPh8PhUHp6upYtWyZJWrt2rW688UZt3bpVXbp00aWXXqr77rtPffr0afbrVVRUKDk5WT6fr8lpbAcPHtS2bds0YMAApgcB4nsCAABYU3OygdTCoNPeCDpA6PieAAAAVtTcoNPmq64BAAAAoTBKDGWvzpZRYoS7FEQxgg4AAAAihlFiyL3crYJ1BXIvdxN20GIEHQAAAEQM7zZvsOmn3WZX0faicJeEKEXQAQAAQMRwDnAGQ47f9MuR7gh3SYhSIS8vDQAAALQV1yCXPFd5VLS9SI50B80/0WKWCzpRsIgc0C74XgAARCvXIBcBB8fMMlPX7Ha7JOnQoUNhrgSIDIcPH5YkxcZa7u8ZAAAAR2WZoBMXF6eEhAT5fD7+kg0osMa83W4P/hEAAACgI7HUn3p79uyp0tJSffXVV0pOTlZcXJxsNlu4ywLalWmaqqysVEVFhVJTU/keAAAAHZKlgk5tZ9R9+/aptLQ0zNUA4WOz2dStWzclJyeHuxQAAICwsFTQkQJhJykpSYcOHZLf7w93OUBYxMXFMWUNABBWRokh7zavnAOcLCyAsLBc0KkVFxenuLi4cJcBAADQ4RglhtzL3bLb7Mr/R748V3kIO2h3llmMAAAAAJHBu80bbPhpt9lVtL0o3CWhAyLoAAAAoFU5BziDIcdv+uVId4S7JHRAlp26BgAAgPBwDXLJc5VHRduL5Eh3MG0NYWEzo6DpTEVFhZKTk+Xz+YIrqwEAAADoeJqbDZi6BgAAAMByCDoAAAAALIegAwAAAMByCDoAAAAALIegAwAAgCYZJYayV2fLKDHCXQoQEoIOAAAAGmWUGHIvd6tgXYHcy92EHUQVgg4AAAAa5d3mDTb9tNvsKtpeFO6SgGYj6AAAAKBRzgHOYMjxm3450h3hLglotthwFwAAAIDI5Brkkucqj4q2F8mR7pBrkCvcJQHNZjNN0wx3EUfT3O6nAAAAAKytudmAqWsAAAAALIegAwAAAMByCDoAAAAALIegAwAAAMByCDoAAAAdgGFI2dmBZ6AjIOgAAABYnGFIbrdUUBB4JuygIyDoAAAAWJzXK9ntkt8feC4qCndFQNsj6AAAAFic01kXcvx+yeEId0VA24sNdwEAAABoWy6X5PEERnIcjsDHgNURdAAAADoAl4uAg46FqWsAAAAALIegAwAAAMByCDoAAAAALIegAwAAAMByCDoAAABRwjCk7GwafgLNQdABAACIAoYhud1SQUHgmbADHBlBBwAAIAp4vXUNP+32QE8cAE0j6AAAAEQBp7Mu5Pj9gcafAJpGw1AAAIAo4HJJHk9gJMfhoPkncDQEHQAAgCjhchFwgOZi6hoAAAAAyyHoAAAAALAcgg4AAAAAyyHoAAAAALAcgg4AAEA7MwwpO5umn0BbIugAAAC0I8OQ3G6poCDwTNgB2gZBBwAAoB15vXVNP+32QF8cAK2PoAMAANCOnM66kOP3B5p/Amh9NAwFAABoRy6X5PEERnIcDhqAAm2FoAMAANDOXC4CDtDWmLoGAAAAwHIIOgAAAAAsh6ADAAAAwHIIOgAAAAAsh6ADAADQQoYhZWfT9BOIRC0KOosWLVJ6eroSExM1cuRIrVu37oj75+fna9CgQerUqZPS0tKUnZ2tgwcPtqhgAACASGAYktstFRQEngk7QGQJOeisWLFCOTk5ys3N1YYNGzRkyBBlZmZqz549je7//PPPa+bMmcrNzdWmTZu0ZMkSrVixQnfccccxFw8AABAuXm9d00+7PdAXB0DkCDnoLFiwQFOmTNHkyZN12mmn6YknnlDnzp21dOnSRvd///33de655+qaa65Renq6LrnkEl199dVHHQUCAACIZE5nXcjx+wPNPwFEjpCCTnV1tdavX6+MjIy6E8TEKCMjQ8XFxY0ec84552j9+vXBYLN161a9/vrruvTSS5t8naqqKlVUVNR7AAAARBKXS/J4pOnTA880AAUiS2woO+/bt09+v18pKSn1tqekpGjz5s2NHnPNNddo3759Ou+882Sapg4fPqypU6cecepaXl6e5s2bF0ppAAAA7c7lIuAAkarNV10rKirSvffeq8cee0wbNmzQypUrtWrVKt1zzz1NHjNr1iz5fL7gY9euXW1dJgAAAAALCWlEp2fPnrLb7SovL6+3vby8XL179270mNmzZ2vChAm6/vrrJUmDBw9WZWWlbrjhBt15552KiWmYtRISEpSQkBBKaQAAAAAQFNKITnx8vIYNG6bCwsLgtpqaGhUWFmrUqFGNHvP99983CDN2u12SZJpmqPUCAAAAwFGFNKIjSTk5OcrKytLw4cM1YsQI5efnq7KyUpMnT5YkTZw4UX379lVeXp4kaezYsVqwYIHOPPNMjRw5Ulu2bNHs2bM1duzYYOABAAAAgNYUctAZP3689u7dqzlz5qisrExDhw7V6tWrgwsU7Ny5s94Izl133SWbzaa77rpLpaWlOv744zV27FjNnz+/9d4FAABACxlGoCeO08nCAoCV2MwomD9WUVGh5ORk+Xw+JSUlhbscAABgEYYhud11vXBYJhqIfM3NBm2+6hoAAECk8nrrQo7dLhUVhbsiAK2FoAMAADosp7Mu5Pj9ksMR7ooAtJaQ79EBAACwCpcrMF2tqCgQcpi2BlgHQQcAAHRoLhcBB7Aipq4BAAAAsByCDgAAAADLIegAAAAAsByCDgAAAADLIegAAABLMAwpOzvwDAAEHQAAEPUMQ3K7pYKCwDNhBwBBBwAARD2vt67pp90e6IsDoGMj6AAAgKjndNaFHL8/0PwTQMdGw1AAABD1XC7J4wmM5DgcNAAFQNABAAAW4XIRcADUYeoaAAAAAMsh6AAAAACwHIIOAAAAAMsh6AAAAACwHIIOAACIGIYhZWfT8BPAsSPoAACAiGAYktstFRQEngk7AI4FQQcAAEQEr7eu4afdHuiJAwAtRdABAAARwemsCzl+f6DxJwC0FA1DAQBARHC5JI8nMJLjcND8E8CxIegAAICI4XIRcAC0DqauAQAAALAcgg4AAAAAyyHoAAAAALAcgg4AAAAAyyHoAACAVmcYUnY2TT8BhA9BBwAAtCrDkNxuqaAg8EzYARAOBB0AANCqvN66pp92e6AvDgC0N4IOAABoVU5nXcjx+wPNPwGgvdEwFAAAtCqXS/J4AiM5DgcNQAGEB0EHAAC0OpeLgAMgvJi6BgAAAMByCDoAAAAALIegAwAAAMByCDoAAAAALIegAwAAmmQYUnY2TT8BRB+CDgAAaJRhSG63VFAQeCbsAIgmBB0AANAor7eu6afdHuiLAwDRgqADAAAa5XTWhRy/P9D8EwCiBQ1DAQBAo1wuyeMJjOQ4HDQABRBdCDoAAKBJLhcBB0B0YuoaAAAAAMsh6AAAAACwHIIOAAAAAMsh6AAAAACwHIIOAAAWZxhSdjYNPwF0LAQdAAAszDAkt1sqKAg8E3YAdBQEHQAALMzrrWv4abcHeuIAQEdA0AEAwMKczrqQ4/cHGn8CQEdAw1AAACzM5ZI8nsBIjsNB808AHQdBBwAAi3O5CDgAOh6mrgEAAACwHIIOAAAAAMsh6AAAAACwHIIOAAAAAMsh6AAAECUMQ8rOpuknADQHQQcAgChgGJLbLRUUBJ4JOwBwZC0KOosWLVJ6eroSExM1cuRIrVu3rsl9HQ6HbDZbg8eYMWNaXDQAAB2N11vX9NNuD/TFAQA0LeSgs2LFCuXk5Cg3N1cbNmzQkCFDlJmZqT179jS6/8qVK7V79+7g49NPP5Xdbtdvf/vbYy4eAICOwumsCzl+f6D5JwCgaTbTNM1QDhg5cqTOPvtsLVy4UJJUU1OjtLQ03XLLLZo5c+ZRj8/Pz9ecOXO0e/duHXfccc16zYqKCiUnJ8vn8ykpKSmUcgEAsAzDCIzkOBw0AAXQcTU3G8SGctLq6mqtX79es2bNCm6LiYlRRkaGiouLm3WOJUuW6KqrrjpiyKmqqlJVVVXw44qKilDKBADAklwuAg4ANFdIU9f27dsnv9+vlJSUettTUlJUVlZ21OPXrVunTz/9VNdff/0R98vLy1NycnLwkZaWFkqZAAAAADq4dl11bcmSJRo8eLBGjBhxxP1mzZoln88XfOzataudKgQAAABgBSFNXevZs6fsdrvKy8vrbS8vL1fv3r2PeGxlZaWWL1+uu++++6ivk5CQoISEhFBKAwAAAICgkEZ04uPjNWzYMBUWFga31dTUqLCwUKNGjTrisX/5y19UVVWla6+9tmWVAgAAAEAzhTx1LScnR4sXL9bTTz+tTZs26cYbb1RlZaUmT54sSZo4cWK9xQpqLVmyROPGjVOPHj2OvWoAAKKYYUjZ2TT9BIC2FNLUNUkaP3689u7dqzlz5qisrExDhw7V6tWrgwsU7Ny5UzEx9fNTSUmJ3n33Xb355putUzUAAFHKMCS3O9APJz9f8nhYSQ0A2kLIfXTCgT46AACryM6WCgrqmn9Ony4tWBDuqgAgejQ3G7TrqmsAAHR0TmddyPH7A80/AQCtL+SpawAAoOVcrsB0taKiQMhh2hoAtA2CDgAA7czlIuAAQFtj6hoAAAAAyyHoAAAAALAcgg4AAAAAyyHoAAAAALAcgg4AAC1gGIGeOIYR7koAAI0h6AAAECLDkNzuQONPt5uwAwCRiKADAECIvN66hp92e6AnDgAgshB0AAAIkdNZF3L8/kDjTwBAZKFhKAAAIXK5JI8nMJLjcND8EwAiEUEHAIAWcLkIOAAQyZi6BgAAAMByCDoAAAAALIegAwAAAMByCDoAAAAALIegAwDo0AxDys6m6ScAWA1BBwDQYRmG5HZLBQWBZ8IOAFgHQQcA0GF5vXVNP+32QF8cAIA1EHQAAB2W01kXcvz+QPNPAIA10DAUANBhuVySxxMYyXE4aAAKAFZC0AEAdGguFwEHAKyIqWsAAAAALIegAwAAAMByCDoAAAAALIegAwAAAMByCDoAgKhnGFJ2Ng0/AQB1CDoAgKhmGJLbLRUUBJ4JOwAAiaADAIhyXm9dw0+7PdATBwAAgg4AIKo5nXUhx+8PNP4EAICGoQCAqOZySR5PYCTH4aD5JwAggKADAIh6LhcBBwBQH1PXAAAAAFgOQQcAAACA5RB0AAAAAFgOQQcAAACA5RB0AAARwzCk7GyafgIAjh1BBwAQEQxDcrulgoLAM2EHAHAsCDoAgIjg9dY1/bTbA31xAABoKYIOACAiOJ11IcfvDzT/BACgpWgYCgCICC6X5PEERnIcDhqAAgCODUEHABAxXC4CDgCgdTB1DQAAAIDlEHQAAAAAWA5BBwAAAIDlEHQAAAAAWA5BBwDQ6gxDys6m6ScAIHwIOgCAVmUYktstFRQEngk7AIBwIOgAAFqV11vX9NNuD/TFAQCgvRF0AACtyumsCzl+f6D5JwAA7Y2GoQCAVuVySR5PYCTH4aABKAAgPAg6AIBW53IRcAAA4cXUNQAAAACWQ9ABAAAAYDkEHQAAAACWQ9ABAAAAYDkEHQBAowxDys6m4ScAIDoRdAAADRiG5HZLBQWBZ8IOACDaEHQAAA14vXUNP+32QE8cAACiCUEHANCA01kXcvz+QONPAACiSYuCzqJFi5Senq7ExESNHDlS69atO+L+3377raZNm6bU1FQlJCTo5JNP1uuvv96iggEAbc/lkjweafr0wDPNPwEA0SY21ANWrFihnJwcPfHEExo5cqTy8/OVmZmpkpIS9erVq8H+1dXVuvjii9WrVy+99NJL6tu3r3bs2KFu3bq1Rv0AgDbichFwAADRy2aaphnKASNHjtTZZ5+thQsXSpJqamqUlpamW265RTNnzmyw/xNPPKEHHnhAmzdvVlxcXLNeo6qqSlVVVcGPKyoqlJaWJp/Pp6SkpFDKBQAAAGAhFRUVSk5OPmo2CGnqWnV1tdavX6+MjIy6E8TEKCMjQ8XFxY0eYxiGRo0apWnTpiklJUWnn3667r33Xvn9/iZfJy8vT8nJycFHWlpaKGUCAAAA6OBCCjr79u2T3+9XSkpKve0pKSkqKytr9JitW7fqpZdekt/v1+uvv67Zs2froYce0v/8z/80+TqzZs2Sz+cLPnbt2hVKmQAAAAA6uJDv0QlVTU2NevXqpf/93/+V3W7XsGHDVFpaqgceeEC5ubmNHpOQkKCEhIS2Lg0AAACARYUUdHr27Cm73a7y8vJ628vLy9W7d+9Gj0lNTVVcXJzsdntw26mnnqqysjJVV1crPj6+BWUDAJrLMAJ9cZxOFhcAAHQcIU1di4+P17Bhw1RYWBjcVlNTo8LCQo0aNarRY84991xt2bJFNTU1wW1ffPGFUlNTCTkA0MYMQ3K7pYKCwLNhhLsiAADaR8h9dHJycrR48WI9/fTT2rRpk2688UZVVlZq8uTJkqSJEydq1qxZwf1vvPFGffPNN7r11lv1xRdfaNWqVbr33ns1bdq01nsXAIBGeb11TT/tdqmoKNwVAQDQPkK+R2f8+PHau3ev5syZo7KyMg0dOlSrV68OLlCwc+dOxcTU5ae0tDS98cYbys7O1hlnnKG+ffvq1ltv1R//+MfWexcAgEY5nVJ+fl3YcTjCXREAAO0j5D464dDctbIBAA0ZRmAkx+HgHh0AQPRrbjZo81XXAADh5XIRcAAAHU/I9+gAAAAAQKQj6AAAAACwHIIOAAAAAMsh6AAAAACwHIIOAEQJw5Cys2n6CQBAcxB0ACAKGIbkdksFBYFnwg4AAEdG0AGAKOD11jX9tNsDfXEAAEDTCDoAEAWczrqQ4/cHmn8CAICm0TAUAKKAyyV5PIGRHIeDBqAAABwNQQcAooTLRcABAKC5mLoGAAAAwHIIOgAAAAAsh6ADAAAAwHIIOgAAAAAsh6ADAO3IMKTsbBp+AgDQ1gg6ANBODENyu6WCgsAzYQcAgLZD0AGAduL11jX8tNsDPXEAAEDbIOgAQDtxOutCjt8faPwJAADaBg1DAaCduFySxxMYyXE4aP4JAEBbIugAQDtyuQg4AAC0B6auAQAAALAcgg4AAAAAyyHoAAAAALAcgg4AAAAAyyHoAEALGIaUnU3TTwAAIhVBBwBCZBiS2y0VFASeCTsAAEQegg4AhMjrrWv6abcH+uIAAIDIQtABgBA5nXUhx+8PNP8EAACRhYahABAil0vyeAIjOQ4HDUABAIhEBB0AaAGXi4ADAEAkY+oaAAAAAMsh6AAAAACwHIIOAAAAAMsh6AAAAACwHIIOgA7LMKTsbBp+AgBgRQQdAB2SYUhut1RQEHgm7AAAYC0EHQAdktdb1/DTbg/0xAEAANZB0AHQITmddSHH7w80/gQAANZBw1AAHZLLJXk8gZEch4PmnwAAWA1BB0CH5XIRcAAAsCqmrgEAAACwHIIOAAAAAMsh6AAAAACwHIIOAAAAAMsh6ACIeoYhZWfT9BMAANQh6ACIaoYhud1SQUHgmbADAAAkgg6AKOf11jX9tNsDfXEAAAAIOgCimtNZF3L8/kDzTwAAABqGAohqLpfk8QRGchwOGoACAIAAgg6AqOdyEXAAAEB9TF0DAAAAYDkEHQAAAACWQ9ABAAAAYDkEHQAAAACWQ9ABEDEMQ8rOpuknAAA4dgQdABHBMCS3WyooCDwTdgAAwLEg6ACICF5vXdNPuz3QFwcAAKClCDoAIoLTWRdy/P5A808AAICWomEogIjgckkeT2Akx+GgASgAADg2LRrRWbRokdLT05WYmKiRI0dq3bp1Te67bNky2Wy2eo/ExMQWFwzAulwuacECQg4AADh2IQedFStWKCcnR7m5udqwYYOGDBmizMxM7dmzp8ljkpKStHv37uBjx44dx1Q0AAAAABxJyEFnwYIFmjJliiZPnqzTTjtNTzzxhDp37qylS5c2eYzNZlPv3r2Dj5SUlGMqGgAAAACOJKSgU11drfXr1ysjI6PuBDExysjIUHFxcZPHfffdd+rfv7/S0tLkdrv12WefHfF1qqqqVFFRUe8BAAAAAM0VUtDZt2+f/H5/gxGZlJQUlZWVNXrMoEGDtHTpUnk8Hj333HOqqanROeeco6+++qrJ18nLy1NycnLwkZaWFkqZAAAAADq4Nl9eetSoUZo4caKGDh2q0aNHa+XKlTr++OP15JNPNnnMrFmz5PP5go9du3a1dZkAWolhSNnZNPwEAADhFdLy0j179pTdbld5eXm97eXl5erdu3ezzhEXF6czzzxTW7ZsaXKfhIQEJSQkhFIagAhgGJLbHeiFk58fWC6aFdQAAEA4hDSiEx8fr2HDhqmwsDC4raamRoWFhRo1alSzzuH3+/XJJ58oNTU1tEoBRDyvt67hp90e6IkDAAAQDiFPXcvJydHixYv19NNPa9OmTbrxxhtVWVmpyZMnS5ImTpyoWbNmBfe/++679eabb2rr1q3asGGDrr32Wu3YsUPXX399670LABHB6awLOX5/oPEnAABAOIQ0dU2Sxo8fr71792rOnDkqKyvT0KFDtXr16uACBTt37lRMTF1+2r9/v6ZMmaKysjL97Gc/07Bhw/T+++/rtNNOa713ASAiuFyB6WpFRYGQw7Q1AAAQLjbTNM1wF3E0FRUVSk5Ols/nU1JSUrjLAQAAABAmzc0Gbb7qGgAAAAC0N4IOAAAAAMsh6AAAAACwHIIOAAAAAMsh6ABolGFI2dmBZwAAgGhD0AHQgGFIbrdUUBB4JuwAAIBoQ9AB0IDXW9f0024P9MUBAACIJgQdAA04nXUhx+8PNP8EAACIJrHhLgBA5HG5JI8nMJLjcAQ+BgAAiCYEHQCNcrkIOAAAIHoxdQ0AAACA5RB0AAAAAFgOQQcAAACA5RB0AAAAAFgOQQewMMOQsrNp+AkAADoegg5gUYYhud1SQUHgmbADAAA6EoIOYFFeb13DT7s90BMHAACgoyDoABbldNaFHL8/0PgTAACgo6BhKGBRLpfk8QRGchwOmn8CAICOhaADWJjLRcABAAAdE1PXAAAAADQtSpdxJegAAAAAaFwUL+NK0AEAAADQuChexpWgAwAAAKBxUbyMK4sRAFHAMAJ/UHE6WVwAAAC0oyhextVmmqYZ7iKOpqKiQsnJyfL5fEpKSgp3OUC7qp0aW/uHFI8nqv4fAwAAIoVF/nLa3GzA1DUgwkXx1FgAABAponhRgZYi6AARLoqnxgIAgEjRAf9yStABIlzt1Njp05m2BgAAWqgD/uWUe3QAAACAjsAwonJRgZ9qbjZg1TUAAAAgmrR0UQGXK6oDTqiYugYAAABEiw64qEBLEXQAAACAaNEBFxVoKYIOAAAAEC064KICLcU9OkA7skifLgAAEC61y7FaYFGBtsaqa0A7qZ1SW/sHGJaKBgCgA+Ovny3W3GzA1DWgnTClFgAASGJBgXZC0AHaCVNqAQCAJP762U4IOkA7qZ1SO30609YAAOjQ+Otnu+AeHQAAAKC9GQYLCrRQc7MBq64BAAAALdXSRQVcLgJOG2PqGgAAANASLCoQ0Qg6AAAAQEuwqEBEI+gAAAAALcGiAhGNe3SAENHfCwAAC2rJD/jaJVVZVCAiseoaEILaqbi1f7hhmWgAACyAH/BRpbnZgKlrQAiYigsAgAXxA96SCDpACJiKCwCABfED3pK4RwcIAVNxAQCwIH7AWxL36AAAAMAaWDGoQ+AeHQAAAHQcNO/ETxB0AAAAEP1YUAA/QdABAABA9GNBAfwEixEAAAAg+rGgAH6CoIMOi/sVAQCIUC39Ie1y8UMdQay6hg6JBsgAAEQofkjjKFh1DTgC7lcEACBC8UMarYSggw6J+xUBAIhQ/JBGK+EeHXRI3K8IAECE4oc0Wgn36AAAAKD1seoP2gj36AAAACA8ahcUKCgIPBtGuCtCB9SioLNo0SKlp6crMTFRI0eO1Lp165p13PLly2Wz2TRu3LiWvCwAAACiAQsKIAKEHHRWrFihnJwc5ebmasOGDRoyZIgyMzO1Z8+eIx63fft23X777Tr//PNbXCwAAACiAAsKIAKEfI/OyJEjdfbZZ2vhwoWSpJqaGqWlpemWW27RzJkzGz3G7/frggsu0O9+9zv9/e9/17fffqtXX321ydeoqqpSVVVV8OOKigqlpaVxjw4AAEC0MAwWFECbaJN7dKqrq7V+/XplZGTUnSAmRhkZGSouLm7yuLvvvlu9evXSdddd16zXycvLU3JycvCRlpYWSpnoYAxDys5m+i8AAG2ipT9oXS5pwQJCDsImpKCzb98++f1+paSk1NuekpKisrKyRo959913tWTJEi1evLjZrzNr1iz5fL7gY9euXaGUiQ6Eex0BAGhD/KBFFGvTVdcOHDigCRMmaPHixerZs2ezj0tISFBSUlK9B9AY7nUEAKAN8YMWUSykoNOzZ0/Z7XaVl5fX215eXq7evXs32P/LL7/U9u3bNXbsWMXGxio2NlbPPPOMDMNQbGysvvzyy2OrHh0e9zoCANCG+EGLKBYbys7x8fEaNmyYCgsLg0tE19TUqLCwUDfffHOD/U855RR98skn9bbdddddOnDggB555BHuvcExo3kyAABtiB+0iGIhBR1JysnJUVZWloYPH64RI0YoPz9flZWVmjx5siRp4sSJ6tu3r/Ly8pSYmKjTTz+93vHdunWTpAbbgZZyufj/LgAAbYYftIhSIQed8ePHa+/evZozZ47Kyso0dOhQrV69OrhAwc6dOxUT06a3/gAAAADAEYXcRyccmrtWNgAAAABra5M+OgAAAAAQDQg6AAAAACyHoIOI0NKmywAAAEBjCDoIO5ouAwAAoLURdBB2NF0GAABAayPoIOxougwAAIDWFnIfHaC10XQZAAAArY2gg4hA02UAAAC0JqauAQAAALAcgg4AAAAAyyHoAAAAALAcgg4AAAAAyyHooFUZhpSdTdNPAAAAhBdBB63GMCS3WyooCDwTdgAAABAuBB20Gq+3rumn3R7oiwMAAACEA0EHrcbprAs5fn+g+ScAAAAQDjQMRatxuSSPJzCS43DQABQAAADhQ9BBq3K5CDgAAAAIP6auAQAAALAcgg4AAAAAyyHoAAAAALAcgg4AAAAAyyHooAHDkLKzafgJAACA6EXQQT2GIbndUkFB4JmwAwAAgGhE0EE9Xm9dw0+7PdATBwAAAIg2BB3U43TWhRy/P9D4EwAAAIg2NAxFPS6X5PEERnIcDpp/AgAAIDoRdNCAy0XAAQAAQHRj6hoAAAAAyyHoAAAAALAcgg4AAAAAyyHoAAAAALAcgo6FGYaUnU3TTwAAAHQ8BB2LMgzJ7ZYKCgLPhB0AAAB0JAQdi/J665p+2u2BvjgAAABAR0HQsSinsy7k+P2B5p8AAABAR0HDUItyuSSPJzCS43DQABQAAAAdC0HHwlwuAg4AAAA6JqauAQAAALAcgg4AAAAAyyHoAAAAALAcgg4AAAAAyyHoRAHDkLKzafoJAAAANBdBJ8IZhuR2SwUFgWfCDgAAAHB0BJ0I5/XWNf202wN9cQAAAAAcGUEnwjmddSHH7w80/wQAAABwZDQMjXAul+TxBEZyHA4agAIAAADNQdCJAi4XAQcAAAAIBVPXAAAAAFgOQQcAAACA5RB0AAAAAFgOQQcAAACA5RB02olhSNnZNPwEAAAA2gNBpx0YhuR2SwUFgWfCDgAAANC2CDrtwOuta/hptwd64gAAAABoOwSdduB01oUcvz/Q+BMAAABA26FhaDtwuSSPJzCS43DQ/BMAAABoawSdduJyEXAAAACA9sLUNQAAAACWQ9ABAAAAYDktCjqLFi1Senq6EhMTNXLkSK1bt67JfVeuXKnhw4erW7duOu644zR06FA9++yzLS4YAAAAAI4m5KCzYsUK5eTkKDc3Vxs2bNCQIUOUmZmpPXv2NLp/9+7ddeedd6q4uFj/+te/NHnyZE2ePFlvvPHGMRcPAAAAAI2xmaZphnLAyJEjdfbZZ2vhwoWSpJqaGqWlpemWW27RzJkzm3WOs846S2PGjNE999zTrP0rKiqUnJwsn8+npKSkUMptdYYR6IvjdLK4AAAAANDempsNQhrRqa6u1vr165WRkVF3gpgYZWRkqLi4+KjHm6apwsJClZSU6IILLmhyv6qqKlVUVNR7RALDkNxuqaAg8GwY4a4IAAAAQGNCCjr79u2T3+9XSkpKve0pKSkqKytr8jifz6cuXbooPj5eY8aMUUFBgS6++OIm98/Ly1NycnLwkZaWFkqZbcbrrWv6abcH+uIAAAAAiDztsupa165dtXHjRn344YeaP3++cnJyVHSElDBr1iz5fL7gY9euXe1R5lE5nXUhx+8PNP8EAAAAEHlCahjas2dP2e12lZeX19teXl6u3r17N3lcTEyMTjzxREnS0KFDtWnTJuXl5cnRRFJISEhQQkJCKKW1C5dL8ngCIzkOB/foAAAAAJEqpBGd+Ph4DRs2TIWFhcFtNTU1Kiws1KhRo5p9npqaGlVVVYXy0hHD5ZIWLCDkAAAAAJEspBEdScrJyVFWVpaGDx+uESNGKD8/X5WVlZo8ebIkaeLEierbt6/y8vIkBe63GT58uAYOHKiqqiq9/vrrevbZZ/X444+37jsBAAAAgP9fyEFn/Pjx2rt3r+bMmaOysjINHTpUq1evDi5QsHPnTsXE1A0UVVZW6qabbtJXX32lTp066ZRTTtFzzz2n8ePHt967AAAAAIAfCbmPTjhEUh8dAAAAAOHTJn10AAAAACAaEHQAAAAAWA5BBwAAAIDlEHQAAAAAWA5BBwAAAIDlEHQAAAAAWA5BBwAAAIDlEHQAAAAAWA5BBwAAAIDlEHQAAAAAWA5BBwAAAIDlEHQAAAAAWA5BBwAAAIDlEHQAAAAAWA5BBwAAAIDlEHQAAAAAWE5suAtoDtM0JUkVFRVhrgQAAABAONVmgtqM0JSoCDoHDhyQJKWlpYW5EgAAAACR4MCBA0pOTm7y8zbzaFEoAtTU1Ojrr79W165dZbPZwlpLRUWF0tLStGvXLiUlJYW1FkQfrh8cC64ftBTXDo4F1w+ORVtcP6Zp6sCBA+rTp49iYpq+EycqRnRiYmLUr1+/cJdRT1JSEt/saDGuHxwLrh+0FNcOjgXXD45Fa18/RxrJqcViBAAAAAAsh6ADAAAAwHIIOiFKSEhQbm6uEhISwl0KohDXD44F1w9aimsHx4LrB8cinNdPVCxGAAAAAAChYEQHAAAAgOUQdAAAAABYDkEHAAAAgOUQdAAAAABYDkEHAAAAgOUQdBqxaNEipaenKzExUSNHjtS6deuOuP9f/vIXnXLKKUpMTNTgwYP1+uuvt1OliEShXD+LFy/W+eefr5/97Gf62c9+poyMjKNeb7CuUP/fU2v58uWy2WwaN25c2xaIiBbq9fPtt99q2rRpSk1NVUJCgk4++WR+fnVgoV4/+fn5GjRokDp16qS0tDRlZ2fr4MGD7VQtIsU777yjsWPHqk+fPrLZbHr11VePekxRUZHOOussJSQk6MQTT9SyZcvarD6Czk+sWLFCOTk5ys3N1YYNGzRkyBBlZmZqz549je7//vvv6+qrr9Z1112njz76SOPGjdO4ceP06aeftnPliAShXj9FRUW6+uqr5fV6VVxcrLS0NF1yySUqLS1t58oRbqFeO7W2b9+u22+/Xeeff347VYpIFOr1U11drYsvvljbt2/XSy+9pJKSEi1evFh9+/Zt58oRCUK9fp5//nnNnDlTubm52rRpk5YsWaIVK1bojjvuaOfKEW6VlZUaMmSIFi1a1Kz9t23bpjFjxsjpdGrjxo36/e9/r+uvv15vvPFG2xRoop4RI0aY06ZNC37s9/vNPn36mHl5eY3uf+WVV5pjxoypt23kyJHm//t//69N60RkCvX6+anDhw+bXbt2NZ9++um2KhERqiXXzuHDh81zzjnH/L//+z8zKyvLdLvd7VApIlGo18/jjz9unnDCCWZ1dXV7lYgIFur1M23aNPPCCy+sty0nJ8c899xz27RORDZJ5iuvvHLEfWbMmGH+4he/qLdt/PjxZmZmZpvUxIjOj1RXV2v9+vXKyMgIbouJiVFGRoaKi4sbPaa4uLje/pKUmZnZ5P6wrpZcPz/1/fff69ChQ+revXtblYkI1NJr5+6771avXr103XXXtUeZiFAtuX4Mw9CoUaM0bdo0paSk6PTTT9e9994rv9/fXmUjQrTk+jnnnHO0fv364PS2rVu36vXXX9ell17aLjUjerX3782xbXLWKLVv3z75/X6lpKTU256SkqLNmzc3ekxZWVmj+5eVlbVZnYhMLbl+fuqPf/yj+vTp0+B/ArC2llw77777rpYsWaKNGze2Q4WIZC25frZu3aq3335b//3f/63XX39dW7Zs0U033aRDhw4pNze3PcpGhGjJ9XPNNddo3759Ou+882Sapg4fPqypU6cydQ1H1dTvzRUVFfrhhx/UqVOnVn09RnSACHHfffdp+fLleuWVV5SYmBjuchDBDhw4oAkTJmjx4sXq2bNnuMtBFKqpqVGvXr30v//7vxo2bJjGjx+vO++8U0888US4S0MUKCoq0r333qvHHntMGzZs0MqVK7Vq1Srdc8894S4NqIcRnR/p2bOn7Ha7ysvL620vLy9X7969Gz2md+/eIe0P62rJ9VPrwQcf1H333ac1a9bojDPOaMsyEYFCvXa+/PJLbd++XWPHjg1uq6mpkSTFxsaqpKREAwcObNuiETFa8v+e1NRUxcXFyW63B7edeuqpKisrU3V1teLj49u0ZkSOllw/s2fP1oQJE3T99ddLkgYPHqzKykrdcMMNuvPOOxUTw9/R0bimfm9OSkpq9dEciRGdeuLj4zVs2DAVFhYGt9XU1KiwsFCjRo1q9JhRo0bV21+S3nrrrSb3h3W15PqRpD/96U+65557tHr1ag0fPrw9SkWECfXaOeWUU/TJJ59o48aNwYfL5QquYpOWltae5SPMWvL/nnPPPVdbtmwJBmRJ+uKLL5SamkrI6WBacv18//33DcJMbWgO3JMONK7df29ukyUOotjy5cvNhIQEc9myZebnn39u3nDDDWa3bt3MsrIy0zRNc8KECebMmTOD+7/33ntmbGys+eCDD5qbNm0yc3Nzzbi4OPOTTz4J11tAGIV6/dx3331mfHy8+dJLL5m7d+8OPg4cOBCut4AwCfXa+SlWXevYQr1+du7caXbt2tW8+eabzZKSEvO1114ze/XqZf7P//xPuN4CwijU6yc3N9fs2rWr+cILL5hbt24133zzTXPgwIHmlVdeGa63gDA5cOCA+dFHH5kfffSRKclcsGCB+dFHH5k7duwwTdM0Z86caU6YMCG4/9atW83OnTubf/jDH8xNmzaZixYtMu12u7l69eo2qY+g04iCggLz5z//uRkfH2+OGDHC/OCDD4KfGz16tJmVlVVv/xdffNE8+eSTzfj4ePMXv/iFuWrVqnauGJEklOunf//+pqQGj9zc3PYvHGEX6v97foygg1Cvn/fff98cOXKkmZCQYJ5wwgnm/PnzzcOHD7dz1YgUoVw/hw4dMufOnWsOHDjQTExMNNPS0sybbrrJ3L9/f/sXjrDyer2N/h5Te71kZWWZo0ePbnDM0KFDzfj4ePOEE04wn3rqqTarz2aajDECAAAAsBbu0QEAAABgOQQdAAAAAJZD0AEAAABgOQQdAAAAAJZD0AEAAABgOQQdAAAAAJZD0AEAAABgOQQdAAAAAJZD0AEAAABgOQQdAAAAAJZD0AEAAABgOf8f2Jzj1ZAxxOsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To visuslize this res for now\n",
    "plot_predictions(predictions=y_preds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613e6e7d-cb7f-4fd8-a00e-cc4ff3f10658",
   "metadata": {},
   "source": [
    "## Train model\n",
    "the whole idea of training is for a model to move from some *unknown* parameters (these may be random) to some *known* parameters.\n",
    "\n",
    "or in other words from a poor representation of the data\n",
    "\n",
    " - One way to measure how poor or how wrong your models prodiction are is to use a loss function.\n",
    "\n",
    " - * Note: loss function may also be called cost function or criterion in different areas. For our case, We're going to refer to it as losss function.\n",
    "  \n",
    "Things we need to train: \n",
    "* **Loss function:** A function to measure how wrong your model's predictions are to the ideal outputs, lower is better [https://pytorch.org/docs/stable/nn.html#loss-functions] -- for vary functions for different purpose\n",
    "* **Optimizer:** Takes into account the loss of a model and adjusts the model's parameters (e.g. weight & bias) to improve the loss function.\n",
    "\n",
    "  And specifically for PyTorch, we need:\n",
    "  - a training loop\n",
    "  - a test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96b274b9-4118-4c4d-b2c4-84ef2c315221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chosse and set uo a loss function\n",
    "loss_fn = nn.L1Loss()\n",
    "\n",
    "#setup an optimizer (stochastic gradient descent)\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr = 0.0001) # lr stand for --> learning Rate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8919e73b-10f0-466d-a482-ec86a15c4e31",
   "metadata": {},
   "source": [
    "### Build up the training loop  --> [Also a testing loop] in pyTorch\n",
    "A couple of things we need in a training loop: \n",
    "0. Loop through the data\n",
    "1. Forward pass (this involves data moving through our model's `forward()` functions) to make predictions on data - also called forward propagation\n",
    "2. Calculate the loss (Compare forward pass predictions to ground truth labels)\n",
    "3. Optimizer zero grad\n",
    "4. Loss backward - move backwards through the newwork to calculate the gradients of each of the parameters of our model with respect to the loss\n",
    "5. Optimizer step - use the optimizer to adjust our model's parameters to try and improve the loss (**gradient descent**)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f61649d4-1dcb-49d4-89fc-805529185d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.4944015145301819 \n",
      "Epoch: 10 | MAE Train Loss: 0.31172919273376465 | MAE Test Loss: 0.49305421113967896 \n",
      "Epoch: 20 | MAE Train Loss: 0.31057703495025635 | MAE Test Loss: 0.4917070269584656 \n",
      "Epoch: 30 | MAE Train Loss: 0.30942484736442566 | MAE Test Loss: 0.4903597831726074 \n",
      "Epoch: 40 | MAE Train Loss: 0.30827268958091736 | MAE Test Loss: 0.48901262879371643 \n",
      "Epoch: 50 | MAE Train Loss: 0.30712056159973145 | MAE Test Loss: 0.4876653552055359 \n",
      "Epoch: 60 | MAE Train Loss: 0.30596840381622314 | MAE Test Loss: 0.4863181710243225 \n",
      "Epoch: 70 | MAE Train Loss: 0.30481624603271484 | MAE Test Loss: 0.48497089743614197 \n",
      "Epoch: 80 | MAE Train Loss: 0.30366405844688416 | MAE Test Loss: 0.483623743057251 \n",
      "Epoch: 90 | MAE Train Loss: 0.30251190066337585 | MAE Test Loss: 0.4822765290737152 \n",
      "Epoch: 100 | MAE Train Loss: 0.30135971307754517 | MAE Test Loss: 0.4809292256832123 \n",
      "Epoch: 110 | MAE Train Loss: 0.30020755529403687 | MAE Test Loss: 0.4795820713043213 \n",
      "Epoch: 120 | MAE Train Loss: 0.29905542731285095 | MAE Test Loss: 0.4782348573207855 \n",
      "Epoch: 130 | MAE Train Loss: 0.29790323972702026 | MAE Test Loss: 0.47688764333724976 \n",
      "Epoch: 140 | MAE Train Loss: 0.29675108194351196 | MAE Test Loss: 0.4755404591560364 \n",
      "Epoch: 150 | MAE Train Loss: 0.29559892416000366 | MAE Test Loss: 0.47419318556785583 \n",
      "Epoch: 160 | MAE Train Loss: 0.29444676637649536 | MAE Test Loss: 0.47284597158432007 \n",
      "Epoch: 170 | MAE Train Loss: 0.29329460859298706 | MAE Test Loss: 0.4714987874031067 \n",
      "Epoch: 180 | MAE Train Loss: 0.29214248061180115 | MAE Test Loss: 0.4701515734195709 \n",
      "Epoch: 190 | MAE Train Loss: 0.29099029302597046 | MAE Test Loss: 0.4688042998313904 \n",
      "Epoch: 200 | MAE Train Loss: 0.28983813524246216 | MAE Test Loss: 0.467457115650177 \n",
      "Epoch: 210 | MAE Train Loss: 0.28868597745895386 | MAE Test Loss: 0.46610990166664124 \n",
      "Epoch: 220 | MAE Train Loss: 0.28753381967544556 | MAE Test Loss: 0.46476268768310547 \n",
      "Epoch: 230 | MAE Train Loss: 0.28638166189193726 | MAE Test Loss: 0.4634154438972473 \n",
      "Epoch: 240 | MAE Train Loss: 0.28522950410842896 | MAE Test Loss: 0.46206825971603394 \n",
      "Epoch: 250 | MAE Train Loss: 0.28407734632492065 | MAE Test Loss: 0.4607210159301758 \n",
      "Epoch: 260 | MAE Train Loss: 0.28292518854141235 | MAE Test Loss: 0.4593738615512848 \n",
      "Epoch: 270 | MAE Train Loss: 0.28177303075790405 | MAE Test Loss: 0.45802658796310425 \n",
      "Epoch: 280 | MAE Train Loss: 0.28062087297439575 | MAE Test Loss: 0.4566793441772461 \n",
      "Epoch: 290 | MAE Train Loss: 0.27946868538856506 | MAE Test Loss: 0.4553321897983551 \n",
      "Epoch: 300 | MAE Train Loss: 0.27831655740737915 | MAE Test Loss: 0.45398491621017456 \n",
      "Epoch: 310 | MAE Train Loss: 0.27716436982154846 | MAE Test Loss: 0.45263776183128357 \n",
      "Epoch: 320 | MAE Train Loss: 0.2760121822357178 | MAE Test Loss: 0.4512905180454254 \n",
      "Epoch: 330 | MAE Train Loss: 0.27486005425453186 | MAE Test Loss: 0.44994330406188965 \n",
      "Epoch: 340 | MAE Train Loss: 0.27370789647102356 | MAE Test Loss: 0.4485960900783539 \n",
      "Epoch: 350 | MAE Train Loss: 0.27255573868751526 | MAE Test Loss: 0.4472488760948181 \n",
      "Epoch: 360 | MAE Train Loss: 0.27140355110168457 | MAE Test Loss: 0.44590163230895996 \n",
      "Epoch: 370 | MAE Train Loss: 0.27025142312049866 | MAE Test Loss: 0.4445544183254242 \n",
      "Epoch: 380 | MAE Train Loss: 0.26909923553466797 | MAE Test Loss: 0.4432072043418884 \n",
      "Epoch: 390 | MAE Train Loss: 0.26794707775115967 | MAE Test Loss: 0.44186002016067505 \n",
      "Epoch: 400 | MAE Train Loss: 0.26679491996765137 | MAE Test Loss: 0.4405128061771393 \n",
      "Epoch: 410 | MAE Train Loss: 0.26564276218414307 | MAE Test Loss: 0.43916553258895874 \n",
      "Epoch: 420 | MAE Train Loss: 0.26449060440063477 | MAE Test Loss: 0.43781834840774536 \n",
      "Epoch: 430 | MAE Train Loss: 0.26333844661712646 | MAE Test Loss: 0.4364711344242096 \n",
      "Epoch: 440 | MAE Train Loss: 0.26218628883361816 | MAE Test Loss: 0.43512392044067383 \n",
      "Epoch: 450 | MAE Train Loss: 0.2610341012477875 | MAE Test Loss: 0.4337766766548157 \n",
      "Epoch: 460 | MAE Train Loss: 0.2598819434642792 | MAE Test Loss: 0.4324294924736023 \n",
      "Epoch: 470 | MAE Train Loss: 0.2587297856807709 | MAE Test Loss: 0.43108224868774414 \n",
      "Epoch: 480 | MAE Train Loss: 0.2575775980949402 | MAE Test Loss: 0.4297350347042084 \n",
      "Epoch: 490 | MAE Train Loss: 0.2564254701137543 | MAE Test Loss: 0.4283878207206726 \n",
      "Epoch: 500 | MAE Train Loss: 0.2552732825279236 | MAE Test Loss: 0.42704063653945923 \n",
      "Epoch: 510 | MAE Train Loss: 0.25412115454673767 | MAE Test Loss: 0.4256933629512787 \n",
      "Epoch: 520 | MAE Train Loss: 0.252968966960907 | MAE Test Loss: 0.4243461489677429 \n",
      "Epoch: 530 | MAE Train Loss: 0.25181683897972107 | MAE Test Loss: 0.42299899458885193 \n",
      "Epoch: 540 | MAE Train Loss: 0.2506646513938904 | MAE Test Loss: 0.4216517508029938 \n",
      "Epoch: 550 | MAE Train Loss: 0.24951250851154327 | MAE Test Loss: 0.420304536819458 \n",
      "Epoch: 560 | MAE Train Loss: 0.24836035072803497 | MAE Test Loss: 0.41895729303359985 \n",
      "Epoch: 570 | MAE Train Loss: 0.24720819294452667 | MAE Test Loss: 0.4176101088523865 \n",
      "Epoch: 580 | MAE Train Loss: 0.24605603516101837 | MAE Test Loss: 0.4162628650665283 \n",
      "Epoch: 590 | MAE Train Loss: 0.24490387737751007 | MAE Test Loss: 0.41491565108299255 \n",
      "Epoch: 600 | MAE Train Loss: 0.24375171959400177 | MAE Test Loss: 0.4135684370994568 \n",
      "Epoch: 610 | MAE Train Loss: 0.24259953200817108 | MAE Test Loss: 0.41222119331359863 \n",
      "Epoch: 620 | MAE Train Loss: 0.24144737422466278 | MAE Test Loss: 0.41087397933006287 \n",
      "Epoch: 630 | MAE Train Loss: 0.24029521644115448 | MAE Test Loss: 0.4095267653465271 \n",
      "Epoch: 640 | MAE Train Loss: 0.23914304375648499 | MAE Test Loss: 0.4081795811653137 \n",
      "Epoch: 650 | MAE Train Loss: 0.23799090087413788 | MAE Test Loss: 0.40683236718177795 \n",
      "Epoch: 660 | MAE Train Loss: 0.23683874309062958 | MAE Test Loss: 0.4054851531982422 \n",
      "Epoch: 670 | MAE Train Loss: 0.23568658530712128 | MAE Test Loss: 0.4041379392147064 \n",
      "Epoch: 680 | MAE Train Loss: 0.23453441262245178 | MAE Test Loss: 0.40279069542884827 \n",
      "Epoch: 690 | MAE Train Loss: 0.23338226974010468 | MAE Test Loss: 0.4014434814453125 \n",
      "Epoch: 700 | MAE Train Loss: 0.23223009705543518 | MAE Test Loss: 0.40009626746177673 \n",
      "Epoch: 710 | MAE Train Loss: 0.23107793927192688 | MAE Test Loss: 0.39874905347824097 \n",
      "Epoch: 720 | MAE Train Loss: 0.22992578148841858 | MAE Test Loss: 0.3974018096923828 \n",
      "Epoch: 730 | MAE Train Loss: 0.22877362370491028 | MAE Test Loss: 0.39605459570884705 \n",
      "Epoch: 740 | MAE Train Loss: 0.2276214361190796 | MAE Test Loss: 0.3947073817253113 \n",
      "Epoch: 750 | MAE Train Loss: 0.2264692783355713 | MAE Test Loss: 0.3933602273464203 \n",
      "Epoch: 760 | MAE Train Loss: 0.225317120552063 | MAE Test Loss: 0.39201295375823975 \n",
      "Epoch: 770 | MAE Train Loss: 0.2241649627685547 | MAE Test Loss: 0.39066576957702637 \n",
      "Epoch: 780 | MAE Train Loss: 0.2230128049850464 | MAE Test Loss: 0.3893185257911682 \n",
      "Epoch: 790 | MAE Train Loss: 0.22186064720153809 | MAE Test Loss: 0.38797131180763245 \n",
      "Epoch: 800 | MAE Train Loss: 0.2207084596157074 | MAE Test Loss: 0.3866240978240967 \n",
      "Epoch: 810 | MAE Train Loss: 0.21955633163452148 | MAE Test Loss: 0.3852768540382385 \n",
      "Epoch: 820 | MAE Train Loss: 0.21840420365333557 | MAE Test Loss: 0.38392966985702515 \n",
      "Epoch: 830 | MAE Train Loss: 0.21725201606750488 | MAE Test Loss: 0.3825824558734894 \n",
      "Epoch: 840 | MAE Train Loss: 0.2160998284816742 | MAE Test Loss: 0.3812352120876312 \n",
      "Epoch: 850 | MAE Train Loss: 0.21494770050048828 | MAE Test Loss: 0.37988802790641785 \n",
      "Epoch: 860 | MAE Train Loss: 0.2137955129146576 | MAE Test Loss: 0.3785407841205597 \n",
      "Epoch: 870 | MAE Train Loss: 0.2126433551311493 | MAE Test Loss: 0.3771935999393463 \n",
      "Epoch: 880 | MAE Train Loss: 0.211491197347641 | MAE Test Loss: 0.37584635615348816 \n",
      "Epoch: 890 | MAE Train Loss: 0.2103390395641327 | MAE Test Loss: 0.37449911236763 \n",
      "Epoch: 900 | MAE Train Loss: 0.2091868668794632 | MAE Test Loss: 0.373151957988739 \n",
      "Epoch: 910 | MAE Train Loss: 0.2080347239971161 | MAE Test Loss: 0.37180468440055847 \n",
      "Epoch: 920 | MAE Train Loss: 0.2068825662136078 | MAE Test Loss: 0.3704574704170227 \n",
      "Epoch: 930 | MAE Train Loss: 0.2057303935289383 | MAE Test Loss: 0.3691102862358093 \n",
      "Epoch: 940 | MAE Train Loss: 0.20457823574543 | MAE Test Loss: 0.36776304244995117 \n",
      "Epoch: 950 | MAE Train Loss: 0.2034260779619217 | MAE Test Loss: 0.3664158582687378 \n",
      "Epoch: 960 | MAE Train Loss: 0.20227393507957458 | MAE Test Loss: 0.3650686740875244 \n",
      "Epoch: 970 | MAE Train Loss: 0.2011217623949051 | MAE Test Loss: 0.36372143030166626 \n",
      "Epoch: 980 | MAE Train Loss: 0.1999696046113968 | MAE Test Loss: 0.3623741865158081 \n",
      "Epoch: 990 | MAE Train Loss: 0.1988174170255661 | MAE Test Loss: 0.36102694272994995 \n",
      "Epoch: 1000 | MAE Train Loss: 0.1976652592420578 | MAE Test Loss: 0.3596797585487366 \n",
      "Epoch: 1010 | MAE Train Loss: 0.1965131163597107 | MAE Test Loss: 0.3583325445652008 \n",
      "Epoch: 1020 | MAE Train Loss: 0.1953609734773636 | MAE Test Loss: 0.35698533058166504 \n",
      "Epoch: 1030 | MAE Train Loss: 0.1942088007926941 | MAE Test Loss: 0.3556380867958069 \n",
      "Epoch: 1040 | MAE Train Loss: 0.1930566281080246 | MAE Test Loss: 0.3542909026145935 \n",
      "Epoch: 1050 | MAE Train Loss: 0.1919044703245163 | MAE Test Loss: 0.35294368863105774 \n",
      "Epoch: 1060 | MAE Train Loss: 0.1907522976398468 | MAE Test Loss: 0.351596474647522 \n",
      "Epoch: 1070 | MAE Train Loss: 0.1896001547574997 | MAE Test Loss: 0.3502492308616638 \n",
      "Epoch: 1080 | MAE Train Loss: 0.1884479969739914 | MAE Test Loss: 0.34890201687812805 \n",
      "Epoch: 1090 | MAE Train Loss: 0.1872958242893219 | MAE Test Loss: 0.3475547730922699 \n",
      "Epoch: 1100 | MAE Train Loss: 0.1861436665058136 | MAE Test Loss: 0.34620755910873413 \n",
      "Epoch: 1110 | MAE Train Loss: 0.1849915087223053 | MAE Test Loss: 0.34486037492752075 \n",
      "Epoch: 1120 | MAE Train Loss: 0.183839350938797 | MAE Test Loss: 0.3435131311416626 \n",
      "Epoch: 1130 | MAE Train Loss: 0.1826871931552887 | MAE Test Loss: 0.3421659469604492 \n",
      "Epoch: 1140 | MAE Train Loss: 0.1815350204706192 | MAE Test Loss: 0.34081873297691345 \n",
      "Epoch: 1150 | MAE Train Loss: 0.1803828626871109 | MAE Test Loss: 0.3394715189933777 \n",
      "Epoch: 1160 | MAE Train Loss: 0.1792306900024414 | MAE Test Loss: 0.3381243348121643 \n",
      "Epoch: 1170 | MAE Train Loss: 0.1780785471200943 | MAE Test Loss: 0.33677709102630615 \n",
      "Epoch: 1180 | MAE Train Loss: 0.176926389336586 | MAE Test Loss: 0.335429847240448 \n",
      "Epoch: 1190 | MAE Train Loss: 0.1757742166519165 | MAE Test Loss: 0.33408263325691223 \n",
      "Epoch: 1200 | MAE Train Loss: 0.174622043967247 | MAE Test Loss: 0.33273541927337646 \n",
      "Epoch: 1210 | MAE Train Loss: 0.1734698861837387 | MAE Test Loss: 0.3313881754875183 \n",
      "Epoch: 1220 | MAE Train Loss: 0.17231786251068115 | MAE Test Loss: 0.3300411105155945 \n",
      "Epoch: 1230 | MAE Train Loss: 0.1711658537387848 | MAE Test Loss: 0.32869404554367065 \n",
      "Epoch: 1240 | MAE Train Loss: 0.17001384496688843 | MAE Test Loss: 0.3273469805717468 \n",
      "Epoch: 1250 | MAE Train Loss: 0.16886183619499207 | MAE Test Loss: 0.3259999454021454 \n",
      "Epoch: 1260 | MAE Train Loss: 0.1677098125219345 | MAE Test Loss: 0.32465288043022156 \n",
      "Epoch: 1270 | MAE Train Loss: 0.16655781865119934 | MAE Test Loss: 0.32330575585365295 \n",
      "Epoch: 1280 | MAE Train Loss: 0.16540579497814178 | MAE Test Loss: 0.3219586908817291 \n",
      "Epoch: 1290 | MAE Train Loss: 0.16425378620624542 | MAE Test Loss: 0.3206116557121277 \n",
      "Epoch: 1300 | MAE Train Loss: 0.16310177743434906 | MAE Test Loss: 0.31926456093788147 \n",
      "Epoch: 1310 | MAE Train Loss: 0.1619497537612915 | MAE Test Loss: 0.31791752576828003 \n",
      "Epoch: 1320 | MAE Train Loss: 0.16079774498939514 | MAE Test Loss: 0.3165704607963562 \n",
      "Epoch: 1330 | MAE Train Loss: 0.15964575111865997 | MAE Test Loss: 0.3152233958244324 \n",
      "Epoch: 1340 | MAE Train Loss: 0.15849372744560242 | MAE Test Loss: 0.31387633085250854 \n",
      "Epoch: 1350 | MAE Train Loss: 0.15734171867370605 | MAE Test Loss: 0.3125292658805847 \n",
      "Epoch: 1360 | MAE Train Loss: 0.1561896950006485 | MAE Test Loss: 0.3111822009086609 \n",
      "Epoch: 1370 | MAE Train Loss: 0.15503768622875214 | MAE Test Loss: 0.30983513593673706 \n",
      "Epoch: 1380 | MAE Train Loss: 0.15388567745685577 | MAE Test Loss: 0.30848804116249084 \n",
      "Epoch: 1390 | MAE Train Loss: 0.1527336686849594 | MAE Test Loss: 0.307140976190567 \n",
      "Epoch: 1400 | MAE Train Loss: 0.15158167481422424 | MAE Test Loss: 0.3057939112186432 \n",
      "Epoch: 1410 | MAE Train Loss: 0.1504296511411667 | MAE Test Loss: 0.304446816444397 \n",
      "Epoch: 1420 | MAE Train Loss: 0.14927762746810913 | MAE Test Loss: 0.30309978127479553 \n",
      "Epoch: 1430 | MAE Train Loss: 0.14812563359737396 | MAE Test Loss: 0.3017526865005493 \n",
      "Epoch: 1440 | MAE Train Loss: 0.1469736099243164 | MAE Test Loss: 0.3004056215286255 \n",
      "Epoch: 1450 | MAE Train Loss: 0.14582160115242004 | MAE Test Loss: 0.29905855655670166 \n",
      "Epoch: 1460 | MAE Train Loss: 0.14466959238052368 | MAE Test Loss: 0.29771149158477783 \n",
      "Epoch: 1470 | MAE Train Loss: 0.14351758360862732 | MAE Test Loss: 0.2963644564151764 \n",
      "Epoch: 1480 | MAE Train Loss: 0.14236557483673096 | MAE Test Loss: 0.29501739144325256 \n",
      "Epoch: 1490 | MAE Train Loss: 0.1412135511636734 | MAE Test Loss: 0.29367026686668396 \n",
      "Epoch: 1500 | MAE Train Loss: 0.14006154239177704 | MAE Test Loss: 0.2923232614994049 \n",
      "Epoch: 1510 | MAE Train Loss: 0.13890953361988068 | MAE Test Loss: 0.2909761369228363 \n",
      "Epoch: 1520 | MAE Train Loss: 0.13775750994682312 | MAE Test Loss: 0.2896290719509125 \n",
      "Epoch: 1530 | MAE Train Loss: 0.13660551607608795 | MAE Test Loss: 0.28828200697898865 \n",
      "Epoch: 1540 | MAE Train Loss: 0.1354534924030304 | MAE Test Loss: 0.2869349718093872 \n",
      "Epoch: 1550 | MAE Train Loss: 0.13430148363113403 | MAE Test Loss: 0.2855879068374634 \n",
      "Epoch: 1560 | MAE Train Loss: 0.13314947485923767 | MAE Test Loss: 0.28424081206321716 \n",
      "Epoch: 1570 | MAE Train Loss: 0.13199745118618011 | MAE Test Loss: 0.2828937768936157 \n",
      "Epoch: 1580 | MAE Train Loss: 0.13084545731544495 | MAE Test Loss: 0.2815466523170471 \n",
      "Epoch: 1590 | MAE Train Loss: 0.1296934336423874 | MAE Test Loss: 0.2801996171474457 \n",
      "Epoch: 1600 | MAE Train Loss: 0.12854143977165222 | MAE Test Loss: 0.27885255217552185 \n",
      "Epoch: 1610 | MAE Train Loss: 0.12738941609859467 | MAE Test Loss: 0.27750545740127563 \n",
      "Epoch: 1620 | MAE Train Loss: 0.1262374073266983 | MAE Test Loss: 0.2761583924293518 \n",
      "Epoch: 1630 | MAE Train Loss: 0.12508539855480194 | MAE Test Loss: 0.274811327457428 \n",
      "Epoch: 1640 | MAE Train Loss: 0.12393335998058319 | MAE Test Loss: 0.27346426248550415 \n",
      "Epoch: 1650 | MAE Train Loss: 0.12278135865926743 | MAE Test Loss: 0.2721172273159027 \n",
      "Epoch: 1660 | MAE Train Loss: 0.12162934243679047 | MAE Test Loss: 0.2707701623439789 \n",
      "Epoch: 1670 | MAE Train Loss: 0.1204773411154747 | MAE Test Loss: 0.2694230377674103 \n",
      "Epoch: 1680 | MAE Train Loss: 0.11932532489299774 | MAE Test Loss: 0.26807600259780884 \n",
      "Epoch: 1690 | MAE Train Loss: 0.11817331612110138 | MAE Test Loss: 0.2667289078235626 \n",
      "Epoch: 1700 | MAE Train Loss: 0.11702130734920502 | MAE Test Loss: 0.2653818726539612 \n",
      "Epoch: 1710 | MAE Train Loss: 0.11586929857730865 | MAE Test Loss: 0.26403480768203735 \n",
      "Epoch: 1720 | MAE Train Loss: 0.11479532718658447 | MAE Test Loss: 0.26273253560066223 \n",
      "Epoch: 1730 | MAE Train Loss: 0.11374060064554214 | MAE Test Loss: 0.26143524050712585 \n",
      "Epoch: 1740 | MAE Train Loss: 0.11268587410449982 | MAE Test Loss: 0.2601379454135895 \n",
      "Epoch: 1750 | MAE Train Loss: 0.1116311326622963 | MAE Test Loss: 0.2588406205177307 \n",
      "Epoch: 1760 | MAE Train Loss: 0.11057639122009277 | MAE Test Loss: 0.25754329562187195 \n",
      "Epoch: 1770 | MAE Train Loss: 0.10952164977788925 | MAE Test Loss: 0.25624600052833557 \n",
      "Epoch: 1780 | MAE Train Loss: 0.1085236445069313 | MAE Test Loss: 0.2549845278263092 \n",
      "Epoch: 1790 | MAE Train Loss: 0.10756231844425201 | MAE Test Loss: 0.2537383735179901 \n",
      "Epoch: 1800 | MAE Train Loss: 0.10660099983215332 | MAE Test Loss: 0.25249215960502625 \n",
      "Epoch: 1810 | MAE Train Loss: 0.10563969612121582 | MAE Test Loss: 0.25124603509902954 \n",
      "Epoch: 1820 | MAE Train Loss: 0.10467837005853653 | MAE Test Loss: 0.24999985098838806 \n",
      "Epoch: 1830 | MAE Train Loss: 0.10371705144643784 | MAE Test Loss: 0.24875369668006897 \n",
      "Epoch: 1840 | MAE Train Loss: 0.10276877880096436 | MAE Test Loss: 0.24751785397529602 \n",
      "Epoch: 1850 | MAE Train Loss: 0.10189647972583771 | MAE Test Loss: 0.24632331728935242 \n",
      "Epoch: 1860 | MAE Train Loss: 0.10102419555187225 | MAE Test Loss: 0.2451288253068924 \n",
      "Epoch: 1870 | MAE Train Loss: 0.1001519113779068 | MAE Test Loss: 0.24393431842327118 \n",
      "Epoch: 1880 | MAE Train Loss: 0.09927962720394135 | MAE Test Loss: 0.2427397519350052 \n",
      "Epoch: 1890 | MAE Train Loss: 0.0984073281288147 | MAE Test Loss: 0.24154527485370636 \n",
      "Epoch: 1900 | MAE Train Loss: 0.09753505140542984 | MAE Test Loss: 0.24035079777240753 \n",
      "Epoch: 1910 | MAE Train Loss: 0.09670595079660416 | MAE Test Loss: 0.2391880750656128 \n",
      "Epoch: 1920 | MAE Train Loss: 0.09591863304376602 | MAE Test Loss: 0.2380465269088745 \n",
      "Epoch: 1930 | MAE Train Loss: 0.09513132274150848 | MAE Test Loss: 0.23690500855445862 \n",
      "Epoch: 1940 | MAE Train Loss: 0.09434400498867035 | MAE Test Loss: 0.23576350510120392 \n",
      "Epoch: 1950 | MAE Train Loss: 0.09355668723583221 | MAE Test Loss: 0.23462197184562683 \n",
      "Epoch: 1960 | MAE Train Loss: 0.09276936948299408 | MAE Test Loss: 0.23348042368888855 \n",
      "Epoch: 1970 | MAE Train Loss: 0.09198205173015594 | MAE Test Loss: 0.23233893513679504 \n",
      "Epoch: 1980 | MAE Train Loss: 0.09123730659484863 | MAE Test Loss: 0.23122933506965637 \n",
      "Epoch: 1990 | MAE Train Loss: 0.09053032100200653 | MAE Test Loss: 0.23014101386070251 \n",
      "Epoch: 2000 | MAE Train Loss: 0.08982332795858383 | MAE Test Loss: 0.22905269265174866 \n",
      "Epoch: 2010 | MAE Train Loss: 0.08911634981632233 | MAE Test Loss: 0.22796443104743958 \n",
      "Epoch: 2020 | MAE Train Loss: 0.08840936422348022 | MAE Test Loss: 0.22687609493732452 \n",
      "Epoch: 2030 | MAE Train Loss: 0.08770237863063812 | MAE Test Loss: 0.22578778862953186 \n",
      "Epoch: 2040 | MAE Train Loss: 0.08699538558721542 | MAE Test Loss: 0.2246994972229004 \n",
      "Epoch: 2050 | MAE Train Loss: 0.08630740642547607 | MAE Test Loss: 0.22362756729125977 \n",
      "Epoch: 2060 | MAE Train Loss: 0.08567677438259125 | MAE Test Loss: 0.22259381413459778 \n",
      "Epoch: 2070 | MAE Train Loss: 0.08504614233970642 | MAE Test Loss: 0.22156009078025818 \n",
      "Epoch: 2080 | MAE Train Loss: 0.0844155102968216 | MAE Test Loss: 0.22052636742591858 \n",
      "Epoch: 2090 | MAE Train Loss: 0.08378488570451736 | MAE Test Loss: 0.21949262917041779 \n",
      "Epoch: 2100 | MAE Train Loss: 0.08315424621105194 | MAE Test Loss: 0.21845892071723938 \n",
      "Epoch: 2110 | MAE Train Loss: 0.08252362161874771 | MAE Test Loss: 0.2174251526594162 \n",
      "Epoch: 2120 | MAE Train Loss: 0.08189298212528229 | MAE Test Loss: 0.2163914442062378 \n",
      "Epoch: 2130 | MAE Train Loss: 0.08130882680416107 | MAE Test Loss: 0.2153964787721634 \n",
      "Epoch: 2140 | MAE Train Loss: 0.08075019717216492 | MAE Test Loss: 0.21441809833049774 \n",
      "Epoch: 2150 | MAE Train Loss: 0.08019156754016876 | MAE Test Loss: 0.21343974769115448 \n",
      "Epoch: 2160 | MAE Train Loss: 0.07963293045759201 | MAE Test Loss: 0.21246139705181122 \n",
      "Epoch: 2170 | MAE Train Loss: 0.07907428592443466 | MAE Test Loss: 0.21148303151130676 \n",
      "Epoch: 2180 | MAE Train Loss: 0.07851565629243851 | MAE Test Loss: 0.2105046808719635 \n",
      "Epoch: 2190 | MAE Train Loss: 0.07795701920986176 | MAE Test Loss: 0.20952634513378143 \n",
      "Epoch: 2200 | MAE Train Loss: 0.077398382127285 | MAE Test Loss: 0.20854797959327698 \n",
      "Epoch: 2210 | MAE Train Loss: 0.07688353955745697 | MAE Test Loss: 0.20760893821716309 \n",
      "Epoch: 2220 | MAE Train Loss: 0.07639250159263611 | MAE Test Loss: 0.20668676495552063 \n",
      "Epoch: 2230 | MAE Train Loss: 0.07590147852897644 | MAE Test Loss: 0.2057645618915558 \n",
      "Epoch: 2240 | MAE Train Loss: 0.07541044056415558 | MAE Test Loss: 0.20484237372875214 \n",
      "Epoch: 2250 | MAE Train Loss: 0.07491941750049591 | MAE Test Loss: 0.2039201706647873 \n",
      "Epoch: 2260 | MAE Train Loss: 0.07442837953567505 | MAE Test Loss: 0.20299801230430603 \n",
      "Epoch: 2270 | MAE Train Loss: 0.07393734157085419 | MAE Test Loss: 0.2020758092403412 \n",
      "Epoch: 2280 | MAE Train Loss: 0.07344631105661392 | MAE Test Loss: 0.20115363597869873 \n",
      "Epoch: 2290 | MAE Train Loss: 0.0729697197675705 | MAE Test Loss: 0.2002486288547516 \n",
      "Epoch: 2300 | MAE Train Loss: 0.0725419670343399 | MAE Test Loss: 0.19938364624977112 \n",
      "Epoch: 2310 | MAE Train Loss: 0.07211421430110931 | MAE Test Loss: 0.19851867854595184 \n",
      "Epoch: 2320 | MAE Train Loss: 0.07168644666671753 | MAE Test Loss: 0.19765372574329376 \n",
      "Epoch: 2330 | MAE Train Loss: 0.07125870883464813 | MAE Test Loss: 0.19678878784179688 \n",
      "Epoch: 2340 | MAE Train Loss: 0.07083095610141754 | MAE Test Loss: 0.1959238052368164 \n",
      "Epoch: 2350 | MAE Train Loss: 0.07040319591760635 | MAE Test Loss: 0.19505885243415833 \n",
      "Epoch: 2360 | MAE Train Loss: 0.06997544318437576 | MAE Test Loss: 0.19419388473033905 \n",
      "Epoch: 2370 | MAE Train Loss: 0.06954768300056458 | MAE Test Loss: 0.19332894682884216 \n",
      "Epoch: 2380 | MAE Train Loss: 0.06913762539625168 | MAE Test Loss: 0.19248707592487335 \n",
      "Epoch: 2390 | MAE Train Loss: 0.06876852363348007 | MAE Test Loss: 0.19167983531951904 \n",
      "Epoch: 2400 | MAE Train Loss: 0.06839941442012787 | MAE Test Loss: 0.19087259471416473 \n",
      "Epoch: 2410 | MAE Train Loss: 0.06803031265735626 | MAE Test Loss: 0.19006536900997162 \n",
      "Epoch: 2420 | MAE Train Loss: 0.06766121089458466 | MAE Test Loss: 0.1892581284046173 \n",
      "Epoch: 2430 | MAE Train Loss: 0.06729210913181305 | MAE Test Loss: 0.1884509027004242 \n",
      "Epoch: 2440 | MAE Train Loss: 0.06692300736904144 | MAE Test Loss: 0.18764367699623108 \n",
      "Epoch: 2450 | MAE Train Loss: 0.06655389070510864 | MAE Test Loss: 0.18683645129203796 \n",
      "Epoch: 2460 | MAE Train Loss: 0.06618479639291763 | MAE Test Loss: 0.18602922558784485 \n",
      "Epoch: 2470 | MAE Train Loss: 0.06581568717956543 | MAE Test Loss: 0.18522198498249054 \n",
      "Epoch: 2480 | MAE Train Loss: 0.0654863566160202 | MAE Test Loss: 0.1844620257616043 \n",
      "Epoch: 2490 | MAE Train Loss: 0.06517163664102554 | MAE Test Loss: 0.1837138831615448 \n",
      "Epoch: 2500 | MAE Train Loss: 0.06485690176486969 | MAE Test Loss: 0.1829657256603241 \n",
      "Epoch: 2510 | MAE Train Loss: 0.06454218924045563 | MAE Test Loss: 0.18221759796142578 \n",
      "Epoch: 2520 | MAE Train Loss: 0.06422744691371918 | MAE Test Loss: 0.18146945536136627 \n",
      "Epoch: 2530 | MAE Train Loss: 0.06391273438930511 | MAE Test Loss: 0.18072131276130676 \n",
      "Epoch: 2540 | MAE Train Loss: 0.06359801441431046 | MAE Test Loss: 0.17997315526008606 \n",
      "Epoch: 2550 | MAE Train Loss: 0.0632832795381546 | MAE Test Loss: 0.17922499775886536 \n",
      "Epoch: 2560 | MAE Train Loss: 0.06296855956315994 | MAE Test Loss: 0.17847685515880585 \n",
      "Epoch: 2570 | MAE Train Loss: 0.06265383213758469 | MAE Test Loss: 0.17772871255874634 \n",
      "Epoch: 2580 | MAE Train Loss: 0.062364548444747925 | MAE Test Loss: 0.1770164966583252 \n",
      "Epoch: 2590 | MAE Train Loss: 0.06209961324930191 | MAE Test Loss: 0.17632822692394257 \n",
      "Epoch: 2600 | MAE Train Loss: 0.0618346631526947 | MAE Test Loss: 0.17563997209072113 \n",
      "Epoch: 2610 | MAE Train Loss: 0.061569731682538986 | MAE Test Loss: 0.1749517023563385 \n",
      "Epoch: 2620 | MAE Train Loss: 0.06130479648709297 | MAE Test Loss: 0.17426344752311707 \n",
      "Epoch: 2630 | MAE Train Loss: 0.061039846390485764 | MAE Test Loss: 0.17357519268989563 \n",
      "Epoch: 2640 | MAE Train Loss: 0.06077491492033005 | MAE Test Loss: 0.1728869378566742 \n",
      "Epoch: 2650 | MAE Train Loss: 0.060509972274303436 | MAE Test Loss: 0.17219865322113037 \n",
      "Epoch: 2660 | MAE Train Loss: 0.06024503707885742 | MAE Test Loss: 0.17151038348674774 \n",
      "Epoch: 2670 | MAE Train Loss: 0.05998010188341141 | MAE Test Loss: 0.1708221435546875 \n",
      "Epoch: 2680 | MAE Train Loss: 0.0597151517868042 | MAE Test Loss: 0.17013385891914368 \n",
      "Epoch: 2690 | MAE Train Loss: 0.05946909636259079 | MAE Test Loss: 0.16947592794895172 \n",
      "Epoch: 2700 | MAE Train Loss: 0.05924928933382034 | MAE Test Loss: 0.16884835064411163 \n",
      "Epoch: 2710 | MAE Train Loss: 0.059029471129179 | MAE Test Loss: 0.16822075843811035 \n",
      "Epoch: 2720 | MAE Train Loss: 0.05880966782569885 | MAE Test Loss: 0.16759316623210907 \n",
      "Epoch: 2730 | MAE Train Loss: 0.05858985707163811 | MAE Test Loss: 0.16696560382843018 \n",
      "Epoch: 2740 | MAE Train Loss: 0.05837004631757736 | MAE Test Loss: 0.1663379967212677 \n",
      "Epoch: 2750 | MAE Train Loss: 0.05815023183822632 | MAE Test Loss: 0.1657104194164276 \n",
      "Epoch: 2760 | MAE Train Loss: 0.05793040990829468 | MAE Test Loss: 0.16508284211158752 \n",
      "Epoch: 2770 | MAE Train Loss: 0.05771061033010483 | MAE Test Loss: 0.16445522010326385 \n",
      "Epoch: 2780 | MAE Train Loss: 0.05749080330133438 | MAE Test Loss: 0.16382765769958496 \n",
      "Epoch: 2790 | MAE Train Loss: 0.057270992547273636 | MAE Test Loss: 0.16320006549358368 \n",
      "Epoch: 2800 | MAE Train Loss: 0.057051174342632294 | MAE Test Loss: 0.1625724881887436 \n",
      "Epoch: 2810 | MAE Train Loss: 0.0568438284099102 | MAE Test Loss: 0.16196948289871216 \n",
      "Epoch: 2820 | MAE Train Loss: 0.056664418429136276 | MAE Test Loss: 0.1614033430814743 \n",
      "Epoch: 2830 | MAE Train Loss: 0.05648501589894295 | MAE Test Loss: 0.16083717346191406 \n",
      "Epoch: 2840 | MAE Train Loss: 0.05630560591816902 | MAE Test Loss: 0.1602710485458374 \n",
      "Epoch: 2850 | MAE Train Loss: 0.0561261884868145 | MAE Test Loss: 0.15970489382743835 \n",
      "Epoch: 2860 | MAE Train Loss: 0.05594678595662117 | MAE Test Loss: 0.1591387540102005 \n",
      "Epoch: 2870 | MAE Train Loss: 0.05576737970113754 | MAE Test Loss: 0.15857258439064026 \n",
      "Epoch: 2880 | MAE Train Loss: 0.05558796972036362 | MAE Test Loss: 0.1580064594745636 \n",
      "Epoch: 2890 | MAE Train Loss: 0.05540855973958969 | MAE Test Loss: 0.15744030475616455 \n",
      "Epoch: 2900 | MAE Train Loss: 0.055229149758815765 | MAE Test Loss: 0.1568741500377655 \n",
      "Epoch: 2910 | MAE Train Loss: 0.05504973977804184 | MAE Test Loss: 0.15630802512168884 \n",
      "Epoch: 2920 | MAE Train Loss: 0.054870329797267914 | MAE Test Loss: 0.155741885304451 \n",
      "Epoch: 2930 | MAE Train Loss: 0.05469091981649399 | MAE Test Loss: 0.15517573058605194 \n",
      "Epoch: 2940 | MAE Train Loss: 0.05451151728630066 | MAE Test Loss: 0.1546095907688141 \n",
      "Epoch: 2950 | MAE Train Loss: 0.05436427518725395 | MAE Test Loss: 0.15410597622394562 \n",
      "Epoch: 2960 | MAE Train Loss: 0.05422055721282959 | MAE Test Loss: 0.15360236167907715 \n",
      "Epoch: 2970 | MAE Train Loss: 0.05407685041427612 | MAE Test Loss: 0.15309874713420868 \n",
      "Epoch: 2980 | MAE Train Loss: 0.05393313616514206 | MAE Test Loss: 0.1525951325893402 \n",
      "Epoch: 2990 | MAE Train Loss: 0.05378942936658859 | MAE Test Loss: 0.15209153294563293 \n",
      "Epoch: 3000 | MAE Train Loss: 0.05364571884274483 | MAE Test Loss: 0.15158791840076447 \n",
      "Epoch: 3010 | MAE Train Loss: 0.053502004593610764 | MAE Test Loss: 0.1510842889547348 \n",
      "Epoch: 3020 | MAE Train Loss: 0.053358305245637894 | MAE Test Loss: 0.15058067440986633 \n",
      "Epoch: 3030 | MAE Train Loss: 0.05321459099650383 | MAE Test Loss: 0.15007705986499786 \n",
      "Epoch: 3040 | MAE Train Loss: 0.05307087302207947 | MAE Test Loss: 0.14957347512245178 \n",
      "Epoch: 3050 | MAE Train Loss: 0.0529271736741066 | MAE Test Loss: 0.1490698605775833 \n",
      "Epoch: 3060 | MAE Train Loss: 0.052783459424972534 | MAE Test Loss: 0.14856624603271484 \n",
      "Epoch: 3070 | MAE Train Loss: 0.05263974517583847 | MAE Test Loss: 0.14806261658668518 \n",
      "Epoch: 3080 | MAE Train Loss: 0.052496038377285004 | MAE Test Loss: 0.1475590169429779 \n",
      "Epoch: 3090 | MAE Train Loss: 0.05235233157873154 | MAE Test Loss: 0.14705541729927063 \n",
      "Epoch: 3100 | MAE Train Loss: 0.05223175883293152 | MAE Test Loss: 0.14660246670246124 \n",
      "Epoch: 3110 | MAE Train Loss: 0.05211885645985603 | MAE Test Loss: 0.14616218209266663 \n",
      "Epoch: 3120 | MAE Train Loss: 0.052005965262651443 | MAE Test Loss: 0.14572188258171082 \n",
      "Epoch: 3130 | MAE Train Loss: 0.05189306288957596 | MAE Test Loss: 0.1452815979719162 \n",
      "Epoch: 3140 | MAE Train Loss: 0.05178016424179077 | MAE Test Loss: 0.14484134316444397 \n",
      "Epoch: 3150 | MAE Train Loss: 0.051667265594005585 | MAE Test Loss: 0.14440104365348816 \n",
      "Epoch: 3160 | MAE Train Loss: 0.051554370671510696 | MAE Test Loss: 0.14396075904369354 \n",
      "Epoch: 3170 | MAE Train Loss: 0.05144147202372551 | MAE Test Loss: 0.14352047443389893 \n",
      "Epoch: 3180 | MAE Train Loss: 0.05132858082652092 | MAE Test Loss: 0.1430802047252655 \n",
      "Epoch: 3190 | MAE Train Loss: 0.051215678453445435 | MAE Test Loss: 0.1426399201154709 \n",
      "Epoch: 3200 | MAE Train Loss: 0.051102787256240845 | MAE Test Loss: 0.14219963550567627 \n",
      "Epoch: 3210 | MAE Train Loss: 0.05098988488316536 | MAE Test Loss: 0.14175935089588165 \n",
      "Epoch: 3220 | MAE Train Loss: 0.05087698623538017 | MAE Test Loss: 0.14131906628608704 \n",
      "Epoch: 3230 | MAE Train Loss: 0.05076408386230469 | MAE Test Loss: 0.14087876677513123 \n",
      "Epoch: 3240 | MAE Train Loss: 0.0506511926651001 | MAE Test Loss: 0.140438511967659 \n",
      "Epoch: 3250 | MAE Train Loss: 0.05053829401731491 | MAE Test Loss: 0.13999824225902557 \n",
      "Epoch: 3260 | MAE Train Loss: 0.05042539909482002 | MAE Test Loss: 0.13955795764923096 \n",
      "Epoch: 3270 | MAE Train Loss: 0.050323016941547394 | MAE Test Loss: 0.13914987444877625 \n",
      "Epoch: 3280 | MAE Train Loss: 0.0502360537648201 | MAE Test Loss: 0.13877396285533905 \n",
      "Epoch: 3290 | MAE Train Loss: 0.050149060785770416 | MAE Test Loss: 0.13839808106422424 \n",
      "Epoch: 3300 | MAE Train Loss: 0.05006207898259163 | MAE Test Loss: 0.13802221417427063 \n",
      "Epoch: 3310 | MAE Train Loss: 0.04997509717941284 | MAE Test Loss: 0.13764628767967224 \n",
      "Epoch: 3320 | MAE Train Loss: 0.04988812282681465 | MAE Test Loss: 0.13727042078971863 \n",
      "Epoch: 3330 | MAE Train Loss: 0.04980113357305527 | MAE Test Loss: 0.13689450919628143 \n",
      "Epoch: 3340 | MAE Train Loss: 0.04971415922045708 | MAE Test Loss: 0.13651862740516663 \n",
      "Epoch: 3350 | MAE Train Loss: 0.04962717741727829 | MAE Test Loss: 0.13614273071289062 \n",
      "Epoch: 3360 | MAE Train Loss: 0.0495401993393898 | MAE Test Loss: 0.135766863822937 \n",
      "Epoch: 3370 | MAE Train Loss: 0.049453213810920715 | MAE Test Loss: 0.13539095222949982 \n",
      "Epoch: 3380 | MAE Train Loss: 0.04936624690890312 | MAE Test Loss: 0.13501505553722382 \n",
      "Epoch: 3390 | MAE Train Loss: 0.04927925392985344 | MAE Test Loss: 0.134639173746109 \n",
      "Epoch: 3400 | MAE Train Loss: 0.04919227212667465 | MAE Test Loss: 0.1342633068561554 \n",
      "Epoch: 3410 | MAE Train Loss: 0.04910529404878616 | MAE Test Loss: 0.133887380361557 \n",
      "Epoch: 3420 | MAE Train Loss: 0.049018315970897675 | MAE Test Loss: 0.1335115134716034 \n",
      "Epoch: 3430 | MAE Train Loss: 0.04893132671713829 | MAE Test Loss: 0.1331356018781662 \n",
      "Epoch: 3440 | MAE Train Loss: 0.0488443486392498 | MAE Test Loss: 0.1327597200870514 \n",
      "Epoch: 3450 | MAE Train Loss: 0.04875737428665161 | MAE Test Loss: 0.1323838233947754 \n",
      "Epoch: 3460 | MAE Train Loss: 0.048670392483472824 | MAE Test Loss: 0.13200794160366058 \n",
      "Epoch: 3470 | MAE Train Loss: 0.04858340695500374 | MAE Test Loss: 0.13163205981254578 \n",
      "Epoch: 3480 | MAE Train Loss: 0.04851120337843895 | MAE Test Loss: 0.13130804896354675 \n",
      "Epoch: 3490 | MAE Train Loss: 0.04844503849744797 | MAE Test Loss: 0.13099706172943115 \n",
      "Epoch: 3500 | MAE Train Loss: 0.04837886989116669 | MAE Test Loss: 0.13068607449531555 \n",
      "Epoch: 3510 | MAE Train Loss: 0.048312705010175705 | MAE Test Loss: 0.13037505745887756 \n",
      "Epoch: 3520 | MAE Train Loss: 0.04824654385447502 | MAE Test Loss: 0.13006405532360077 \n",
      "Epoch: 3530 | MAE Train Loss: 0.04818037897348404 | MAE Test Loss: 0.12975306808948517 \n",
      "Epoch: 3540 | MAE Train Loss: 0.04811421036720276 | MAE Test Loss: 0.12944205105304718 \n",
      "Epoch: 3550 | MAE Train Loss: 0.04804804176092148 | MAE Test Loss: 0.12913104891777039 \n",
      "Epoch: 3560 | MAE Train Loss: 0.047981880605220795 | MAE Test Loss: 0.1288200318813324 \n",
      "Epoch: 3570 | MAE Train Loss: 0.047915711998939514 | MAE Test Loss: 0.1285090446472168 \n",
      "Epoch: 3580 | MAE Train Loss: 0.047849543392658234 | MAE Test Loss: 0.1281980574131012 \n",
      "Epoch: 3590 | MAE Train Loss: 0.04778338223695755 | MAE Test Loss: 0.1278870403766632 \n",
      "Epoch: 3600 | MAE Train Loss: 0.047717221081256866 | MAE Test Loss: 0.1275760382413864 \n",
      "Epoch: 3610 | MAE Train Loss: 0.047651052474975586 | MAE Test Loss: 0.1272650510072708 \n",
      "Epoch: 3620 | MAE Train Loss: 0.04758488014340401 | MAE Test Loss: 0.12695403397083282 \n",
      "Epoch: 3630 | MAE Train Loss: 0.04751871898770332 | MAE Test Loss: 0.12664303183555603 \n",
      "Epoch: 3640 | MAE Train Loss: 0.04745255410671234 | MAE Test Loss: 0.12633201479911804 \n",
      "Epoch: 3650 | MAE Train Loss: 0.04738638550043106 | MAE Test Loss: 0.12602102756500244 \n",
      "Epoch: 3660 | MAE Train Loss: 0.04732022061944008 | MAE Test Loss: 0.12571004033088684 \n",
      "Epoch: 3670 | MAE Train Loss: 0.0472540557384491 | MAE Test Loss: 0.12539902329444885 \n",
      "Epoch: 3680 | MAE Train Loss: 0.04718789458274841 | MAE Test Loss: 0.12508802115917206 \n",
      "Epoch: 3690 | MAE Train Loss: 0.04712172597646713 | MAE Test Loss: 0.12477703392505646 \n",
      "Epoch: 3700 | MAE Train Loss: 0.047055553644895554 | MAE Test Loss: 0.12446601688861847 \n",
      "Epoch: 3710 | MAE Train Loss: 0.04698939248919487 | MAE Test Loss: 0.12415502220392227 \n",
      "Epoch: 3720 | MAE Train Loss: 0.04692322760820389 | MAE Test Loss: 0.12384400516748428 \n",
      "Epoch: 3730 | MAE Train Loss: 0.046857062727212906 | MAE Test Loss: 0.12353301048278809 \n",
      "Epoch: 3740 | MAE Train Loss: 0.04680521786212921 | MAE Test Loss: 0.12328799068927765 \n",
      "Epoch: 3750 | MAE Train Loss: 0.04675474762916565 | MAE Test Loss: 0.12304297834634781 \n",
      "Epoch: 3760 | MAE Train Loss: 0.04670426994562149 | MAE Test Loss: 0.12279794365167618 \n",
      "Epoch: 3770 | MAE Train Loss: 0.04665379226207733 | MAE Test Loss: 0.12255294620990753 \n",
      "Epoch: 3780 | MAE Train Loss: 0.04660331457853317 | MAE Test Loss: 0.1223079189658165 \n",
      "Epoch: 3790 | MAE Train Loss: 0.04655285179615021 | MAE Test Loss: 0.12206289917230606 \n",
      "Epoch: 3800 | MAE Train Loss: 0.04650237038731575 | MAE Test Loss: 0.12181787192821503 \n",
      "Epoch: 3810 | MAE Train Loss: 0.04645188897848129 | MAE Test Loss: 0.12157285213470459 \n",
      "Epoch: 3820 | MAE Train Loss: 0.04640141874551773 | MAE Test Loss: 0.12132783234119415 \n",
      "Epoch: 3830 | MAE Train Loss: 0.04635094851255417 | MAE Test Loss: 0.12108281999826431 \n",
      "Epoch: 3840 | MAE Train Loss: 0.04630047455430031 | MAE Test Loss: 0.12083778530359268 \n",
      "Epoch: 3850 | MAE Train Loss: 0.04624999314546585 | MAE Test Loss: 0.12059278786182404 \n",
      "Epoch: 3860 | MAE Train Loss: 0.04619951918721199 | MAE Test Loss: 0.120347760617733 \n",
      "Epoch: 3870 | MAE Train Loss: 0.04614905267953873 | MAE Test Loss: 0.12010274082422256 \n",
      "Epoch: 3880 | MAE Train Loss: 0.04609857127070427 | MAE Test Loss: 0.11985771358013153 \n",
      "Epoch: 3890 | MAE Train Loss: 0.04604809358716011 | MAE Test Loss: 0.1196126937866211 \n",
      "Epoch: 3900 | MAE Train Loss: 0.04599762335419655 | MAE Test Loss: 0.11936767399311066 \n",
      "Epoch: 3910 | MAE Train Loss: 0.045947153121232986 | MAE Test Loss: 0.11912266165018082 \n",
      "Epoch: 3920 | MAE Train Loss: 0.04589667543768883 | MAE Test Loss: 0.11887762695550919 \n",
      "Epoch: 3930 | MAE Train Loss: 0.04584619775414467 | MAE Test Loss: 0.11863262951374054 \n",
      "Epoch: 3940 | MAE Train Loss: 0.04579572007060051 | MAE Test Loss: 0.1183876022696495 \n",
      "Epoch: 3950 | MAE Train Loss: 0.045745257288217545 | MAE Test Loss: 0.11814258247613907 \n",
      "Epoch: 3960 | MAE Train Loss: 0.04569477587938309 | MAE Test Loss: 0.11789755523204803 \n",
      "Epoch: 3970 | MAE Train Loss: 0.04564429447054863 | MAE Test Loss: 0.1176525354385376 \n",
      "Epoch: 3980 | MAE Train Loss: 0.04559382423758507 | MAE Test Loss: 0.11740751564502716 \n",
      "Epoch: 3990 | MAE Train Loss: 0.045543354004621506 | MAE Test Loss: 0.11716250330209732 \n",
      "Epoch: 4000 | MAE Train Loss: 0.045492880046367645 | MAE Test Loss: 0.11691746860742569 \n",
      "Epoch: 4010 | MAE Train Loss: 0.04544239863753319 | MAE Test Loss: 0.11667247116565704 \n",
      "Epoch: 4020 | MAE Train Loss: 0.04539192467927933 | MAE Test Loss: 0.11642744392156601 \n",
      "Epoch: 4030 | MAE Train Loss: 0.045341458171606064 | MAE Test Loss: 0.11618242412805557 \n",
      "Epoch: 4040 | MAE Train Loss: 0.045290976762771606 | MAE Test Loss: 0.11593739688396454 \n",
      "Epoch: 4050 | MAE Train Loss: 0.04524049907922745 | MAE Test Loss: 0.1156923770904541 \n",
      "Epoch: 4060 | MAE Train Loss: 0.045190028846263885 | MAE Test Loss: 0.11544735729694366 \n",
      "Epoch: 4070 | MAE Train Loss: 0.045139558613300323 | MAE Test Loss: 0.11520234495401382 \n",
      "Epoch: 4080 | MAE Train Loss: 0.045089080929756165 | MAE Test Loss: 0.1149573102593422 \n",
      "Epoch: 4090 | MAE Train Loss: 0.045048438012599945 | MAE Test Loss: 0.11477935314178467 \n",
      "Epoch: 4100 | MAE Train Loss: 0.04500844329595566 | MAE Test Loss: 0.11460139602422714 \n",
      "Epoch: 4110 | MAE Train Loss: 0.04496845230460167 | MAE Test Loss: 0.11442339420318604 \n",
      "Epoch: 4120 | MAE Train Loss: 0.044928453862667084 | MAE Test Loss: 0.11424541473388672 \n",
      "Epoch: 4130 | MAE Train Loss: 0.0448884591460228 | MAE Test Loss: 0.1140674352645874 \n",
      "Epoch: 4140 | MAE Train Loss: 0.04484846070408821 | MAE Test Loss: 0.11388945579528809 \n",
      "Epoch: 4150 | MAE Train Loss: 0.04480846971273422 | MAE Test Loss: 0.11371149122714996 \n",
      "Epoch: 4160 | MAE Train Loss: 0.04476846754550934 | MAE Test Loss: 0.11353351920843124 \n",
      "Epoch: 4170 | MAE Train Loss: 0.04472848027944565 | MAE Test Loss: 0.11335553973913193 \n",
      "Epoch: 4180 | MAE Train Loss: 0.04468848556280136 | MAE Test Loss: 0.11317756026983261 \n",
      "Epoch: 4190 | MAE Train Loss: 0.044648490846157074 | MAE Test Loss: 0.1129995808005333 \n",
      "Epoch: 4200 | MAE Train Loss: 0.04460849612951279 | MAE Test Loss: 0.11282160133123398 \n",
      "Epoch: 4210 | MAE Train Loss: 0.0445685014128685 | MAE Test Loss: 0.11264362186193466 \n",
      "Epoch: 4220 | MAE Train Loss: 0.04452850669622421 | MAE Test Loss: 0.11246565729379654 \n",
      "Epoch: 4230 | MAE Train Loss: 0.04448850825428963 | MAE Test Loss: 0.11228767782449722 \n",
      "Epoch: 4240 | MAE Train Loss: 0.04444851726293564 | MAE Test Loss: 0.1121096983551979 \n",
      "Epoch: 4250 | MAE Train Loss: 0.04440852254629135 | MAE Test Loss: 0.11193172633647919 \n",
      "Epoch: 4260 | MAE Train Loss: 0.044368527829647064 | MAE Test Loss: 0.11175373941659927 \n",
      "Epoch: 4270 | MAE Train Loss: 0.04432853311300278 | MAE Test Loss: 0.11157576739788055 \n",
      "Epoch: 4280 | MAE Train Loss: 0.04428853839635849 | MAE Test Loss: 0.11139778047800064 \n",
      "Epoch: 4290 | MAE Train Loss: 0.0442485436797142 | MAE Test Loss: 0.11121980845928192 \n",
      "Epoch: 4300 | MAE Train Loss: 0.044208552688360214 | MAE Test Loss: 0.11104185879230499 \n",
      "Epoch: 4310 | MAE Train Loss: 0.04416855797171593 | MAE Test Loss: 0.11086386442184448 \n",
      "Epoch: 4320 | MAE Train Loss: 0.04412855952978134 | MAE Test Loss: 0.11068589985370636 \n",
      "Epoch: 4330 | MAE Train Loss: 0.044088564813137054 | MAE Test Loss: 0.11050790548324585 \n",
      "Epoch: 4340 | MAE Train Loss: 0.04404856264591217 | MAE Test Loss: 0.11032994091510773 \n",
      "Epoch: 4350 | MAE Train Loss: 0.04400857537984848 | MAE Test Loss: 0.11015196144580841 \n",
      "Epoch: 4360 | MAE Train Loss: 0.043968576937913895 | MAE Test Loss: 0.1099739819765091 \n",
      "Epoch: 4370 | MAE Train Loss: 0.043928585946559906 | MAE Test Loss: 0.10979600250720978 \n",
      "Epoch: 4380 | MAE Train Loss: 0.04388859122991562 | MAE Test Loss: 0.10961802303791046 \n",
      "Epoch: 4390 | MAE Train Loss: 0.04384859651327133 | MAE Test Loss: 0.10944006592035294 \n",
      "Epoch: 4400 | MAE Train Loss: 0.04380860924720764 | MAE Test Loss: 0.10926208645105362 \n",
      "Epoch: 4410 | MAE Train Loss: 0.04376860707998276 | MAE Test Loss: 0.10908409208059311 \n",
      "Epoch: 4420 | MAE Train Loss: 0.04372861608862877 | MAE Test Loss: 0.10890612751245499 \n",
      "Epoch: 4430 | MAE Train Loss: 0.043688613921403885 | MAE Test Loss: 0.10872814804315567 \n",
      "Epoch: 4440 | MAE Train Loss: 0.043648622930049896 | MAE Test Loss: 0.10855016857385635 \n",
      "Epoch: 4450 | MAE Train Loss: 0.04360862821340561 | MAE Test Loss: 0.10837218910455704 \n",
      "Epoch: 4460 | MAE Train Loss: 0.04356863722205162 | MAE Test Loss: 0.10819420963525772 \n",
      "Epoch: 4470 | MAE Train Loss: 0.043528638780117035 | MAE Test Loss: 0.1080162525177002 \n",
      "Epoch: 4480 | MAE Train Loss: 0.04348864406347275 | MAE Test Loss: 0.10783825069665909 \n",
      "Epoch: 4490 | MAE Train Loss: 0.04344864934682846 | MAE Test Loss: 0.10766029357910156 \n",
      "Epoch: 4500 | MAE Train Loss: 0.043408654630184174 | MAE Test Loss: 0.10748233646154404 \n",
      "Epoch: 4510 | MAE Train Loss: 0.043368663638830185 | MAE Test Loss: 0.10730433464050293 \n",
      "Epoch: 4520 | MAE Train Loss: 0.0433286651968956 | MAE Test Loss: 0.10712635517120361 \n",
      "Epoch: 4530 | MAE Train Loss: 0.04328867048025131 | MAE Test Loss: 0.1069483757019043 \n",
      "Epoch: 4540 | MAE Train Loss: 0.04324867203831673 | MAE Test Loss: 0.10677039623260498 \n",
      "Epoch: 4550 | MAE Train Loss: 0.04320868104696274 | MAE Test Loss: 0.10659243911504745 \n",
      "Epoch: 4560 | MAE Train Loss: 0.043168678879737854 | MAE Test Loss: 0.10641445964574814 \n",
      "Epoch: 4570 | MAE Train Loss: 0.043128691613674164 | MAE Test Loss: 0.10623648017644882 \n",
      "Epoch: 4580 | MAE Train Loss: 0.04308869689702988 | MAE Test Loss: 0.1060585007071495 \n",
      "Epoch: 4590 | MAE Train Loss: 0.04304870218038559 | MAE Test Loss: 0.10588052123785019 \n",
      "Epoch: 4600 | MAE Train Loss: 0.0430087074637413 | MAE Test Loss: 0.10570255666971207 \n",
      "Epoch: 4610 | MAE Train Loss: 0.042968712747097015 | MAE Test Loss: 0.10552456229925156 \n",
      "Epoch: 4620 | MAE Train Loss: 0.04292871803045273 | MAE Test Loss: 0.10534659773111343 \n",
      "Epoch: 4630 | MAE Train Loss: 0.04288871958851814 | MAE Test Loss: 0.10516861826181412 \n",
      "Epoch: 4640 | MAE Train Loss: 0.042848728597164154 | MAE Test Loss: 0.1049906387925148 \n",
      "Epoch: 4650 | MAE Train Loss: 0.042811933904886246 | MAE Test Loss: 0.10486016422510147 \n",
      "Epoch: 4660 | MAE Train Loss: 0.04277702793478966 | MAE Test Loss: 0.10475001484155655 \n",
      "Epoch: 4670 | MAE Train Loss: 0.04274212196469307 | MAE Test Loss: 0.10463988780975342 \n",
      "Epoch: 4680 | MAE Train Loss: 0.04270721971988678 | MAE Test Loss: 0.1045297384262085 \n",
      "Epoch: 4690 | MAE Train Loss: 0.04267231374979019 | MAE Test Loss: 0.10441961139440536 \n",
      "Epoch: 4700 | MAE Train Loss: 0.0426374115049839 | MAE Test Loss: 0.10430946201086044 \n",
      "Epoch: 4710 | MAE Train Loss: 0.042602505534887314 | MAE Test Loss: 0.10419933497905731 \n",
      "Epoch: 4720 | MAE Train Loss: 0.042567599564790726 | MAE Test Loss: 0.10408918559551239 \n",
      "Epoch: 4730 | MAE Train Loss: 0.042532697319984436 | MAE Test Loss: 0.10397906601428986 \n",
      "Epoch: 4740 | MAE Train Loss: 0.04249779134988785 | MAE Test Loss: 0.10386891663074493 \n",
      "Epoch: 4750 | MAE Train Loss: 0.04246288910508156 | MAE Test Loss: 0.1037587895989418 \n",
      "Epoch: 4760 | MAE Train Loss: 0.04242798313498497 | MAE Test Loss: 0.10364864021539688 \n",
      "Epoch: 4770 | MAE Train Loss: 0.04239307716488838 | MAE Test Loss: 0.10353851318359375 \n",
      "Epoch: 4780 | MAE Train Loss: 0.04235817492008209 | MAE Test Loss: 0.10342836380004883 \n",
      "Epoch: 4790 | MAE Train Loss: 0.042323268949985504 | MAE Test Loss: 0.1033182367682457 \n",
      "Epoch: 4800 | MAE Train Loss: 0.042288366705179214 | MAE Test Loss: 0.10320808738470078 \n",
      "Epoch: 4810 | MAE Train Loss: 0.042253460735082626 | MAE Test Loss: 0.10309796035289764 \n",
      "Epoch: 4820 | MAE Train Loss: 0.04221855476498604 | MAE Test Loss: 0.10298781096935272 \n",
      "Epoch: 4830 | MAE Train Loss: 0.04218365252017975 | MAE Test Loss: 0.10287769138813019 \n",
      "Epoch: 4840 | MAE Train Loss: 0.04214874655008316 | MAE Test Loss: 0.10276754200458527 \n",
      "Epoch: 4850 | MAE Train Loss: 0.04211384430527687 | MAE Test Loss: 0.10265741497278214 \n",
      "Epoch: 4860 | MAE Train Loss: 0.04207893833518028 | MAE Test Loss: 0.10254726558923721 \n",
      "Epoch: 4870 | MAE Train Loss: 0.042044032365083694 | MAE Test Loss: 0.10243713855743408 \n",
      "Epoch: 4880 | MAE Train Loss: 0.042009130120277405 | MAE Test Loss: 0.10232698917388916 \n",
      "Epoch: 4890 | MAE Train Loss: 0.04197422415018082 | MAE Test Loss: 0.10221686214208603 \n",
      "Epoch: 4900 | MAE Train Loss: 0.04193932190537453 | MAE Test Loss: 0.10210671275854111 \n",
      "Epoch: 4910 | MAE Train Loss: 0.04190441593527794 | MAE Test Loss: 0.10199658572673798 \n",
      "Epoch: 4920 | MAE Train Loss: 0.04186950996518135 | MAE Test Loss: 0.10188643634319305 \n",
      "Epoch: 4930 | MAE Train Loss: 0.04183460772037506 | MAE Test Loss: 0.10177631676197052 \n",
      "Epoch: 4940 | MAE Train Loss: 0.04179970175027847 | MAE Test Loss: 0.1016661673784256 \n",
      "Epoch: 4950 | MAE Train Loss: 0.04176479950547218 | MAE Test Loss: 0.10155604034662247 \n",
      "Epoch: 4960 | MAE Train Loss: 0.041729893535375595 | MAE Test Loss: 0.10144589096307755 \n",
      "Epoch: 4970 | MAE Train Loss: 0.04169498756527901 | MAE Test Loss: 0.10133576393127441 \n",
      "Epoch: 4980 | MAE Train Loss: 0.04166008532047272 | MAE Test Loss: 0.10122561454772949 \n",
      "Epoch: 4990 | MAE Train Loss: 0.04162517935037613 | MAE Test Loss: 0.10111548751592636 \n",
      "Epoch: 5000 | MAE Train Loss: 0.04159027710556984 | MAE Test Loss: 0.10100533813238144 \n",
      "Epoch: 5010 | MAE Train Loss: 0.04155537113547325 | MAE Test Loss: 0.10089521110057831 \n",
      "Epoch: 5020 | MAE Train Loss: 0.04152046516537666 | MAE Test Loss: 0.10078506171703339 \n",
      "Epoch: 5030 | MAE Train Loss: 0.041485562920570374 | MAE Test Loss: 0.10067494213581085 \n",
      "Epoch: 5040 | MAE Train Loss: 0.041450656950473785 | MAE Test Loss: 0.10056479275226593 \n",
      "Epoch: 5050 | MAE Train Loss: 0.041415754705667496 | MAE Test Loss: 0.1004546657204628 \n",
      "Epoch: 5060 | MAE Train Loss: 0.04138084873557091 | MAE Test Loss: 0.10034451633691788 \n",
      "Epoch: 5070 | MAE Train Loss: 0.04134594276547432 | MAE Test Loss: 0.10023438930511475 \n",
      "Epoch: 5080 | MAE Train Loss: 0.04131104052066803 | MAE Test Loss: 0.10012423992156982 \n",
      "Epoch: 5090 | MAE Train Loss: 0.04127613455057144 | MAE Test Loss: 0.1000141128897667 \n",
      "Epoch: 5100 | MAE Train Loss: 0.04124123230576515 | MAE Test Loss: 0.09990397095680237 \n",
      "Epoch: 5110 | MAE Train Loss: 0.041206326335668564 | MAE Test Loss: 0.09979383647441864 \n",
      "Epoch: 5120 | MAE Train Loss: 0.041171420365571976 | MAE Test Loss: 0.09968369454145432 \n",
      "Epoch: 5130 | MAE Train Loss: 0.041136518120765686 | MAE Test Loss: 0.09957356005907059 \n",
      "Epoch: 5140 | MAE Train Loss: 0.0411016121506691 | MAE Test Loss: 0.09946341812610626 \n",
      "Epoch: 5150 | MAE Train Loss: 0.04106670990586281 | MAE Test Loss: 0.09935328364372253 \n",
      "Epoch: 5160 | MAE Train Loss: 0.04103180393576622 | MAE Test Loss: 0.0992431491613388 \n",
      "Epoch: 5170 | MAE Train Loss: 0.04099689796566963 | MAE Test Loss: 0.09913300722837448 \n",
      "Epoch: 5180 | MAE Train Loss: 0.04096199572086334 | MAE Test Loss: 0.09902287274599075 \n",
      "Epoch: 5190 | MAE Train Loss: 0.040927089750766754 | MAE Test Loss: 0.09891273081302643 \n",
      "Epoch: 5200 | MAE Train Loss: 0.040892187505960464 | MAE Test Loss: 0.0988025963306427 \n",
      "Epoch: 5210 | MAE Train Loss: 0.040857281535863876 | MAE Test Loss: 0.09869246184825897 \n",
      "Epoch: 5220 | MAE Train Loss: 0.04082237556576729 | MAE Test Loss: 0.09858231991529465 \n",
      "Epoch: 5230 | MAE Train Loss: 0.040787473320961 | MAE Test Loss: 0.09847218543291092 \n",
      "Epoch: 5240 | MAE Train Loss: 0.04075256735086441 | MAE Test Loss: 0.0983620434999466 \n",
      "Epoch: 5250 | MAE Train Loss: 0.04071766510605812 | MAE Test Loss: 0.09825190901756287 \n",
      "Epoch: 5260 | MAE Train Loss: 0.04068275913596153 | MAE Test Loss: 0.09814177453517914 \n",
      "Epoch: 5270 | MAE Train Loss: 0.040647853165864944 | MAE Test Loss: 0.09803163260221481 \n",
      "Epoch: 5280 | MAE Train Loss: 0.040612950921058655 | MAE Test Loss: 0.09792149811983109 \n",
      "Epoch: 5290 | MAE Train Loss: 0.04057804495096207 | MAE Test Loss: 0.09781135618686676 \n",
      "Epoch: 5300 | MAE Train Loss: 0.04054314270615578 | MAE Test Loss: 0.09770122170448303 \n",
      "Epoch: 5310 | MAE Train Loss: 0.04050823673605919 | MAE Test Loss: 0.0975910872220993 \n",
      "Epoch: 5320 | MAE Train Loss: 0.0404733307659626 | MAE Test Loss: 0.09748094528913498 \n",
      "Epoch: 5330 | MAE Train Loss: 0.04043842852115631 | MAE Test Loss: 0.09737081080675125 \n",
      "Epoch: 5340 | MAE Train Loss: 0.04040352255105972 | MAE Test Loss: 0.09726066887378693 \n",
      "Epoch: 5350 | MAE Train Loss: 0.04036862030625343 | MAE Test Loss: 0.0971505343914032 \n",
      "Epoch: 5360 | MAE Train Loss: 0.040333714336156845 | MAE Test Loss: 0.09704039990901947 \n",
      "Epoch: 5370 | MAE Train Loss: 0.04029880836606026 | MAE Test Loss: 0.09693025797605515 \n",
      "Epoch: 5380 | MAE Train Loss: 0.04026390612125397 | MAE Test Loss: 0.09682012349367142 \n",
      "Epoch: 5390 | MAE Train Loss: 0.04022900015115738 | MAE Test Loss: 0.09670998156070709 \n",
      "Epoch: 5400 | MAE Train Loss: 0.04019409790635109 | MAE Test Loss: 0.09659984707832336 \n",
      "Epoch: 5410 | MAE Train Loss: 0.0401591919362545 | MAE Test Loss: 0.09648971259593964 \n",
      "Epoch: 5420 | MAE Train Loss: 0.04012428596615791 | MAE Test Loss: 0.09637957066297531 \n",
      "Epoch: 5430 | MAE Train Loss: 0.040089383721351624 | MAE Test Loss: 0.09626946598291397 \n",
      "Epoch: 5440 | MAE Train Loss: 0.040054481476545334 | MAE Test Loss: 0.09615931659936905 \n",
      "Epoch: 5450 | MAE Train Loss: 0.040019579231739044 | MAE Test Loss: 0.09604916721582413 \n",
      "Epoch: 5460 | MAE Train Loss: 0.03998466953635216 | MAE Test Loss: 0.09593904763460159 \n",
      "Epoch: 5470 | MAE Train Loss: 0.03994976729154587 | MAE Test Loss: 0.09582890570163727 \n",
      "Epoch: 5480 | MAE Train Loss: 0.03991486877202988 | MAE Test Loss: 0.09571877866983414 \n",
      "Epoch: 5490 | MAE Train Loss: 0.03987995907664299 | MAE Test Loss: 0.09560862928628922 \n",
      "Epoch: 5500 | MAE Train Loss: 0.039845060557127 | MAE Test Loss: 0.0954984799027443 \n",
      "Epoch: 5510 | MAE Train Loss: 0.039810147136449814 | MAE Test Loss: 0.09538836032152176 \n",
      "Epoch: 5520 | MAE Train Loss: 0.039775244891643524 | MAE Test Loss: 0.09527821838855743 \n",
      "Epoch: 5530 | MAE Train Loss: 0.03974034637212753 | MAE Test Loss: 0.0951680913567543 \n",
      "Epoch: 5540 | MAE Train Loss: 0.039705436676740646 | MAE Test Loss: 0.09505794197320938 \n",
      "Epoch: 5550 | MAE Train Loss: 0.03967053443193436 | MAE Test Loss: 0.09494779258966446 \n",
      "Epoch: 5560 | MAE Train Loss: 0.03963562846183777 | MAE Test Loss: 0.09483767300844193 \n",
      "Epoch: 5570 | MAE Train Loss: 0.03960072249174118 | MAE Test Loss: 0.0947275310754776 \n",
      "Epoch: 5580 | MAE Train Loss: 0.03956582397222519 | MAE Test Loss: 0.09461740404367447 \n",
      "Epoch: 5590 | MAE Train Loss: 0.0395309142768383 | MAE Test Loss: 0.09450725466012955 \n",
      "Epoch: 5600 | MAE Train Loss: 0.03949601575732231 | MAE Test Loss: 0.09439710527658463 \n",
      "Epoch: 5610 | MAE Train Loss: 0.039461106061935425 | MAE Test Loss: 0.09428698569536209 \n",
      "Epoch: 5620 | MAE Train Loss: 0.03942620009183884 | MAE Test Loss: 0.09417684376239777 \n",
      "Epoch: 5630 | MAE Train Loss: 0.039391301572322845 | MAE Test Loss: 0.09406671673059464 \n",
      "Epoch: 5640 | MAE Train Loss: 0.03935639187693596 | MAE Test Loss: 0.09395656734704971 \n",
      "Epoch: 5650 | MAE Train Loss: 0.03932148963212967 | MAE Test Loss: 0.09384641796350479 \n",
      "Epoch: 5660 | MAE Train Loss: 0.03928658366203308 | MAE Test Loss: 0.09373629838228226 \n",
      "Epoch: 5670 | MAE Train Loss: 0.03925167769193649 | MAE Test Loss: 0.09362615644931793 \n",
      "Epoch: 5680 | MAE Train Loss: 0.0392167791724205 | MAE Test Loss: 0.0935160294175148 \n",
      "Epoch: 5690 | MAE Train Loss: 0.039181869477033615 | MAE Test Loss: 0.09340588003396988 \n",
      "Epoch: 5700 | MAE Train Loss: 0.039146970957517624 | MAE Test Loss: 0.09329573065042496 \n",
      "Epoch: 5710 | MAE Train Loss: 0.03911206126213074 | MAE Test Loss: 0.09318561106920242 \n",
      "Epoch: 5720 | MAE Train Loss: 0.03907715529203415 | MAE Test Loss: 0.0930754691362381 \n",
      "Epoch: 5730 | MAE Train Loss: 0.03904225677251816 | MAE Test Loss: 0.09296534210443497 \n",
      "Epoch: 5740 | MAE Train Loss: 0.03900734707713127 | MAE Test Loss: 0.09285519272089005 \n",
      "Epoch: 5750 | MAE Train Loss: 0.03897244483232498 | MAE Test Loss: 0.09274504333734512 \n",
      "Epoch: 5760 | MAE Train Loss: 0.038937538862228394 | MAE Test Loss: 0.09263492375612259 \n",
      "Epoch: 5770 | MAE Train Loss: 0.038902632892131805 | MAE Test Loss: 0.09252478182315826 \n",
      "Epoch: 5780 | MAE Train Loss: 0.038867734372615814 | MAE Test Loss: 0.09241465479135513 \n",
      "Epoch: 5790 | MAE Train Loss: 0.03883282467722893 | MAE Test Loss: 0.09230450540781021 \n",
      "Epoch: 5800 | MAE Train Loss: 0.038797926157712936 | MAE Test Loss: 0.09219435602426529 \n",
      "Epoch: 5810 | MAE Train Loss: 0.03876301646232605 | MAE Test Loss: 0.09208423644304276 \n",
      "Epoch: 5820 | MAE Train Loss: 0.03872811049222946 | MAE Test Loss: 0.09197409451007843 \n",
      "Epoch: 5830 | MAE Train Loss: 0.03869321197271347 | MAE Test Loss: 0.0918639674782753 \n",
      "Epoch: 5840 | MAE Train Loss: 0.038658302277326584 | MAE Test Loss: 0.09175381809473038 \n",
      "Epoch: 5850 | MAE Train Loss: 0.038623400032520294 | MAE Test Loss: 0.09164366871118546 \n",
      "Epoch: 5860 | MAE Train Loss: 0.038588494062423706 | MAE Test Loss: 0.09153354912996292 \n",
      "Epoch: 5870 | MAE Train Loss: 0.03855358809232712 | MAE Test Loss: 0.0914234071969986 \n",
      "Epoch: 5880 | MAE Train Loss: 0.03851868957281113 | MAE Test Loss: 0.09131328016519547 \n",
      "Epoch: 5890 | MAE Train Loss: 0.03848377987742424 | MAE Test Loss: 0.09120313078165054 \n",
      "Epoch: 5900 | MAE Train Loss: 0.03844888135790825 | MAE Test Loss: 0.09109298139810562 \n",
      "Epoch: 5910 | MAE Train Loss: 0.03841397166252136 | MAE Test Loss: 0.09098286181688309 \n",
      "Epoch: 5920 | MAE Train Loss: 0.038379065692424774 | MAE Test Loss: 0.09087271988391876 \n",
      "Epoch: 5930 | MAE Train Loss: 0.03834416717290878 | MAE Test Loss: 0.09076259285211563 \n",
      "Epoch: 5940 | MAE Train Loss: 0.038309257477521896 | MAE Test Loss: 0.09065244346857071 \n",
      "Epoch: 5950 | MAE Train Loss: 0.03827435523271561 | MAE Test Loss: 0.09054229408502579 \n",
      "Epoch: 5960 | MAE Train Loss: 0.03823944926261902 | MAE Test Loss: 0.09043217450380325 \n",
      "Epoch: 5970 | MAE Train Loss: 0.03820454329252243 | MAE Test Loss: 0.09032203257083893 \n",
      "Epoch: 5980 | MAE Train Loss: 0.03816964477300644 | MAE Test Loss: 0.0902119055390358 \n",
      "Epoch: 5990 | MAE Train Loss: 0.03813473507761955 | MAE Test Loss: 0.09010175615549088 \n",
      "Epoch: 6000 | MAE Train Loss: 0.03809983655810356 | MAE Test Loss: 0.08999161422252655 \n",
      "Epoch: 6010 | MAE Train Loss: 0.038064926862716675 | MAE Test Loss: 0.08988148719072342 \n",
      "Epoch: 6020 | MAE Train Loss: 0.03803002089262009 | MAE Test Loss: 0.0897713452577591 \n",
      "Epoch: 6030 | MAE Train Loss: 0.037995122373104095 | MAE Test Loss: 0.08966121822595596 \n",
      "Epoch: 6040 | MAE Train Loss: 0.03796021267771721 | MAE Test Loss: 0.08955106884241104 \n",
      "Epoch: 6050 | MAE Train Loss: 0.03792531043291092 | MAE Test Loss: 0.08944092690944672 \n",
      "Epoch: 6060 | MAE Train Loss: 0.03789040446281433 | MAE Test Loss: 0.08933079987764359 \n",
      "Epoch: 6070 | MAE Train Loss: 0.03785549849271774 | MAE Test Loss: 0.08922065794467926 \n",
      "Epoch: 6080 | MAE Train Loss: 0.03782059997320175 | MAE Test Loss: 0.08911053091287613 \n",
      "Epoch: 6090 | MAE Train Loss: 0.037785690277814865 | MAE Test Loss: 0.08900038152933121 \n",
      "Epoch: 6100 | MAE Train Loss: 0.037750791758298874 | MAE Test Loss: 0.08889023959636688 \n",
      "Epoch: 6110 | MAE Train Loss: 0.03771588206291199 | MAE Test Loss: 0.08878011256456375 \n",
      "Epoch: 6120 | MAE Train Loss: 0.0376809760928154 | MAE Test Loss: 0.08866997063159943 \n",
      "Epoch: 6130 | MAE Train Loss: 0.03764607757329941 | MAE Test Loss: 0.0885598435997963 \n",
      "Epoch: 6140 | MAE Train Loss: 0.03761116787791252 | MAE Test Loss: 0.08844969421625137 \n",
      "Epoch: 6150 | MAE Train Loss: 0.03757626563310623 | MAE Test Loss: 0.08833955228328705 \n",
      "Epoch: 6160 | MAE Train Loss: 0.037541359663009644 | MAE Test Loss: 0.08822942525148392 \n",
      "Epoch: 6170 | MAE Train Loss: 0.037506453692913055 | MAE Test Loss: 0.08811928331851959 \n",
      "Epoch: 6180 | MAE Train Loss: 0.037471555173397064 | MAE Test Loss: 0.08800915628671646 \n",
      "Epoch: 6190 | MAE Train Loss: 0.03743664547801018 | MAE Test Loss: 0.08789900690317154 \n",
      "Epoch: 6200 | MAE Train Loss: 0.037401746958494186 | MAE Test Loss: 0.08778886497020721 \n",
      "Epoch: 6210 | MAE Train Loss: 0.0373668372631073 | MAE Test Loss: 0.08767873793840408 \n",
      "Epoch: 6220 | MAE Train Loss: 0.03733193129301071 | MAE Test Loss: 0.08756859600543976 \n",
      "Epoch: 6230 | MAE Train Loss: 0.03729703277349472 | MAE Test Loss: 0.08745846897363663 \n",
      "Epoch: 6240 | MAE Train Loss: 0.037262123078107834 | MAE Test Loss: 0.0873483195900917 \n",
      "Epoch: 6250 | MAE Train Loss: 0.037227220833301544 | MAE Test Loss: 0.08723817765712738 \n",
      "Epoch: 6260 | MAE Train Loss: 0.037192314863204956 | MAE Test Loss: 0.08712805062532425 \n",
      "Epoch: 6270 | MAE Train Loss: 0.03715740889310837 | MAE Test Loss: 0.08701790869235992 \n",
      "Epoch: 6280 | MAE Train Loss: 0.03712251037359238 | MAE Test Loss: 0.0869077816605568 \n",
      "Epoch: 6290 | MAE Train Loss: 0.03708760067820549 | MAE Test Loss: 0.08679763227701187 \n",
      "Epoch: 6300 | MAE Train Loss: 0.0370527021586895 | MAE Test Loss: 0.08668749034404755 \n",
      "Epoch: 6310 | MAE Train Loss: 0.03701779246330261 | MAE Test Loss: 0.08657736331224442 \n",
      "Epoch: 6320 | MAE Train Loss: 0.036982886493206024 | MAE Test Loss: 0.08646722137928009 \n",
      "Epoch: 6330 | MAE Train Loss: 0.0369485467672348 | MAE Test Loss: 0.0863914042711258 \n",
      "Epoch: 6340 | MAE Train Loss: 0.03691420704126358 | MAE Test Loss: 0.08630871772766113 \n",
      "Epoch: 6350 | MAE Train Loss: 0.036879800260066986 | MAE Test Loss: 0.08622604608535767 \n",
      "Epoch: 6360 | MAE Train Loss: 0.03684546798467636 | MAE Test Loss: 0.08615021407604218 \n",
      "Epoch: 6370 | MAE Train Loss: 0.03681112080812454 | MAE Test Loss: 0.08606753498315811 \n",
      "Epoch: 6380 | MAE Train Loss: 0.03677671402692795 | MAE Test Loss: 0.08598484098911285 \n",
      "Epoch: 6390 | MAE Train Loss: 0.03674238920211792 | MAE Test Loss: 0.08590902388095856 \n",
      "Epoch: 6400 | MAE Train Loss: 0.036708004772663116 | MAE Test Loss: 0.08582980185747147 \n",
      "Epoch: 6410 | MAE Train Loss: 0.036673642694950104 | MAE Test Loss: 0.08574371039867401 \n",
      "Epoch: 6420 | MAE Train Loss: 0.036639317870140076 | MAE Test Loss: 0.08566790074110031 \n",
      "Epoch: 6430 | MAE Train Loss: 0.03660495951771736 | MAE Test Loss: 0.08558520674705505 \n",
      "Epoch: 6440 | MAE Train Loss: 0.036570556461811066 | MAE Test Loss: 0.0855025202035904 \n",
      "Epoch: 6450 | MAE Train Loss: 0.03653622418642044 | MAE Test Loss: 0.0854233056306839 \n",
      "Epoch: 6460 | MAE Train Loss: 0.03650186210870743 | MAE Test Loss: 0.085347481071949 \n",
      "Epoch: 6470 | MAE Train Loss: 0.036467473953962326 | MAE Test Loss: 0.08526138961315155 \n",
      "Epoch: 6480 | MAE Train Loss: 0.03643317148089409 | MAE Test Loss: 0.08518558740615845 \n",
      "Epoch: 6490 | MAE Train Loss: 0.03639879450201988 | MAE Test Loss: 0.08510288596153259 \n",
      "Epoch: 6500 | MAE Train Loss: 0.036364395171403885 | MAE Test Loss: 0.0850236564874649 \n",
      "Epoch: 6510 | MAE Train Loss: 0.03633005544543266 | MAE Test Loss: 0.08494096994400024 \n",
      "Epoch: 6520 | MAE Train Loss: 0.03629571944475174 | MAE Test Loss: 0.08486174792051315 \n",
      "Epoch: 6530 | MAE Train Loss: 0.03626132756471634 | MAE Test Loss: 0.08478593081235886 \n",
      "Epoch: 6540 | MAE Train Loss: 0.036227013915777206 | MAE Test Loss: 0.08470325171947479 \n",
      "Epoch: 6550 | MAE Train Loss: 0.0361926332116127 | MAE Test Loss: 0.08462056517601013 \n",
      "Epoch: 6560 | MAE Train Loss: 0.03615829348564148 | MAE Test Loss: 0.08454133570194244 \n",
      "Epoch: 6570 | MAE Train Loss: 0.03612395003437996 | MAE Test Loss: 0.08446212112903595 \n",
      "Epoch: 6580 | MAE Train Loss: 0.03608956187963486 | MAE Test Loss: 0.08437944948673248 \n",
      "Epoch: 6590 | MAE Train Loss: 0.03605516999959946 | MAE Test Loss: 0.08430361747741699 \n",
      "Epoch: 6600 | MAE Train Loss: 0.03602086380124092 | MAE Test Loss: 0.08422093093395233 \n",
      "Epoch: 6610 | MAE Train Loss: 0.035986486822366714 | MAE Test Loss: 0.08414170891046524 \n",
      "Epoch: 6620 | MAE Train Loss: 0.0359521359205246 | MAE Test Loss: 0.08405902236700058 \n",
      "Epoch: 6630 | MAE Train Loss: 0.035917796194553375 | MAE Test Loss: 0.0839797854423523 \n",
      "Epoch: 6640 | MAE Train Loss: 0.03588339313864708 | MAE Test Loss: 0.08389712870121002 \n",
      "Epoch: 6650 | MAE Train Loss: 0.03584901988506317 | MAE Test Loss: 0.08382128924131393 \n",
      "Epoch: 6660 | MAE Train Loss: 0.03581465408205986 | MAE Test Loss: 0.08373520523309708 \n",
      "Epoch: 6670 | MAE Train Loss: 0.03578033298254013 | MAE Test Loss: 0.08365938067436218 \n",
      "Epoch: 6680 | MAE Train Loss: 0.035745952278375626 | MAE Test Loss: 0.0835801512002945 \n",
      "Epoch: 6690 | MAE Train Loss: 0.03571164980530739 | MAE Test Loss: 0.08349746465682983 \n",
      "Epoch: 6700 | MAE Train Loss: 0.0356772355735302 | MAE Test Loss: 0.08341478556394577 \n",
      "Epoch: 6710 | MAE Train Loss: 0.035642869770526886 | MAE Test Loss: 0.08333896100521088 \n",
      "Epoch: 6720 | MAE Train Loss: 0.03560849279165268 | MAE Test Loss: 0.08325288444757462 \n",
      "Epoch: 6730 | MAE Train Loss: 0.03557416424155235 | MAE Test Loss: 0.08317366242408752 \n",
      "Epoch: 6740 | MAE Train Loss: 0.035539790987968445 | MAE Test Loss: 0.08309783786535263 \n",
      "Epoch: 6750 | MAE Train Loss: 0.03550548106431961 | MAE Test Loss: 0.08301515877246857 \n",
      "Epoch: 6760 | MAE Train Loss: 0.035471074283123016 | MAE Test Loss: 0.08293245732784271 \n",
      "Epoch: 6770 | MAE Train Loss: 0.03543673828244209 | MAE Test Loss: 0.08285324275493622 \n",
      "Epoch: 6780 | MAE Train Loss: 0.0354023352265358 | MAE Test Loss: 0.08277402073144913 \n",
      "Epoch: 6790 | MAE Train Loss: 0.03536800295114517 | MAE Test Loss: 0.08269132673740387 \n",
      "Epoch: 6800 | MAE Train Loss: 0.035333652049303055 | MAE Test Loss: 0.08261550962924957 \n",
      "Epoch: 6810 | MAE Train Loss: 0.03529931977391243 | MAE Test Loss: 0.08253283053636551 \n",
      "Epoch: 6820 | MAE Train Loss: 0.035264961421489716 | MAE Test Loss: 0.08245360106229782 \n",
      "Epoch: 6830 | MAE Train Loss: 0.03523057699203491 | MAE Test Loss: 0.08237092196941376 \n",
      "Epoch: 6840 | MAE Train Loss: 0.03519623726606369 | MAE Test Loss: 0.08229169994592667 \n",
      "Epoch: 6850 | MAE Train Loss: 0.03516183793544769 | MAE Test Loss: 0.082209013402462 \n",
      "Epoch: 6860 | MAE Train Loss: 0.03512750193476677 | MAE Test Loss: 0.08213319629430771 \n",
      "Epoch: 6870 | MAE Train Loss: 0.03509315848350525 | MAE Test Loss: 0.08205050975084305 \n",
      "Epoch: 6880 | MAE Train Loss: 0.03505881130695343 | MAE Test Loss: 0.08197127282619476 \n",
      "Epoch: 6890 | MAE Train Loss: 0.03502442687749863 | MAE Test Loss: 0.08189205825328827 \n",
      "Epoch: 6900 | MAE Train Loss: 0.03499007970094681 | MAE Test Loss: 0.0818093791604042 \n",
      "Epoch: 6910 | MAE Train Loss: 0.034955672919750214 | MAE Test Loss: 0.08172668516635895 \n",
      "Epoch: 6920 | MAE Train Loss: 0.034921348094940186 | MAE Test Loss: 0.08165086805820465 \n",
      "Epoch: 6930 | MAE Train Loss: 0.03488696366548538 | MAE Test Loss: 0.08157164603471756 \n",
      "Epoch: 6940 | MAE Train Loss: 0.03485260158777237 | MAE Test Loss: 0.0814855545759201 \n",
      "Epoch: 6950 | MAE Train Loss: 0.03481827676296234 | MAE Test Loss: 0.08140973746776581 \n",
      "Epoch: 6960 | MAE Train Loss: 0.03478391841053963 | MAE Test Loss: 0.08132705092430115 \n",
      "Epoch: 6970 | MAE Train Loss: 0.03474951535463333 | MAE Test Loss: 0.08124436438083649 \n",
      "Epoch: 6980 | MAE Train Loss: 0.034715183079242706 | MAE Test Loss: 0.08116514980792999 \n",
      "Epoch: 6990 | MAE Train Loss: 0.034680821001529694 | MAE Test Loss: 0.0810893252491951 \n",
      "Epoch: 7000 | MAE Train Loss: 0.03464643657207489 | MAE Test Loss: 0.08100324124097824 \n",
      "Epoch: 7010 | MAE Train Loss: 0.034612126648426056 | MAE Test Loss: 0.08092742413282394 \n",
      "Epoch: 7020 | MAE Train Loss: 0.034577757120132446 | MAE Test Loss: 0.08084473758935928 \n",
      "Epoch: 7030 | MAE Train Loss: 0.03454335033893585 | MAE Test Loss: 0.080765500664711 \n",
      "Epoch: 7040 | MAE Train Loss: 0.03450901433825493 | MAE Test Loss: 0.08068281412124634 \n",
      "Epoch: 7050 | MAE Train Loss: 0.034474678337574005 | MAE Test Loss: 0.08060359954833984 \n",
      "Epoch: 7060 | MAE Train Loss: 0.034440286457538605 | MAE Test Loss: 0.08052776753902435 \n",
      "Epoch: 7070 | MAE Train Loss: 0.03440597280859947 | MAE Test Loss: 0.08044509589672089 \n",
      "Epoch: 7080 | MAE Train Loss: 0.03437159210443497 | MAE Test Loss: 0.08036240935325623 \n",
      "Epoch: 7090 | MAE Train Loss: 0.034337252378463745 | MAE Test Loss: 0.08028317987918854 \n",
      "Epoch: 7100 | MAE Train Loss: 0.034302908927202225 | MAE Test Loss: 0.08020396530628204 \n",
      "Epoch: 7110 | MAE Train Loss: 0.03426852077245712 | MAE Test Loss: 0.08012129366397858 \n",
      "Epoch: 7120 | MAE Train Loss: 0.03423412889242172 | MAE Test Loss: 0.08004546165466309 \n",
      "Epoch: 7130 | MAE Train Loss: 0.03419982269406319 | MAE Test Loss: 0.07996277511119843 \n",
      "Epoch: 7140 | MAE Train Loss: 0.03416544571518898 | MAE Test Loss: 0.07988355308771133 \n",
      "Epoch: 7150 | MAE Train Loss: 0.03413109481334686 | MAE Test Loss: 0.07980086654424667 \n",
      "Epoch: 7160 | MAE Train Loss: 0.03409675508737564 | MAE Test Loss: 0.07972162961959839 \n",
      "Epoch: 7170 | MAE Train Loss: 0.034062352031469345 | MAE Test Loss: 0.07963897287845612 \n",
      "Epoch: 7180 | MAE Train Loss: 0.03402797877788544 | MAE Test Loss: 0.07956313341856003 \n",
      "Epoch: 7190 | MAE Train Loss: 0.033993612974882126 | MAE Test Loss: 0.07947704941034317 \n",
      "Epoch: 7200 | MAE Train Loss: 0.033959291875362396 | MAE Test Loss: 0.07940122485160828 \n",
      "Epoch: 7210 | MAE Train Loss: 0.03392491117119789 | MAE Test Loss: 0.07932199537754059 \n",
      "Epoch: 7220 | MAE Train Loss: 0.033890608698129654 | MAE Test Loss: 0.07923930883407593 \n",
      "Epoch: 7230 | MAE Train Loss: 0.03385619446635246 | MAE Test Loss: 0.07915662974119186 \n",
      "Epoch: 7240 | MAE Train Loss: 0.03382182866334915 | MAE Test Loss: 0.07908080518245697 \n",
      "Epoch: 7250 | MAE Train Loss: 0.033787451684474945 | MAE Test Loss: 0.07899472117424011 \n",
      "Epoch: 7260 | MAE Train Loss: 0.03375312313437462 | MAE Test Loss: 0.07891550660133362 \n",
      "Epoch: 7270 | MAE Train Loss: 0.03371874988079071 | MAE Test Loss: 0.07883968204259872 \n",
      "Epoch: 7280 | MAE Train Loss: 0.033684439957141876 | MAE Test Loss: 0.07875700294971466 \n",
      "Epoch: 7290 | MAE Train Loss: 0.03365003317594528 | MAE Test Loss: 0.0786743015050888 \n",
      "Epoch: 7300 | MAE Train Loss: 0.03361569717526436 | MAE Test Loss: 0.07859508693218231 \n",
      "Epoch: 7310 | MAE Train Loss: 0.03358129411935806 | MAE Test Loss: 0.07851586490869522 \n",
      "Epoch: 7320 | MAE Train Loss: 0.03354696184396744 | MAE Test Loss: 0.07843317091464996 \n",
      "Epoch: 7330 | MAE Train Loss: 0.03351261094212532 | MAE Test Loss: 0.07835735380649567 \n",
      "Epoch: 7340 | MAE Train Loss: 0.033478278666734695 | MAE Test Loss: 0.0782746747136116 \n",
      "Epoch: 7350 | MAE Train Loss: 0.03344392031431198 | MAE Test Loss: 0.07819544523954391 \n",
      "Epoch: 7360 | MAE Train Loss: 0.03340953588485718 | MAE Test Loss: 0.07811276614665985 \n",
      "Epoch: 7370 | MAE Train Loss: 0.033375196158885956 | MAE Test Loss: 0.07803353667259216 \n",
      "Epoch: 7380 | MAE Train Loss: 0.03334079682826996 | MAE Test Loss: 0.0779508575797081 \n",
      "Epoch: 7390 | MAE Train Loss: 0.033306460827589035 | MAE Test Loss: 0.0778750404715538 \n",
      "Epoch: 7400 | MAE Train Loss: 0.033272117376327515 | MAE Test Loss: 0.07779236137866974 \n",
      "Epoch: 7410 | MAE Train Loss: 0.033237770199775696 | MAE Test Loss: 0.07771311700344086 \n",
      "Epoch: 7420 | MAE Train Loss: 0.03320338577032089 | MAE Test Loss: 0.07763390243053436 \n",
      "Epoch: 7430 | MAE Train Loss: 0.033169038593769073 | MAE Test Loss: 0.0775512233376503 \n",
      "Epoch: 7440 | MAE Train Loss: 0.03313463181257248 | MAE Test Loss: 0.07746852934360504 \n",
      "Epoch: 7450 | MAE Train Loss: 0.03310030698776245 | MAE Test Loss: 0.07739271223545074 \n",
      "Epoch: 7460 | MAE Train Loss: 0.03306592255830765 | MAE Test Loss: 0.07731349021196365 \n",
      "Epoch: 7470 | MAE Train Loss: 0.033031560480594635 | MAE Test Loss: 0.0772273987531662 \n",
      "Epoch: 7480 | MAE Train Loss: 0.03299723565578461 | MAE Test Loss: 0.0771515816450119 \n",
      "Epoch: 7490 | MAE Train Loss: 0.03296287730336189 | MAE Test Loss: 0.07706889510154724 \n",
      "Epoch: 7500 | MAE Train Loss: 0.0329284742474556 | MAE Test Loss: 0.07698620855808258 \n",
      "Epoch: 7510 | MAE Train Loss: 0.03289414197206497 | MAE Test Loss: 0.07690699398517609 \n",
      "Epoch: 7520 | MAE Train Loss: 0.03285977989435196 | MAE Test Loss: 0.07683117687702179 \n",
      "Epoch: 7530 | MAE Train Loss: 0.032825395464897156 | MAE Test Loss: 0.07674508541822433 \n",
      "Epoch: 7540 | MAE Train Loss: 0.03279108554124832 | MAE Test Loss: 0.07666926831007004 \n",
      "Epoch: 7550 | MAE Train Loss: 0.03275671601295471 | MAE Test Loss: 0.07658658176660538 \n",
      "Epoch: 7560 | MAE Train Loss: 0.032722312957048416 | MAE Test Loss: 0.07650734484195709 \n",
      "Epoch: 7570 | MAE Train Loss: 0.032687973231077194 | MAE Test Loss: 0.07642466574907303 \n",
      "Epoch: 7580 | MAE Train Loss: 0.03265363723039627 | MAE Test Loss: 0.07634544372558594 \n",
      "Epoch: 7590 | MAE Train Loss: 0.03261924535036087 | MAE Test Loss: 0.07626961171627045 \n",
      "Epoch: 7600 | MAE Train Loss: 0.03258493170142174 | MAE Test Loss: 0.07618694752454758 \n",
      "Epoch: 7610 | MAE Train Loss: 0.03255055472254753 | MAE Test Loss: 0.07610425353050232 \n",
      "Epoch: 7620 | MAE Train Loss: 0.03251621127128601 | MAE Test Loss: 0.07602502405643463 \n",
      "Epoch: 7630 | MAE Train Loss: 0.03248186782002449 | MAE Test Loss: 0.07594580948352814 \n",
      "Epoch: 7640 | MAE Train Loss: 0.03244747966527939 | MAE Test Loss: 0.07586313784122467 \n",
      "Epoch: 7650 | MAE Train Loss: 0.03241308778524399 | MAE Test Loss: 0.07578729838132858 \n",
      "Epoch: 7660 | MAE Train Loss: 0.03237878158688545 | MAE Test Loss: 0.07570461928844452 \n",
      "Epoch: 7670 | MAE Train Loss: 0.032344404608011246 | MAE Test Loss: 0.07562539726495743 \n",
      "Epoch: 7680 | MAE Train Loss: 0.03231005370616913 | MAE Test Loss: 0.07554271072149277 \n",
      "Epoch: 7690 | MAE Train Loss: 0.032275713980197906 | MAE Test Loss: 0.07546348124742508 \n",
      "Epoch: 7700 | MAE Train Loss: 0.03224131092429161 | MAE Test Loss: 0.07538080215454102 \n",
      "Epoch: 7710 | MAE Train Loss: 0.0322069376707077 | MAE Test Loss: 0.07530497759580612 \n",
      "Epoch: 7720 | MAE Train Loss: 0.03217257186770439 | MAE Test Loss: 0.07521889358758926 \n",
      "Epoch: 7730 | MAE Train Loss: 0.03213825076818466 | MAE Test Loss: 0.07514306902885437 \n",
      "Epoch: 7740 | MAE Train Loss: 0.03210387006402016 | MAE Test Loss: 0.07506383955478668 \n",
      "Epoch: 7750 | MAE Train Loss: 0.03206956386566162 | MAE Test Loss: 0.07498115301132202 \n",
      "Epoch: 7760 | MAE Train Loss: 0.03203515335917473 | MAE Test Loss: 0.07489847391843796 \n",
      "Epoch: 7770 | MAE Train Loss: 0.03200078755617142 | MAE Test Loss: 0.07482265681028366 \n",
      "Epoch: 7780 | MAE Train Loss: 0.03196641057729721 | MAE Test Loss: 0.0747365653514862 \n",
      "Epoch: 7790 | MAE Train Loss: 0.031932078301906586 | MAE Test Loss: 0.07465735077857971 \n",
      "Epoch: 7800 | MAE Train Loss: 0.031897708773612976 | MAE Test Loss: 0.07458152621984482 \n",
      "Epoch: 7810 | MAE Train Loss: 0.03186339884996414 | MAE Test Loss: 0.07449884712696075 \n",
      "Epoch: 7820 | MAE Train Loss: 0.03182899206876755 | MAE Test Loss: 0.0744161456823349 \n",
      "Epoch: 7830 | MAE Train Loss: 0.031794656068086624 | MAE Test Loss: 0.0743369311094284 \n",
      "Epoch: 7840 | MAE Train Loss: 0.03176025301218033 | MAE Test Loss: 0.07425770908594131 \n",
      "Epoch: 7850 | MAE Train Loss: 0.0317259207367897 | MAE Test Loss: 0.07417501509189606 \n",
      "Epoch: 7860 | MAE Train Loss: 0.031691569834947586 | MAE Test Loss: 0.07409919798374176 \n",
      "Epoch: 7870 | MAE Train Loss: 0.03165723755955696 | MAE Test Loss: 0.0740165188908577 \n",
      "Epoch: 7880 | MAE Train Loss: 0.03162287920713425 | MAE Test Loss: 0.07393728941679001 \n",
      "Epoch: 7890 | MAE Train Loss: 0.03158849850296974 | MAE Test Loss: 0.07385461032390594 \n",
      "Epoch: 7900 | MAE Train Loss: 0.03155415505170822 | MAE Test Loss: 0.07377538084983826 \n",
      "Epoch: 7910 | MAE Train Loss: 0.031519755721092224 | MAE Test Loss: 0.0736927017569542 \n",
      "Epoch: 7920 | MAE Train Loss: 0.0314854197204113 | MAE Test Loss: 0.0736168846487999 \n",
      "Epoch: 7930 | MAE Train Loss: 0.03145107626914978 | MAE Test Loss: 0.07353420555591583 \n",
      "Epoch: 7940 | MAE Train Loss: 0.03141672909259796 | MAE Test Loss: 0.07345496863126755 \n",
      "Epoch: 7950 | MAE Train Loss: 0.03138234466314316 | MAE Test Loss: 0.07337574660778046 \n",
      "Epoch: 7960 | MAE Train Loss: 0.03134799748659134 | MAE Test Loss: 0.07329306751489639 \n",
      "Epoch: 7970 | MAE Train Loss: 0.031313590705394745 | MAE Test Loss: 0.07321037352085114 \n",
      "Epoch: 7980 | MAE Train Loss: 0.03127926588058472 | MAE Test Loss: 0.07313455641269684 \n",
      "Epoch: 7990 | MAE Train Loss: 0.031244879588484764 | MAE Test Loss: 0.07305533438920975 \n",
      "Epoch: 8000 | MAE Train Loss: 0.03121051751077175 | MAE Test Loss: 0.07296924293041229 \n",
      "Epoch: 8010 | MAE Train Loss: 0.031176194548606873 | MAE Test Loss: 0.072893425822258 \n",
      "Epoch: 8020 | MAE Train Loss: 0.03114183619618416 | MAE Test Loss: 0.07281073927879333 \n",
      "Epoch: 8030 | MAE Train Loss: 0.031107431277632713 | MAE Test Loss: 0.07272805273532867 \n",
      "Epoch: 8040 | MAE Train Loss: 0.03107309900224209 | MAE Test Loss: 0.07264883816242218 \n",
      "Epoch: 8050 | MAE Train Loss: 0.031038736924529076 | MAE Test Loss: 0.07257302105426788 \n",
      "Epoch: 8060 | MAE Train Loss: 0.03100435435771942 | MAE Test Loss: 0.07248692959547043 \n",
      "Epoch: 8070 | MAE Train Loss: 0.030970042571425438 | MAE Test Loss: 0.07241111248731613 \n",
      "Epoch: 8080 | MAE Train Loss: 0.030935674905776978 | MAE Test Loss: 0.07232841849327087 \n",
      "Epoch: 8090 | MAE Train Loss: 0.030901271849870682 | MAE Test Loss: 0.07224918901920319 \n",
      "Epoch: 8100 | MAE Train Loss: 0.03086693212389946 | MAE Test Loss: 0.07216650992631912 \n",
      "Epoch: 8110 | MAE Train Loss: 0.030832597985863686 | MAE Test Loss: 0.07208728790283203 \n",
      "Epoch: 8120 | MAE Train Loss: 0.030798202380537987 | MAE Test Loss: 0.07201145589351654 \n",
      "Epoch: 8130 | MAE Train Loss: 0.030763890594244003 | MAE Test Loss: 0.07192879170179367 \n",
      "Epoch: 8140 | MAE Train Loss: 0.030729513615369797 | MAE Test Loss: 0.07184609025716782 \n",
      "Epoch: 8150 | MAE Train Loss: 0.030695170164108276 | MAE Test Loss: 0.07176686823368073 \n",
      "Epoch: 8160 | MAE Train Loss: 0.030660826712846756 | MAE Test Loss: 0.07168765366077423 \n",
      "Epoch: 8170 | MAE Train Loss: 0.030626440420746803 | MAE Test Loss: 0.07160498201847076 \n",
      "Epoch: 8180 | MAE Train Loss: 0.030592044815421104 | MAE Test Loss: 0.07152914255857468 \n",
      "Epoch: 8190 | MAE Train Loss: 0.030557742342352867 | MAE Test Loss: 0.07144646346569061 \n",
      "Epoch: 8200 | MAE Train Loss: 0.030523359775543213 | MAE Test Loss: 0.07136723399162292 \n",
      "Epoch: 8210 | MAE Train Loss: 0.030489012598991394 | MAE Test Loss: 0.07128455489873886 \n",
      "Epoch: 8220 | MAE Train Loss: 0.030454671010375023 | MAE Test Loss: 0.07120532542467117 \n",
      "Epoch: 8230 | MAE Train Loss: 0.030420269817113876 | MAE Test Loss: 0.07112264633178711 \n",
      "Epoch: 8240 | MAE Train Loss: 0.030385896563529968 | MAE Test Loss: 0.07104682177305222 \n",
      "Epoch: 8250 | MAE Train Loss: 0.030351530760526657 | MAE Test Loss: 0.07096074521541595 \n",
      "Epoch: 8260 | MAE Train Loss: 0.030317207798361778 | MAE Test Loss: 0.07088490575551987 \n",
      "Epoch: 8270 | MAE Train Loss: 0.030282828956842422 | MAE Test Loss: 0.07080568373203278 \n",
      "Epoch: 8280 | MAE Train Loss: 0.030248522758483887 | MAE Test Loss: 0.07072300463914871 \n",
      "Epoch: 8290 | MAE Train Loss: 0.030214110389351845 | MAE Test Loss: 0.07064031809568405 \n",
      "Epoch: 8300 | MAE Train Loss: 0.030179748311638832 | MAE Test Loss: 0.07056450098752975 \n",
      "Epoch: 8310 | MAE Train Loss: 0.030145373195409775 | MAE Test Loss: 0.0704784095287323 \n",
      "Epoch: 8320 | MAE Train Loss: 0.030111039057374 | MAE Test Loss: 0.07039918750524521 \n",
      "Epoch: 8330 | MAE Train Loss: 0.03007666766643524 | MAE Test Loss: 0.07032337039709091 \n",
      "Epoch: 8340 | MAE Train Loss: 0.030042355880141258 | MAE Test Loss: 0.07024069130420685 \n",
      "Epoch: 8350 | MAE Train Loss: 0.030007952824234962 | MAE Test Loss: 0.07015799731016159 \n",
      "Epoch: 8360 | MAE Train Loss: 0.02997361496090889 | MAE Test Loss: 0.0700787752866745 \n",
      "Epoch: 8370 | MAE Train Loss: 0.029939210042357445 | MAE Test Loss: 0.069999560713768 \n",
      "Epoch: 8380 | MAE Train Loss: 0.02990487776696682 | MAE Test Loss: 0.06991685926914215 \n",
      "Epoch: 8390 | MAE Train Loss: 0.02987052872776985 | MAE Test Loss: 0.06984104216098785 \n",
      "Epoch: 8400 | MAE Train Loss: 0.029836196452379227 | MAE Test Loss: 0.06975836306810379 \n",
      "Epoch: 8410 | MAE Train Loss: 0.02980183996260166 | MAE Test Loss: 0.0696791335940361 \n",
      "Epoch: 8420 | MAE Train Loss: 0.029767457395792007 | MAE Test Loss: 0.06959645450115204 \n",
      "Epoch: 8430 | MAE Train Loss: 0.029733117669820786 | MAE Test Loss: 0.06951722502708435 \n",
      "Epoch: 8440 | MAE Train Loss: 0.02969871461391449 | MAE Test Loss: 0.06943455338478088 \n",
      "Epoch: 8450 | MAE Train Loss: 0.029664378613233566 | MAE Test Loss: 0.0693587213754654 \n",
      "Epoch: 8460 | MAE Train Loss: 0.029630035161972046 | MAE Test Loss: 0.06927604973316193 \n",
      "Epoch: 8470 | MAE Train Loss: 0.029595687985420227 | MAE Test Loss: 0.06919681280851364 \n",
      "Epoch: 8480 | MAE Train Loss: 0.029561305418610573 | MAE Test Loss: 0.06911759078502655 \n",
      "Epoch: 8490 | MAE Train Loss: 0.029526954516768456 | MAE Test Loss: 0.06903491169214249 \n",
      "Epoch: 8500 | MAE Train Loss: 0.02949255146086216 | MAE Test Loss: 0.06895221769809723 \n",
      "Epoch: 8510 | MAE Train Loss: 0.029458224773406982 | MAE Test Loss: 0.06887640058994293 \n",
      "Epoch: 8520 | MAE Train Loss: 0.02942383848130703 | MAE Test Loss: 0.06879717856645584 \n",
      "Epoch: 8530 | MAE Train Loss: 0.029389476403594017 | MAE Test Loss: 0.06871108710765839 \n",
      "Epoch: 8540 | MAE Train Loss: 0.029355153441429138 | MAE Test Loss: 0.06863526999950409 \n",
      "Epoch: 8550 | MAE Train Loss: 0.029320795089006424 | MAE Test Loss: 0.06855258345603943 \n",
      "Epoch: 8560 | MAE Train Loss: 0.02928639017045498 | MAE Test Loss: 0.06846989691257477 \n",
      "Epoch: 8570 | MAE Train Loss: 0.029252057895064354 | MAE Test Loss: 0.06839068233966827 \n",
      "Epoch: 8580 | MAE Train Loss: 0.02921769581735134 | MAE Test Loss: 0.06831486523151398 \n",
      "Epoch: 8590 | MAE Train Loss: 0.029183313250541687 | MAE Test Loss: 0.06822877377271652 \n",
      "Epoch: 8600 | MAE Train Loss: 0.029149001464247704 | MAE Test Loss: 0.06815295666456223 \n",
      "Epoch: 8610 | MAE Train Loss: 0.029114633798599243 | MAE Test Loss: 0.06807026267051697 \n",
      "Epoch: 8620 | MAE Train Loss: 0.029080230742692947 | MAE Test Loss: 0.06799103319644928 \n",
      "Epoch: 8630 | MAE Train Loss: 0.029045891016721725 | MAE Test Loss: 0.06790835410356522 \n",
      "Epoch: 8640 | MAE Train Loss: 0.02901155687868595 | MAE Test Loss: 0.06782912462949753 \n",
      "Epoch: 8650 | MAE Train Loss: 0.028977161273360252 | MAE Test Loss: 0.06775330007076263 \n",
      "Epoch: 8660 | MAE Train Loss: 0.02894284948706627 | MAE Test Loss: 0.06767063587903976 \n",
      "Epoch: 8670 | MAE Train Loss: 0.028908472508192062 | MAE Test Loss: 0.06758793443441391 \n",
      "Epoch: 8680 | MAE Train Loss: 0.028874129056930542 | MAE Test Loss: 0.06750871241092682 \n",
      "Epoch: 8690 | MAE Train Loss: 0.02883978560566902 | MAE Test Loss: 0.06742949038743973 \n",
      "Epoch: 8700 | MAE Train Loss: 0.02880539931356907 | MAE Test Loss: 0.06734682619571686 \n",
      "Epoch: 8710 | MAE Train Loss: 0.02877100370824337 | MAE Test Loss: 0.06727097928524017 \n",
      "Epoch: 8720 | MAE Train Loss: 0.028736701235175133 | MAE Test Loss: 0.0671883076429367 \n",
      "Epoch: 8730 | MAE Train Loss: 0.02870231866836548 | MAE Test Loss: 0.06710907816886902 \n",
      "Epoch: 8740 | MAE Train Loss: 0.02866797149181366 | MAE Test Loss: 0.06702639907598495 \n",
      "Epoch: 8750 | MAE Train Loss: 0.02863362990319729 | MAE Test Loss: 0.06694716960191727 \n",
      "Epoch: 8760 | MAE Train Loss: 0.028599228709936142 | MAE Test Loss: 0.0668644905090332 \n",
      "Epoch: 8770 | MAE Train Loss: 0.028564855456352234 | MAE Test Loss: 0.0667886734008789 \n",
      "Epoch: 8780 | MAE Train Loss: 0.028530489653348923 | MAE Test Loss: 0.06670258194208145 \n",
      "Epoch: 8790 | MAE Train Loss: 0.028496166691184044 | MAE Test Loss: 0.06662674993276596 \n",
      "Epoch: 8800 | MAE Train Loss: 0.028461787849664688 | MAE Test Loss: 0.06654752790927887 \n",
      "Epoch: 8810 | MAE Train Loss: 0.028427477926015854 | MAE Test Loss: 0.0664648488163948 \n",
      "Epoch: 8820 | MAE Train Loss: 0.02839306928217411 | MAE Test Loss: 0.06638216227293015 \n",
      "Epoch: 8830 | MAE Train Loss: 0.028358707204461098 | MAE Test Loss: 0.06630635261535645 \n",
      "Epoch: 8840 | MAE Train Loss: 0.02832433208823204 | MAE Test Loss: 0.0662202462553978 \n",
      "Epoch: 8850 | MAE Train Loss: 0.028289997950196266 | MAE Test Loss: 0.0661410242319107 \n",
      "Epoch: 8860 | MAE Train Loss: 0.028255626559257507 | MAE Test Loss: 0.066065214574337 \n",
      "Epoch: 8870 | MAE Train Loss: 0.028221314772963524 | MAE Test Loss: 0.06598253548145294 \n",
      "Epoch: 8880 | MAE Train Loss: 0.028186911717057228 | MAE Test Loss: 0.06589984148740768 \n",
      "Epoch: 8890 | MAE Train Loss: 0.028152573853731155 | MAE Test Loss: 0.0658206194639206 \n",
      "Epoch: 8900 | MAE Train Loss: 0.02811816893517971 | MAE Test Loss: 0.0657413974404335 \n",
      "Epoch: 8910 | MAE Train Loss: 0.028083834797143936 | MAE Test Loss: 0.06565870344638824 \n",
      "Epoch: 8920 | MAE Train Loss: 0.028049487620592117 | MAE Test Loss: 0.06558288633823395 \n",
      "Epoch: 8930 | MAE Train Loss: 0.028015155345201492 | MAE Test Loss: 0.06550020724534988 \n",
      "Epoch: 8940 | MAE Train Loss: 0.027980798855423927 | MAE Test Loss: 0.0654209777712822 \n",
      "Epoch: 8950 | MAE Train Loss: 0.027946416288614273 | MAE Test Loss: 0.06533829867839813 \n",
      "Epoch: 8960 | MAE Train Loss: 0.02791207656264305 | MAE Test Loss: 0.06525906175374985 \n",
      "Epoch: 8970 | MAE Train Loss: 0.027877673506736755 | MAE Test Loss: 0.06517639011144638 \n",
      "Epoch: 8980 | MAE Train Loss: 0.027843337506055832 | MAE Test Loss: 0.06510056555271149 \n",
      "Epoch: 8990 | MAE Train Loss: 0.027808990329504013 | MAE Test Loss: 0.06501789391040802 \n",
      "Epoch: 9000 | MAE Train Loss: 0.027774646878242493 | MAE Test Loss: 0.06493865698575974 \n",
      "Epoch: 9010 | MAE Train Loss: 0.02774026431143284 | MAE Test Loss: 0.06485943496227264 \n",
      "Epoch: 9020 | MAE Train Loss: 0.02770591340959072 | MAE Test Loss: 0.06477675586938858 \n",
      "Epoch: 9030 | MAE Train Loss: 0.027671510353684425 | MAE Test Loss: 0.06469406187534332 \n",
      "Epoch: 9040 | MAE Train Loss: 0.027637183666229248 | MAE Test Loss: 0.06461824476718903 \n",
      "Epoch: 9050 | MAE Train Loss: 0.027602797374129295 | MAE Test Loss: 0.06453902274370193 \n",
      "Epoch: 9060 | MAE Train Loss: 0.027568435296416283 | MAE Test Loss: 0.06445293128490448 \n",
      "Epoch: 9070 | MAE Train Loss: 0.027534112334251404 | MAE Test Loss: 0.06437711417675018 \n",
      "Epoch: 9080 | MAE Train Loss: 0.02749975398182869 | MAE Test Loss: 0.06429442763328552 \n",
      "Epoch: 9090 | MAE Train Loss: 0.027465347200632095 | MAE Test Loss: 0.06421174108982086 \n",
      "Epoch: 9100 | MAE Train Loss: 0.02743101678788662 | MAE Test Loss: 0.06413251906633377 \n",
      "Epoch: 9110 | MAE Train Loss: 0.027396652847528458 | MAE Test Loss: 0.06405670940876007 \n",
      "Epoch: 9120 | MAE Train Loss: 0.027362272143363953 | MAE Test Loss: 0.06397061049938202 \n",
      "Epoch: 9130 | MAE Train Loss: 0.02732796035706997 | MAE Test Loss: 0.06389480084180832 \n",
      "Epoch: 9140 | MAE Train Loss: 0.02729359269142151 | MAE Test Loss: 0.06381210684776306 \n",
      "Epoch: 9150 | MAE Train Loss: 0.027259189635515213 | MAE Test Loss: 0.06373287737369537 \n",
      "Epoch: 9160 | MAE Train Loss: 0.02722484990954399 | MAE Test Loss: 0.06365019828081131 \n",
      "Epoch: 9170 | MAE Train Loss: 0.02719051204621792 | MAE Test Loss: 0.06357096880674362 \n",
      "Epoch: 9180 | MAE Train Loss: 0.027156120166182518 | MAE Test Loss: 0.06349514424800873 \n",
      "Epoch: 9190 | MAE Train Loss: 0.027121808379888535 | MAE Test Loss: 0.06341248005628586 \n",
      "Epoch: 9200 | MAE Train Loss: 0.027087431401014328 | MAE Test Loss: 0.06332977861166 \n",
      "Epoch: 9210 | MAE Train Loss: 0.027053091675043106 | MAE Test Loss: 0.06325055658817291 \n",
      "Epoch: 9220 | MAE Train Loss: 0.027018744498491287 | MAE Test Loss: 0.06317133456468582 \n",
      "Epoch: 9230 | MAE Train Loss: 0.026984358206391335 | MAE Test Loss: 0.06308867037296295 \n",
      "Epoch: 9240 | MAE Train Loss: 0.026949966326355934 | MAE Test Loss: 0.06301282346248627 \n",
      "Epoch: 9250 | MAE Train Loss: 0.0269156601279974 | MAE Test Loss: 0.0629301518201828 \n",
      "Epoch: 9260 | MAE Train Loss: 0.026881277561187744 | MAE Test Loss: 0.06285092234611511 \n",
      "Epoch: 9270 | MAE Train Loss: 0.026846934109926224 | MAE Test Loss: 0.06276824325323105 \n",
      "Epoch: 9280 | MAE Train Loss: 0.026812588796019554 | MAE Test Loss: 0.06268901377916336 \n",
      "Epoch: 9290 | MAE Train Loss: 0.026778187602758408 | MAE Test Loss: 0.0626063346862793 \n",
      "Epoch: 9300 | MAE Train Loss: 0.0267438143491745 | MAE Test Loss: 0.062530517578125 \n",
      "Epoch: 9310 | MAE Train Loss: 0.02670944854617119 | MAE Test Loss: 0.062444426119327545 \n",
      "Epoch: 9320 | MAE Train Loss: 0.02667512558400631 | MAE Test Loss: 0.062368594110012054 \n",
      "Epoch: 9330 | MAE Train Loss: 0.026640746742486954 | MAE Test Loss: 0.062289368361234665 \n",
      "Epoch: 9340 | MAE Train Loss: 0.02660643681883812 | MAE Test Loss: 0.0622066855430603 \n",
      "Epoch: 9350 | MAE Train Loss: 0.026572028174996376 | MAE Test Loss: 0.06212400645017624 \n",
      "Epoch: 9360 | MAE Train Loss: 0.026537666097283363 | MAE Test Loss: 0.06204819679260254 \n",
      "Epoch: 9370 | MAE Train Loss: 0.026503290981054306 | MAE Test Loss: 0.06196209043264389 \n",
      "Epoch: 9380 | MAE Train Loss: 0.026468956843018532 | MAE Test Loss: 0.0618828721344471 \n",
      "Epoch: 9390 | MAE Train Loss: 0.026434585452079773 | MAE Test Loss: 0.061807066202163696 \n",
      "Epoch: 9400 | MAE Train Loss: 0.02640027366578579 | MAE Test Loss: 0.061724383383989334 \n",
      "Epoch: 9410 | MAE Train Loss: 0.026365870609879494 | MAE Test Loss: 0.06164168566465378 \n",
      "Epoch: 9420 | MAE Train Loss: 0.02633153274655342 | MAE Test Loss: 0.061562467366456985 \n",
      "Epoch: 9430 | MAE Train Loss: 0.026297127828001976 | MAE Test Loss: 0.061483245342969894 \n",
      "Epoch: 9440 | MAE Train Loss: 0.026262793689966202 | MAE Test Loss: 0.06140054389834404 \n",
      "Epoch: 9450 | MAE Train Loss: 0.026228446513414383 | MAE Test Loss: 0.06132473424077034 \n",
      "Epoch: 9460 | MAE Train Loss: 0.026194114238023758 | MAE Test Loss: 0.06124205142259598 \n",
      "Epoch: 9470 | MAE Train Loss: 0.026159759610891342 | MAE Test Loss: 0.06116282194852829 \n",
      "Epoch: 9480 | MAE Train Loss: 0.02612537518143654 | MAE Test Loss: 0.06108013540506363 \n",
      "Epoch: 9490 | MAE Train Loss: 0.026091035455465317 | MAE Test Loss: 0.06100090593099594 \n",
      "Epoch: 9500 | MAE Train Loss: 0.02605663239955902 | MAE Test Loss: 0.060918234288692474 \n",
      "Epoch: 9510 | MAE Train Loss: 0.026022296398878098 | MAE Test Loss: 0.06084241345524788 \n",
      "Epoch: 9520 | MAE Train Loss: 0.025987952947616577 | MAE Test Loss: 0.060759734362363815 \n",
      "Epoch: 9530 | MAE Train Loss: 0.02595360577106476 | MAE Test Loss: 0.06068050116300583 \n",
      "Epoch: 9540 | MAE Train Loss: 0.025919223204255104 | MAE Test Loss: 0.060601282864809036 \n",
      "Epoch: 9550 | MAE Train Loss: 0.025884872302412987 | MAE Test Loss: 0.060518600046634674 \n",
      "Epoch: 9560 | MAE Train Loss: 0.02585046924650669 | MAE Test Loss: 0.060435909777879715 \n",
      "Epoch: 9570 | MAE Train Loss: 0.025816142559051514 | MAE Test Loss: 0.06036009266972542 \n",
      "Epoch: 9580 | MAE Train Loss: 0.02578175626695156 | MAE Test Loss: 0.06028086692094803 \n",
      "Epoch: 9590 | MAE Train Loss: 0.025747397914528847 | MAE Test Loss: 0.06019477918744087 \n",
      "Epoch: 9600 | MAE Train Loss: 0.02571307122707367 | MAE Test Loss: 0.06011895090341568 \n",
      "Epoch: 9610 | MAE Train Loss: 0.025678712874650955 | MAE Test Loss: 0.060036271810531616 \n",
      "Epoch: 9620 | MAE Train Loss: 0.02564430609345436 | MAE Test Loss: 0.059953588992357254 \n",
      "Epoch: 9630 | MAE Train Loss: 0.025609975680708885 | MAE Test Loss: 0.059874363243579865 \n",
      "Epoch: 9640 | MAE Train Loss: 0.025575608015060425 | MAE Test Loss: 0.059798549860715866 \n",
      "Epoch: 9650 | MAE Train Loss: 0.025541231036186218 | MAE Test Loss: 0.05971245840191841 \n",
      "Epoch: 9660 | MAE Train Loss: 0.025506919249892235 | MAE Test Loss: 0.059636641293764114 \n",
      "Epoch: 9670 | MAE Train Loss: 0.025472551584243774 | MAE Test Loss: 0.059553951025009155 \n",
      "Epoch: 9680 | MAE Train Loss: 0.02543814852833748 | MAE Test Loss: 0.059474725276231766 \n",
      "Epoch: 9690 | MAE Train Loss: 0.025403808802366257 | MAE Test Loss: 0.059392042458057404 \n",
      "Epoch: 9700 | MAE Train Loss: 0.025369470939040184 | MAE Test Loss: 0.05931280925869942 \n",
      "Epoch: 9710 | MAE Train Loss: 0.025335079059004784 | MAE Test Loss: 0.05923699215054512 \n",
      "Epoch: 9720 | MAE Train Loss: 0.0253007672727108 | MAE Test Loss: 0.05915432050824165 \n",
      "Epoch: 9730 | MAE Train Loss: 0.025266390293836594 | MAE Test Loss: 0.0590716227889061 \n",
      "Epoch: 9740 | MAE Train Loss: 0.02523205056786537 | MAE Test Loss: 0.058992404490709305 \n",
      "Epoch: 9750 | MAE Train Loss: 0.025197703391313553 | MAE Test Loss: 0.058913178741931915 \n",
      "Epoch: 9760 | MAE Train Loss: 0.0251633170992136 | MAE Test Loss: 0.05883051082491875 \n",
      "Epoch: 9770 | MAE Train Loss: 0.0251289252191782 | MAE Test Loss: 0.05875467136502266 \n",
      "Epoch: 9780 | MAE Train Loss: 0.025094619020819664 | MAE Test Loss: 0.05867199972271919 \n",
      "Epoch: 9790 | MAE Train Loss: 0.02506023645401001 | MAE Test Loss: 0.058592766523361206 \n",
      "Epoch: 9800 | MAE Train Loss: 0.02502589300274849 | MAE Test Loss: 0.05851009488105774 \n",
      "Epoch: 9810 | MAE Train Loss: 0.02499154955148697 | MAE Test Loss: 0.058430857956409454 \n",
      "Epoch: 9820 | MAE Train Loss: 0.024957144632935524 | MAE Test Loss: 0.05834817886352539 \n",
      "Epoch: 9830 | MAE Train Loss: 0.024922773241996765 | MAE Test Loss: 0.058272361755371094 \n",
      "Epoch: 9840 | MAE Train Loss: 0.024888407438993454 | MAE Test Loss: 0.05818627402186394 \n",
      "Epoch: 9850 | MAE Train Loss: 0.024854084476828575 | MAE Test Loss: 0.05811043828725815 \n",
      "Epoch: 9860 | MAE Train Loss: 0.02481970563530922 | MAE Test Loss: 0.058031219989061356 \n",
      "Epoch: 9870 | MAE Train Loss: 0.024785395711660385 | MAE Test Loss: 0.057948529720306396 \n",
      "Epoch: 9880 | MAE Train Loss: 0.02475098706781864 | MAE Test Loss: 0.05786585062742233 \n",
      "Epoch: 9890 | MAE Train Loss: 0.02471662499010563 | MAE Test Loss: 0.05779004096984863 \n",
      "Epoch: 9900 | MAE Train Loss: 0.02468224987387657 | MAE Test Loss: 0.057703934609889984 \n",
      "Epoch: 9910 | MAE Train Loss: 0.024647915735840797 | MAE Test Loss: 0.05762471631169319 \n",
      "Epoch: 9920 | MAE Train Loss: 0.02461354434490204 | MAE Test Loss: 0.05754891037940979 \n",
      "Epoch: 9930 | MAE Train Loss: 0.024579234421253204 | MAE Test Loss: 0.05746622756123543 \n",
      "Epoch: 9940 | MAE Train Loss: 0.02454482950270176 | MAE Test Loss: 0.05738352984189987 \n",
      "Epoch: 9950 | MAE Train Loss: 0.024510491639375687 | MAE Test Loss: 0.05730431154370308 \n",
      "Epoch: 9960 | MAE Train Loss: 0.024476084858179092 | MAE Test Loss: 0.05722508952021599 \n",
      "Epoch: 9970 | MAE Train Loss: 0.024441752582788467 | MAE Test Loss: 0.057142388075590134 \n",
      "Epoch: 9980 | MAE Train Loss: 0.02440740540623665 | MAE Test Loss: 0.057066578418016434 \n",
      "Epoch: 9990 | MAE Train Loss: 0.024373073130846024 | MAE Test Loss: 0.05698389559984207 \n",
      "Epoch: 10000 | MAE Train Loss: 0.024338718503713608 | MAE Test Loss: 0.056904666125774384 \n",
      "Epoch: 10010 | MAE Train Loss: 0.024304334074258804 | MAE Test Loss: 0.05682197958230972 \n",
      "Epoch: 10020 | MAE Train Loss: 0.024269994348287582 | MAE Test Loss: 0.056742750108242035 \n",
      "Epoch: 10030 | MAE Train Loss: 0.024235591292381287 | MAE Test Loss: 0.05666007846593857 \n",
      "Epoch: 10040 | MAE Train Loss: 0.024201255291700363 | MAE Test Loss: 0.05658425763249397 \n",
      "Epoch: 10050 | MAE Train Loss: 0.024166909977793694 | MAE Test Loss: 0.05650157853960991 \n",
      "Epoch: 10060 | MAE Train Loss: 0.024132560938596725 | MAE Test Loss: 0.05642234534025192 \n",
      "Epoch: 10070 | MAE Train Loss: 0.02409818209707737 | MAE Test Loss: 0.05634312704205513 \n",
      "Epoch: 10080 | MAE Train Loss: 0.024063829332590103 | MAE Test Loss: 0.05626044422388077 \n",
      "Epoch: 10090 | MAE Train Loss: 0.024029428139328957 | MAE Test Loss: 0.05617775395512581 \n",
      "Epoch: 10100 | MAE Train Loss: 0.02399510145187378 | MAE Test Loss: 0.05610193684697151 \n",
      "Epoch: 10110 | MAE Train Loss: 0.023960720747709274 | MAE Test Loss: 0.05602271109819412 \n",
      "Epoch: 10120 | MAE Train Loss: 0.023926356807351112 | MAE Test Loss: 0.055936623364686966 \n",
      "Epoch: 10130 | MAE Train Loss: 0.023892030119895935 | MAE Test Loss: 0.055860795080661774 \n",
      "Epoch: 10140 | MAE Train Loss: 0.02385767176747322 | MAE Test Loss: 0.05577811598777771 \n",
      "Epoch: 10150 | MAE Train Loss: 0.023823264986276627 | MAE Test Loss: 0.05569542571902275 \n",
      "Epoch: 10160 | MAE Train Loss: 0.02378893457353115 | MAE Test Loss: 0.05561620742082596 \n",
      "Epoch: 10170 | MAE Train Loss: 0.02375456877052784 | MAE Test Loss: 0.05554039031267166 \n",
      "Epoch: 10180 | MAE Train Loss: 0.023720186203718185 | MAE Test Loss: 0.055454302579164505 \n",
      "Epoch: 10190 | MAE Train Loss: 0.023685883730649948 | MAE Test Loss: 0.05537847802042961 \n",
      "Epoch: 10200 | MAE Train Loss: 0.02365151047706604 | MAE Test Loss: 0.05529579520225525 \n",
      "Epoch: 10210 | MAE Train Loss: 0.023617107421159744 | MAE Test Loss: 0.05521656945347786 \n",
      "Epoch: 10220 | MAE Train Loss: 0.023582767695188522 | MAE Test Loss: 0.0551338866353035 \n",
      "Epoch: 10230 | MAE Train Loss: 0.023548435419797897 | MAE Test Loss: 0.05505465343594551 \n",
      "Epoch: 10240 | MAE Train Loss: 0.0235140360891819 | MAE Test Loss: 0.054978836327791214 \n",
      "Epoch: 10250 | MAE Train Loss: 0.023479729890823364 | MAE Test Loss: 0.05489616468548775 \n",
      "Epoch: 10260 | MAE Train Loss: 0.02344534918665886 | MAE Test Loss: 0.05481347441673279 \n",
      "Epoch: 10270 | MAE Train Loss: 0.023411009460687637 | MAE Test Loss: 0.0547342412173748 \n",
      "Epoch: 10280 | MAE Train Loss: 0.02337666228413582 | MAE Test Loss: 0.05465502291917801 \n",
      "Epoch: 10290 | MAE Train Loss: 0.023342274129390717 | MAE Test Loss: 0.05457235127687454 \n",
      "Epoch: 10300 | MAE Train Loss: 0.023307884112000465 | MAE Test Loss: 0.05449651926755905 \n",
      "Epoch: 10310 | MAE Train Loss: 0.02327357791364193 | MAE Test Loss: 0.054413843899965286 \n",
      "Epoch: 10320 | MAE Train Loss: 0.023239195346832275 | MAE Test Loss: 0.0543346107006073 \n",
      "Epoch: 10330 | MAE Train Loss: 0.023204851895570755 | MAE Test Loss: 0.05425193905830383 \n",
      "Epoch: 10340 | MAE Train Loss: 0.023170508444309235 | MAE Test Loss: 0.05417270213365555 \n",
      "Epoch: 10350 | MAE Train Loss: 0.02313610538840294 | MAE Test Loss: 0.054090023040771484 \n",
      "Epoch: 10360 | MAE Train Loss: 0.02310173586010933 | MAE Test Loss: 0.05401420593261719 \n",
      "Epoch: 10370 | MAE Train Loss: 0.023067370057106018 | MAE Test Loss: 0.05392811447381973 \n",
      "Epoch: 10380 | MAE Train Loss: 0.02303304336965084 | MAE Test Loss: 0.05385228991508484 \n",
      "Epoch: 10390 | MAE Train Loss: 0.022998664528131485 | MAE Test Loss: 0.05377305671572685 \n",
      "Epoch: 10400 | MAE Train Loss: 0.02296435460448265 | MAE Test Loss: 0.05369038134813309 \n",
      "Epoch: 10410 | MAE Train Loss: 0.022929945960640907 | MAE Test Loss: 0.053607694804668427 \n",
      "Epoch: 10420 | MAE Train Loss: 0.022895583882927895 | MAE Test Loss: 0.05353188514709473 \n",
      "Epoch: 10430 | MAE Train Loss: 0.022861208766698837 | MAE Test Loss: 0.05344577878713608 \n",
      "Epoch: 10440 | MAE Train Loss: 0.022826874628663063 | MAE Test Loss: 0.053366560488939285 \n",
      "Epoch: 10450 | MAE Train Loss: 0.022792508825659752 | MAE Test Loss: 0.053290754556655884 \n",
      "Epoch: 10460 | MAE Train Loss: 0.02275819331407547 | MAE Test Loss: 0.05320807173848152 \n",
      "Epoch: 10470 | MAE Train Loss: 0.022723788395524025 | MAE Test Loss: 0.053125374019145966 \n",
      "Epoch: 10480 | MAE Train Loss: 0.0226894523948431 | MAE Test Loss: 0.05304615572094917 \n",
      "Epoch: 10490 | MAE Train Loss: 0.022655043751001358 | MAE Test Loss: 0.05296692997217178 \n",
      "Epoch: 10500 | MAE Train Loss: 0.022620711475610733 | MAE Test Loss: 0.052884239703416824 \n",
      "Epoch: 10510 | MAE Train Loss: 0.022586364299058914 | MAE Test Loss: 0.05280842259526253 \n",
      "Epoch: 10520 | MAE Train Loss: 0.02255203202366829 | MAE Test Loss: 0.052725739777088165 \n",
      "Epoch: 10530 | MAE Train Loss: 0.022517677396535873 | MAE Test Loss: 0.05264651030302048 \n",
      "Epoch: 10540 | MAE Train Loss: 0.02248329296708107 | MAE Test Loss: 0.05256382375955582 \n",
      "Epoch: 10550 | MAE Train Loss: 0.022448953241109848 | MAE Test Loss: 0.05248459428548813 \n",
      "Epoch: 10560 | MAE Train Loss: 0.022414550185203552 | MAE Test Loss: 0.05240192264318466 \n",
      "Epoch: 10570 | MAE Train Loss: 0.02238021418452263 | MAE Test Loss: 0.052326101809740067 \n",
      "Epoch: 10580 | MAE Train Loss: 0.02234586887061596 | MAE Test Loss: 0.052243418991565704 \n",
      "Epoch: 10590 | MAE Train Loss: 0.02231152169406414 | MAE Test Loss: 0.052164189517498016 \n",
      "Epoch: 10600 | MAE Train Loss: 0.022277140989899635 | MAE Test Loss: 0.052084971219301224 \n",
      "Epoch: 10610 | MAE Train Loss: 0.022242791950702667 | MAE Test Loss: 0.05200228840112686 \n",
      "Epoch: 10620 | MAE Train Loss: 0.022208387032151222 | MAE Test Loss: 0.0519195981323719 \n",
      "Epoch: 10630 | MAE Train Loss: 0.022174060344696045 | MAE Test Loss: 0.051843781024217606 \n",
      "Epoch: 10640 | MAE Train Loss: 0.02213967964053154 | MAE Test Loss: 0.051764555275440216 \n",
      "Epoch: 10650 | MAE Train Loss: 0.022105315700173378 | MAE Test Loss: 0.05167846754193306 \n",
      "Epoch: 10660 | MAE Train Loss: 0.0220709890127182 | MAE Test Loss: 0.05160263925790787 \n",
      "Epoch: 10670 | MAE Train Loss: 0.022036630660295486 | MAE Test Loss: 0.051519960165023804 \n",
      "Epoch: 10680 | MAE Train Loss: 0.022002223879098892 | MAE Test Loss: 0.051437269896268845 \n",
      "Epoch: 10690 | MAE Train Loss: 0.021967893466353416 | MAE Test Loss: 0.05135805159807205 \n",
      "Epoch: 10700 | MAE Train Loss: 0.021933527663350105 | MAE Test Loss: 0.051282234489917755 \n",
      "Epoch: 10710 | MAE Train Loss: 0.0218991469591856 | MAE Test Loss: 0.0511961467564106 \n",
      "Epoch: 10720 | MAE Train Loss: 0.021864842623472214 | MAE Test Loss: 0.051120322197675705 \n",
      "Epoch: 10730 | MAE Train Loss: 0.021830469369888306 | MAE Test Loss: 0.05103763937950134 \n",
      "Epoch: 10740 | MAE Train Loss: 0.02179606631398201 | MAE Test Loss: 0.05095841363072395 \n",
      "Epoch: 10750 | MAE Train Loss: 0.021761726588010788 | MAE Test Loss: 0.05087573081254959 \n",
      "Epoch: 10760 | MAE Train Loss: 0.021727394312620163 | MAE Test Loss: 0.050796497613191605 \n",
      "Epoch: 10770 | MAE Train Loss: 0.021692994982004166 | MAE Test Loss: 0.05072068050503731 \n",
      "Epoch: 10780 | MAE Train Loss: 0.02165868878364563 | MAE Test Loss: 0.05063800886273384 \n",
      "Epoch: 10790 | MAE Train Loss: 0.021624308079481125 | MAE Test Loss: 0.05055531859397888 \n",
      "Epoch: 10800 | MAE Train Loss: 0.021589968353509903 | MAE Test Loss: 0.050476085394620895 \n",
      "Epoch: 10810 | MAE Train Loss: 0.021555621176958084 | MAE Test Loss: 0.0503968670964241 \n",
      "Epoch: 10820 | MAE Train Loss: 0.021521233022212982 | MAE Test Loss: 0.050314195454120636 \n",
      "Epoch: 10830 | MAE Train Loss: 0.02148684300482273 | MAE Test Loss: 0.050238363444805145 \n",
      "Epoch: 10840 | MAE Train Loss: 0.021452536806464195 | MAE Test Loss: 0.05015568807721138 \n",
      "Epoch: 10850 | MAE Train Loss: 0.02141815423965454 | MAE Test Loss: 0.050076454877853394 \n",
      "Epoch: 10860 | MAE Train Loss: 0.02138381078839302 | MAE Test Loss: 0.04999377578496933 \n",
      "Epoch: 10870 | MAE Train Loss: 0.0213494673371315 | MAE Test Loss: 0.04991454631090164 \n",
      "Epoch: 10880 | MAE Train Loss: 0.021315064281225204 | MAE Test Loss: 0.04983186721801758 \n",
      "Epoch: 10890 | MAE Train Loss: 0.021280691027641296 | MAE Test Loss: 0.04975605010986328 \n",
      "Epoch: 10900 | MAE Train Loss: 0.021246328949928284 | MAE Test Loss: 0.049669958651065826 \n",
      "Epoch: 10910 | MAE Train Loss: 0.021212005987763405 | MAE Test Loss: 0.04959413409233093 \n",
      "Epoch: 10920 | MAE Train Loss: 0.02117762342095375 | MAE Test Loss: 0.049514900892972946 \n",
      "Epoch: 10930 | MAE Train Loss: 0.021143313497304916 | MAE Test Loss: 0.04943222552537918 \n",
      "Epoch: 10940 | MAE Train Loss: 0.021108904853463173 | MAE Test Loss: 0.04934953898191452 \n",
      "Epoch: 10950 | MAE Train Loss: 0.02107454277575016 | MAE Test Loss: 0.04927372932434082 \n",
      "Epoch: 10960 | MAE Train Loss: 0.021040167659521103 | MAE Test Loss: 0.04918763041496277 \n",
      "Epoch: 10970 | MAE Train Loss: 0.02100583352148533 | MAE Test Loss: 0.04910840839147568 \n",
      "Epoch: 10980 | MAE Train Loss: 0.020971467718482018 | MAE Test Loss: 0.04903259873390198 \n",
      "Epoch: 10990 | MAE Train Loss: 0.020937152206897736 | MAE Test Loss: 0.048949915915727615 \n",
      "Epoch: 11000 | MAE Train Loss: 0.02090274728834629 | MAE Test Loss: 0.04886721819639206 \n",
      "Epoch: 11010 | MAE Train Loss: 0.020868411287665367 | MAE Test Loss: 0.04878799989819527 \n",
      "Epoch: 11020 | MAE Train Loss: 0.020834006369113922 | MAE Test Loss: 0.04870877414941788 \n",
      "Epoch: 11030 | MAE Train Loss: 0.020799672231078148 | MAE Test Loss: 0.04862608388066292 \n",
      "Epoch: 11040 | MAE Train Loss: 0.02076532319188118 | MAE Test Loss: 0.04855026677250862 \n",
      "Epoch: 11050 | MAE Train Loss: 0.020730990916490555 | MAE Test Loss: 0.04846758395433426 \n",
      "Epoch: 11060 | MAE Train Loss: 0.02069663628935814 | MAE Test Loss: 0.04838835448026657 \n",
      "Epoch: 11070 | MAE Train Loss: 0.020662251859903336 | MAE Test Loss: 0.04830567166209221 \n",
      "Epoch: 11080 | MAE Train Loss: 0.020627912133932114 | MAE Test Loss: 0.04822644591331482 \n",
      "Epoch: 11090 | MAE Train Loss: 0.020593509078025818 | MAE Test Loss: 0.04814376309514046 \n",
      "Epoch: 11100 | MAE Train Loss: 0.020559173077344894 | MAE Test Loss: 0.04806794598698616 \n",
      "Epoch: 11110 | MAE Train Loss: 0.020524829626083374 | MAE Test Loss: 0.0479852631688118 \n",
      "Epoch: 11120 | MAE Train Loss: 0.020490480586886406 | MAE Test Loss: 0.04790603369474411 \n",
      "Epoch: 11130 | MAE Train Loss: 0.0204560998827219 | MAE Test Loss: 0.04782681539654732 \n",
      "Epoch: 11140 | MAE Train Loss: 0.020421750843524933 | MAE Test Loss: 0.047744132578372955 \n",
      "Epoch: 11150 | MAE Train Loss: 0.020387345924973488 | MAE Test Loss: 0.047661442309617996 \n",
      "Epoch: 11160 | MAE Train Loss: 0.02035301923751831 | MAE Test Loss: 0.0475856252014637 \n",
      "Epoch: 11170 | MAE Train Loss: 0.020318638533353806 | MAE Test Loss: 0.04750639945268631 \n",
      "Epoch: 11180 | MAE Train Loss: 0.020284274592995644 | MAE Test Loss: 0.04742031544446945 \n",
      "Epoch: 11190 | MAE Train Loss: 0.020249947905540466 | MAE Test Loss: 0.04734448716044426 \n",
      "Epoch: 11200 | MAE Train Loss: 0.020215589553117752 | MAE Test Loss: 0.0472618043422699 \n",
      "Epoch: 11210 | MAE Train Loss: 0.020181182771921158 | MAE Test Loss: 0.04717911407351494 \n",
      "Epoch: 11220 | MAE Train Loss: 0.020146852359175682 | MAE Test Loss: 0.047099895775318146 \n",
      "Epoch: 11230 | MAE Train Loss: 0.02011248655617237 | MAE Test Loss: 0.04702407866716385 \n",
      "Epoch: 11240 | MAE Train Loss: 0.020078105852007866 | MAE Test Loss: 0.04693799093365669 \n",
      "Epoch: 11250 | MAE Train Loss: 0.02004380151629448 | MAE Test Loss: 0.0468621663749218 \n",
      "Epoch: 11260 | MAE Train Loss: 0.02000942826271057 | MAE Test Loss: 0.046779483556747437 \n",
      "Epoch: 11270 | MAE Train Loss: 0.019975025206804276 | MAE Test Loss: 0.04670025780797005 \n",
      "Epoch: 11280 | MAE Train Loss: 0.019940685480833054 | MAE Test Loss: 0.04661757871508598 \n",
      "Epoch: 11290 | MAE Train Loss: 0.01990635320544243 | MAE Test Loss: 0.046538345515728 \n",
      "Epoch: 11300 | MAE Train Loss: 0.01987195387482643 | MAE Test Loss: 0.0464625246822834 \n",
      "Epoch: 11310 | MAE Train Loss: 0.019837647676467896 | MAE Test Loss: 0.046379853039979935 \n",
      "Epoch: 11320 | MAE Train Loss: 0.01980326697230339 | MAE Test Loss: 0.046297162771224976 \n",
      "Epoch: 11330 | MAE Train Loss: 0.01976892724633217 | MAE Test Loss: 0.04621792957186699 \n",
      "Epoch: 11340 | MAE Train Loss: 0.01973458006978035 | MAE Test Loss: 0.0461387112736702 \n",
      "Epoch: 11350 | MAE Train Loss: 0.0197001900523901 | MAE Test Loss: 0.04605603963136673 \n",
      "Epoch: 11360 | MAE Train Loss: 0.019665801897644997 | MAE Test Loss: 0.04598020762205124 \n",
      "Epoch: 11370 | MAE Train Loss: 0.01963149569928646 | MAE Test Loss: 0.045897532254457474 \n",
      "Epoch: 11380 | MAE Train Loss: 0.019597113132476807 | MAE Test Loss: 0.04581829905509949 \n",
      "Epoch: 11390 | MAE Train Loss: 0.019562769681215286 | MAE Test Loss: 0.04573562741279602 \n",
      "Epoch: 11400 | MAE Train Loss: 0.019528424367308617 | MAE Test Loss: 0.045656394213438034 \n",
      "Epoch: 11410 | MAE Train Loss: 0.01949402317404747 | MAE Test Loss: 0.045573703944683075 \n",
      "Epoch: 11420 | MAE Train Loss: 0.019459649920463562 | MAE Test Loss: 0.045497894287109375 \n",
      "Epoch: 11430 | MAE Train Loss: 0.01942528784275055 | MAE Test Loss: 0.04541180282831192 \n",
      "Epoch: 11440 | MAE Train Loss: 0.01939096488058567 | MAE Test Loss: 0.045335978269577026 \n",
      "Epoch: 11450 | MAE Train Loss: 0.019356582313776016 | MAE Test Loss: 0.04525674507021904 \n",
      "Epoch: 11460 | MAE Train Loss: 0.019322272390127182 | MAE Test Loss: 0.045174069702625275 \n",
      "Epoch: 11470 | MAE Train Loss: 0.01928786374628544 | MAE Test Loss: 0.045091383159160614 \n",
      "Epoch: 11480 | MAE Train Loss: 0.019253501668572426 | MAE Test Loss: 0.045015573501586914 \n",
      "Epoch: 11490 | MAE Train Loss: 0.01921912655234337 | MAE Test Loss: 0.04492947459220886 \n",
      "Epoch: 11500 | MAE Train Loss: 0.019184792414307594 | MAE Test Loss: 0.04485025256872177 \n",
      "Epoch: 11510 | MAE Train Loss: 0.019150426611304283 | MAE Test Loss: 0.04477444291114807 \n",
      "Epoch: 11520 | MAE Train Loss: 0.01911611109972 | MAE Test Loss: 0.04469176009297371 \n",
      "Epoch: 11530 | MAE Train Loss: 0.019081706181168556 | MAE Test Loss: 0.04460906237363815 \n",
      "Epoch: 11540 | MAE Train Loss: 0.019047370180487633 | MAE Test Loss: 0.04452984407544136 \n",
      "Epoch: 11550 | MAE Train Loss: 0.019012965261936188 | MAE Test Loss: 0.04445061832666397 \n",
      "Epoch: 11560 | MAE Train Loss: 0.018978631123900414 | MAE Test Loss: 0.04436792805790901 \n",
      "Epoch: 11570 | MAE Train Loss: 0.018944282084703445 | MAE Test Loss: 0.044292110949754715 \n",
      "Epoch: 11580 | MAE Train Loss: 0.01890994980931282 | MAE Test Loss: 0.04420942813158035 \n",
      "Epoch: 11590 | MAE Train Loss: 0.018875595182180405 | MAE Test Loss: 0.044130198657512665 \n",
      "Epoch: 11600 | MAE Train Loss: 0.0188412107527256 | MAE Test Loss: 0.0440475158393383 \n",
      "Epoch: 11610 | MAE Train Loss: 0.01880687102675438 | MAE Test Loss: 0.04396829009056091 \n",
      "Epoch: 11620 | MAE Train Loss: 0.018772467970848083 | MAE Test Loss: 0.04388560727238655 \n",
      "Epoch: 11630 | MAE Train Loss: 0.01873813197016716 | MAE Test Loss: 0.043809790164232254 \n",
      "Epoch: 11640 | MAE Train Loss: 0.01870378851890564 | MAE Test Loss: 0.04372710734605789 \n",
      "Epoch: 11650 | MAE Train Loss: 0.01866944134235382 | MAE Test Loss: 0.043647877871990204 \n",
      "Epoch: 11660 | MAE Train Loss: 0.018635058775544167 | MAE Test Loss: 0.04356865957379341 \n",
      "Epoch: 11670 | MAE Train Loss: 0.0186007097363472 | MAE Test Loss: 0.04348597675561905 \n",
      "Epoch: 11680 | MAE Train Loss: 0.018566306680440903 | MAE Test Loss: 0.04340328648686409 \n",
      "Epoch: 11690 | MAE Train Loss: 0.018531978130340576 | MAE Test Loss: 0.04332746937870979 \n",
      "Epoch: 11700 | MAE Train Loss: 0.018497595563530922 | MAE Test Loss: 0.043248243629932404 \n",
      "Epoch: 11710 | MAE Train Loss: 0.01846323348581791 | MAE Test Loss: 0.04316215589642525 \n",
      "Epoch: 11720 | MAE Train Loss: 0.018428906798362732 | MAE Test Loss: 0.04308633133769035 \n",
      "Epoch: 11730 | MAE Train Loss: 0.018394548445940018 | MAE Test Loss: 0.043003641068935394 \n",
      "Epoch: 11740 | MAE Train Loss: 0.018360141664743423 | MAE Test Loss: 0.04292095825076103 \n",
      "Epoch: 11750 | MAE Train Loss: 0.018325811251997948 | MAE Test Loss: 0.04284173995256424 \n",
      "Epoch: 11760 | MAE Train Loss: 0.018291447311639786 | MAE Test Loss: 0.04276592284440994 \n",
      "Epoch: 11770 | MAE Train Loss: 0.01825706660747528 | MAE Test Loss: 0.042679835110902786 \n",
      "Epoch: 11780 | MAE Train Loss: 0.018222760409116745 | MAE Test Loss: 0.04260401055216789 \n",
      "Epoch: 11790 | MAE Train Loss: 0.018188387155532837 | MAE Test Loss: 0.04252132773399353 \n",
      "Epoch: 11800 | MAE Train Loss: 0.01815398409962654 | MAE Test Loss: 0.04244210198521614 \n",
      "Epoch: 11810 | MAE Train Loss: 0.01811964437365532 | MAE Test Loss: 0.04235942289233208 \n",
      "Epoch: 11820 | MAE Train Loss: 0.018085310235619545 | MAE Test Loss: 0.04228019714355469 \n",
      "Epoch: 11830 | MAE Train Loss: 0.018050912767648697 | MAE Test Loss: 0.042204368859529495 \n",
      "Epoch: 11840 | MAE Train Loss: 0.01801660656929016 | MAE Test Loss: 0.04212169721722603 \n",
      "Epoch: 11850 | MAE Train Loss: 0.017982225865125656 | MAE Test Loss: 0.042039014399051666 \n",
      "Epoch: 11860 | MAE Train Loss: 0.017947886139154434 | MAE Test Loss: 0.04195977374911308 \n",
      "Epoch: 11870 | MAE Train Loss: 0.017913538962602615 | MAE Test Loss: 0.04188055545091629 \n",
      "Epoch: 11880 | MAE Train Loss: 0.017879148945212364 | MAE Test Loss: 0.04179787635803223 \n",
      "Epoch: 11890 | MAE Train Loss: 0.01784476265311241 | MAE Test Loss: 0.04172205179929733 \n",
      "Epoch: 11900 | MAE Train Loss: 0.017810454592108727 | MAE Test Loss: 0.04163937643170357 \n",
      "Epoch: 11910 | MAE Train Loss: 0.017776072025299072 | MAE Test Loss: 0.04156014323234558 \n",
      "Epoch: 11920 | MAE Train Loss: 0.017741728574037552 | MAE Test Loss: 0.041477471590042114 \n",
      "Epoch: 11930 | MAE Train Loss: 0.017707383260130882 | MAE Test Loss: 0.04139823839068413 \n",
      "Epoch: 11940 | MAE Train Loss: 0.017672982066869736 | MAE Test Loss: 0.04131554812192917 \n",
      "Epoch: 11950 | MAE Train Loss: 0.017638608813285828 | MAE Test Loss: 0.04123973846435547 \n",
      "Epoch: 11960 | MAE Train Loss: 0.017604246735572815 | MAE Test Loss: 0.041153647005558014 \n",
      "Epoch: 11970 | MAE Train Loss: 0.017569923773407936 | MAE Test Loss: 0.04107782989740372 \n",
      "Epoch: 11980 | MAE Train Loss: 0.017535541206598282 | MAE Test Loss: 0.040998589247465134 \n",
      "Epoch: 11990 | MAE Train Loss: 0.017501231282949448 | MAE Test Loss: 0.04091591387987137 \n",
      "Epoch: 12000 | MAE Train Loss: 0.017466822639107704 | MAE Test Loss: 0.040833234786987305 \n",
      "Epoch: 12010 | MAE Train Loss: 0.01743246056139469 | MAE Test Loss: 0.04075741767883301 \n",
      "Epoch: 12020 | MAE Train Loss: 0.017398085445165634 | MAE Test Loss: 0.040671318769454956 \n",
      "Epoch: 12030 | MAE Train Loss: 0.01736375130712986 | MAE Test Loss: 0.040592093020677567 \n",
      "Epoch: 12040 | MAE Train Loss: 0.01732938550412655 | MAE Test Loss: 0.040516287088394165 \n",
      "Epoch: 12050 | MAE Train Loss: 0.017295069992542267 | MAE Test Loss: 0.0404336042702198 \n",
      "Epoch: 12060 | MAE Train Loss: 0.017260665073990822 | MAE Test Loss: 0.04035090655088425 \n",
      "Epoch: 12070 | MAE Train Loss: 0.0172263290733099 | MAE Test Loss: 0.040271688252687454 \n",
      "Epoch: 12080 | MAE Train Loss: 0.017191924154758453 | MAE Test Loss: 0.040192462503910065 \n",
      "Epoch: 12090 | MAE Train Loss: 0.01715759001672268 | MAE Test Loss: 0.040109772235155106 \n",
      "Epoch: 12100 | MAE Train Loss: 0.01712324097752571 | MAE Test Loss: 0.04003395512700081 \n",
      "Epoch: 12110 | MAE Train Loss: 0.017088908702135086 | MAE Test Loss: 0.03995126485824585 \n",
      "Epoch: 12120 | MAE Train Loss: 0.01705455407500267 | MAE Test Loss: 0.039872050285339355 \n",
      "Epoch: 12130 | MAE Train Loss: 0.017020169645547867 | MAE Test Loss: 0.039789360016584396 \n",
      "Epoch: 12140 | MAE Train Loss: 0.016985829919576645 | MAE Test Loss: 0.03971013426780701 \n",
      "Epoch: 12150 | MAE Train Loss: 0.01695142686367035 | MAE Test Loss: 0.03962745517492294 \n",
      "Epoch: 12160 | MAE Train Loss: 0.016917090862989426 | MAE Test Loss: 0.03955163434147835 \n",
      "Epoch: 12170 | MAE Train Loss: 0.016882747411727905 | MAE Test Loss: 0.039468951523303986 \n",
      "Epoch: 12180 | MAE Train Loss: 0.016848400235176086 | MAE Test Loss: 0.0393897220492363 \n",
      "Epoch: 12190 | MAE Train Loss: 0.016814017668366432 | MAE Test Loss: 0.039310503751039505 \n",
      "Epoch: 12200 | MAE Train Loss: 0.016779668629169464 | MAE Test Loss: 0.03922782093286514 \n",
      "Epoch: 12210 | MAE Train Loss: 0.01674526557326317 | MAE Test Loss: 0.039145130664110184 \n",
      "Epoch: 12220 | MAE Train Loss: 0.016710937023162842 | MAE Test Loss: 0.03906931355595589 \n",
      "Epoch: 12230 | MAE Train Loss: 0.016676554456353188 | MAE Test Loss: 0.0389900803565979 \n",
      "Epoch: 12240 | MAE Train Loss: 0.016642192378640175 | MAE Test Loss: 0.03890400007367134 \n",
      "Epoch: 12250 | MAE Train Loss: 0.016607865691184998 | MAE Test Loss: 0.03882817551493645 \n",
      "Epoch: 12260 | MAE Train Loss: 0.016573509201407433 | MAE Test Loss: 0.03874548152089119 \n",
      "Epoch: 12270 | MAE Train Loss: 0.01653910055756569 | MAE Test Loss: 0.038662802428007126 \n",
      "Epoch: 12280 | MAE Train Loss: 0.016504770144820213 | MAE Test Loss: 0.03858358412981033 \n",
      "Epoch: 12290 | MAE Train Loss: 0.01647040620446205 | MAE Test Loss: 0.038507767021656036 \n",
      "Epoch: 12300 | MAE Train Loss: 0.016436023637652397 | MAE Test Loss: 0.03842168301343918 \n",
      "Epoch: 12310 | MAE Train Loss: 0.01640171930193901 | MAE Test Loss: 0.038345854729413986 \n",
      "Epoch: 12320 | MAE Train Loss: 0.016367346048355103 | MAE Test Loss: 0.038263171911239624 \n",
      "Epoch: 12330 | MAE Train Loss: 0.016332942992448807 | MAE Test Loss: 0.038183946162462234 \n",
      "Epoch: 12340 | MAE Train Loss: 0.016298603266477585 | MAE Test Loss: 0.03810126706957817 \n",
      "Epoch: 12350 | MAE Train Loss: 0.01626426912844181 | MAE Test Loss: 0.03802204132080078 \n",
      "Epoch: 12360 | MAE Train Loss: 0.016229871660470963 | MAE Test Loss: 0.03794621676206589 \n",
      "Epoch: 12370 | MAE Train Loss: 0.016195565462112427 | MAE Test Loss: 0.03786354139447212 \n",
      "Epoch: 12380 | MAE Train Loss: 0.016161184757947922 | MAE Test Loss: 0.03778085857629776 \n",
      "Epoch: 12390 | MAE Train Loss: 0.0161268450319767 | MAE Test Loss: 0.03770161792635918 \n",
      "Epoch: 12400 | MAE Train Loss: 0.01609249785542488 | MAE Test Loss: 0.037622399628162384 \n",
      "Epoch: 12410 | MAE Train Loss: 0.01605810783803463 | MAE Test Loss: 0.03753972053527832 \n",
      "Epoch: 12420 | MAE Train Loss: 0.016023719683289528 | MAE Test Loss: 0.037463895976543427 \n",
      "Epoch: 12430 | MAE Train Loss: 0.015989413484930992 | MAE Test Loss: 0.03738122060894966 \n",
      "Epoch: 12440 | MAE Train Loss: 0.015955032780766487 | MAE Test Loss: 0.037301987409591675 \n",
      "Epoch: 12450 | MAE Train Loss: 0.015920687466859818 | MAE Test Loss: 0.03721931576728821 \n",
      "Epoch: 12460 | MAE Train Loss: 0.015886342152953148 | MAE Test Loss: 0.03714008256793022 \n",
      "Epoch: 12470 | MAE Train Loss: 0.015851940959692 | MAE Test Loss: 0.03705739229917526 \n",
      "Epoch: 12480 | MAE Train Loss: 0.015817567706108093 | MAE Test Loss: 0.03698158264160156 \n",
      "Epoch: 12490 | MAE Train Loss: 0.01578320562839508 | MAE Test Loss: 0.03689549118280411 \n",
      "Epoch: 12500 | MAE Train Loss: 0.0157488826662302 | MAE Test Loss: 0.03681967407464981 \n",
      "Epoch: 12510 | MAE Train Loss: 0.015714500099420547 | MAE Test Loss: 0.03674043342471123 \n",
      "Epoch: 12520 | MAE Train Loss: 0.015680190175771713 | MAE Test Loss: 0.03665775805711746 \n",
      "Epoch: 12530 | MAE Train Loss: 0.01564578153192997 | MAE Test Loss: 0.0365750789642334 \n",
      "Epoch: 12540 | MAE Train Loss: 0.015611419454216957 | MAE Test Loss: 0.0364992618560791 \n",
      "Epoch: 12550 | MAE Train Loss: 0.015577045269310474 | MAE Test Loss: 0.03641316294670105 \n",
      "Epoch: 12560 | MAE Train Loss: 0.0155427111312747 | MAE Test Loss: 0.03633393719792366 \n",
      "Epoch: 12570 | MAE Train Loss: 0.01550834160298109 | MAE Test Loss: 0.03625813126564026 \n",
      "Epoch: 12580 | MAE Train Loss: 0.015474028885364532 | MAE Test Loss: 0.0361754484474659 \n",
      "Epoch: 12590 | MAE Train Loss: 0.015439623966813087 | MAE Test Loss: 0.03609275072813034 \n",
      "Epoch: 12600 | MAE Train Loss: 0.015405287966132164 | MAE Test Loss: 0.03601353242993355 \n",
      "Epoch: 12610 | MAE Train Loss: 0.015370883047580719 | MAE Test Loss: 0.03593430668115616 \n",
      "Epoch: 12620 | MAE Train Loss: 0.015336548909544945 | MAE Test Loss: 0.0358516164124012 \n",
      "Epoch: 12630 | MAE Train Loss: 0.015302200801670551 | MAE Test Loss: 0.0357757993042469 \n",
      "Epoch: 12640 | MAE Train Loss: 0.015267866663634777 | MAE Test Loss: 0.03569310903549194 \n",
      "Epoch: 12650 | MAE Train Loss: 0.01523351389914751 | MAE Test Loss: 0.03561389446258545 \n",
      "Epoch: 12660 | MAE Train Loss: 0.015199127607047558 | MAE Test Loss: 0.03553120419383049 \n",
      "Epoch: 12670 | MAE Train Loss: 0.015164789743721485 | MAE Test Loss: 0.0354519784450531 \n",
      "Epoch: 12680 | MAE Train Loss: 0.015130385756492615 | MAE Test Loss: 0.03536929935216904 \n",
      "Epoch: 12690 | MAE Train Loss: 0.015096050687134266 | MAE Test Loss: 0.03529347851872444 \n",
      "Epoch: 12700 | MAE Train Loss: 0.015061704441905022 | MAE Test Loss: 0.03521079570055008 \n",
      "Epoch: 12710 | MAE Train Loss: 0.015027357265353203 | MAE Test Loss: 0.03513156250119209 \n",
      "Epoch: 12720 | MAE Train Loss: 0.014992979355156422 | MAE Test Loss: 0.035052340477705 \n",
      "Epoch: 12730 | MAE Train Loss: 0.014958630315959454 | MAE Test Loss: 0.03496966511011124 \n",
      "Epoch: 12740 | MAE Train Loss: 0.014924225397408009 | MAE Test Loss: 0.03488697484135628 \n",
      "Epoch: 12750 | MAE Train Loss: 0.014889898709952831 | MAE Test Loss: 0.03481115773320198 \n",
      "Epoch: 12760 | MAE Train Loss: 0.014855511486530304 | MAE Test Loss: 0.034731924533843994 \n",
      "Epoch: 12770 | MAE Train Loss: 0.01482115127146244 | MAE Test Loss: 0.03464584797620773 \n",
      "Epoch: 12780 | MAE Train Loss: 0.014786824584007263 | MAE Test Loss: 0.03457002714276314 \n",
      "Epoch: 12790 | MAE Train Loss: 0.014752468094229698 | MAE Test Loss: 0.03448732942342758 \n",
      "Epoch: 12800 | MAE Train Loss: 0.01471805851906538 | MAE Test Loss: 0.034404654055833817 \n",
      "Epoch: 12810 | MAE Train Loss: 0.014683729037642479 | MAE Test Loss: 0.03432542830705643 \n",
      "Epoch: 12820 | MAE Train Loss: 0.014649364165961742 | MAE Test Loss: 0.03424961119890213 \n",
      "Epoch: 12830 | MAE Train Loss: 0.014614982530474663 | MAE Test Loss: 0.034163523465394974 \n",
      "Epoch: 12840 | MAE Train Loss: 0.014580677263438702 | MAE Test Loss: 0.03408769518136978 \n",
      "Epoch: 12850 | MAE Train Loss: 0.014546304941177368 | MAE Test Loss: 0.03400500863790512 \n",
      "Epoch: 12860 | MAE Train Loss: 0.014511900953948498 | MAE Test Loss: 0.03392579033970833 \n",
      "Epoch: 12870 | MAE Train Loss: 0.014477565884590149 | MAE Test Loss: 0.033843111246824265 \n",
      "Epoch: 12880 | MAE Train Loss: 0.014443226158618927 | MAE Test Loss: 0.033763885498046875 \n",
      "Epoch: 12890 | MAE Train Loss: 0.014408831484615803 | MAE Test Loss: 0.03368806093931198 \n",
      "Epoch: 12900 | MAE Train Loss: 0.014374524354934692 | MAE Test Loss: 0.033605385571718216 \n",
      "Epoch: 12910 | MAE Train Loss: 0.014340144582092762 | MAE Test Loss: 0.033522702753543854 \n",
      "Epoch: 12920 | MAE Train Loss: 0.01430580299347639 | MAE Test Loss: 0.03344346955418587 \n",
      "Epoch: 12930 | MAE Train Loss: 0.014271457679569721 | MAE Test Loss: 0.03336424380540848 \n",
      "Epoch: 12940 | MAE Train Loss: 0.014237066730856895 | MAE Test Loss: 0.033281564712524414 \n",
      "Epoch: 12950 | MAE Train Loss: 0.014202679507434368 | MAE Test Loss: 0.03320574015378952 \n",
      "Epoch: 12960 | MAE Train Loss: 0.014168371446430683 | MAE Test Loss: 0.033123064786195755 \n",
      "Epoch: 12970 | MAE Train Loss: 0.014133989810943604 | MAE Test Loss: 0.03304382413625717 \n",
      "Epoch: 12980 | MAE Train Loss: 0.014099645428359509 | MAE Test Loss: 0.032961152493953705 \n",
      "Epoch: 12990 | MAE Train Loss: 0.014065304771065712 | MAE Test Loss: 0.032881926745176315 \n",
      "Epoch: 13000 | MAE Train Loss: 0.014030903577804565 | MAE Test Loss: 0.032799236476421356 \n",
      "Epoch: 13010 | MAE Train Loss: 0.013996528461575508 | MAE Test Loss: 0.032723426818847656 \n",
      "Epoch: 13020 | MAE Train Loss: 0.013962164521217346 | MAE Test Loss: 0.0326373353600502 \n",
      "Epoch: 13030 | MAE Train Loss: 0.013927841559052467 | MAE Test Loss: 0.032561518251895905 \n",
      "Epoch: 13040 | MAE Train Loss: 0.013893458060920238 | MAE Test Loss: 0.03248228505253792 \n",
      "Epoch: 13050 | MAE Train Loss: 0.013859149999916553 | MAE Test Loss: 0.032399605959653854 \n",
      "Epoch: 13060 | MAE Train Loss: 0.013824740424752235 | MAE Test Loss: 0.03231693059206009 \n",
      "Epoch: 13070 | MAE Train Loss: 0.013790378347039223 | MAE Test Loss: 0.03224111348390579 \n",
      "Epoch: 13080 | MAE Train Loss: 0.01375600229948759 | MAE Test Loss: 0.032155007123947144 \n",
      "Epoch: 13090 | MAE Train Loss: 0.013721669092774391 | MAE Test Loss: 0.03207577392458916 \n",
      "Epoch: 13100 | MAE Train Loss: 0.013687300495803356 | MAE Test Loss: 0.031999967992305756 \n",
      "Epoch: 13110 | MAE Train Loss: 0.013652987778186798 | MAE Test Loss: 0.03191728517413139 \n",
      "Epoch: 13120 | MAE Train Loss: 0.013618582859635353 | MAE Test Loss: 0.031834591180086136 \n",
      "Epoch: 13130 | MAE Train Loss: 0.013584248721599579 | MAE Test Loss: 0.031755369156599045 \n",
      "Epoch: 13140 | MAE Train Loss: 0.013549843803048134 | MAE Test Loss: 0.03167615085840225 \n",
      "Epoch: 13150 | MAE Train Loss: 0.01351550780236721 | MAE Test Loss: 0.03159346058964729 \n",
      "Epoch: 13160 | MAE Train Loss: 0.013481159694492817 | MAE Test Loss: 0.031517643481492996 \n",
      "Epoch: 13170 | MAE Train Loss: 0.013446825556457043 | MAE Test Loss: 0.03143495321273804 \n",
      "Epoch: 13180 | MAE Train Loss: 0.013412472791969776 | MAE Test Loss: 0.03135574609041214 \n",
      "Epoch: 13190 | MAE Train Loss: 0.013378086499869823 | MAE Test Loss: 0.03127305582165718 \n",
      "Epoch: 13200 | MAE Train Loss: 0.013343746773898602 | MAE Test Loss: 0.031193822622299194 \n",
      "Epoch: 13210 | MAE Train Loss: 0.01330934464931488 | MAE Test Loss: 0.03111114539206028 \n",
      "Epoch: 13220 | MAE Train Loss: 0.013275009579956532 | MAE Test Loss: 0.031035322695970535 \n",
      "Epoch: 13230 | MAE Train Loss: 0.013240665197372437 | MAE Test Loss: 0.030952638015151024 \n",
      "Epoch: 13240 | MAE Train Loss: 0.013206318020820618 | MAE Test Loss: 0.030873406678438187 \n",
      "Epoch: 13250 | MAE Train Loss: 0.013171938247978687 | MAE Test Loss: 0.030794184654951096 \n",
      "Epoch: 13260 | MAE Train Loss: 0.013137588277459145 | MAE Test Loss: 0.03071150742471218 \n",
      "Epoch: 13270 | MAE Train Loss: 0.013103184290230274 | MAE Test Loss: 0.03062881901860237 \n",
      "Epoch: 13280 | MAE Train Loss: 0.013068857602775097 | MAE Test Loss: 0.030553001910448074 \n",
      "Epoch: 13290 | MAE Train Loss: 0.01303447037935257 | MAE Test Loss: 0.030473768711090088 \n",
      "Epoch: 13300 | MAE Train Loss: 0.013000110164284706 | MAE Test Loss: 0.030387694016098976 \n",
      "Epoch: 13310 | MAE Train Loss: 0.012965783476829529 | MAE Test Loss: 0.03031187132000923 \n",
      "Epoch: 13320 | MAE Train Loss: 0.012931426987051964 | MAE Test Loss: 0.030229175463318825 \n",
      "Epoch: 13330 | MAE Train Loss: 0.012897017411887646 | MAE Test Loss: 0.03014649823307991 \n",
      "Epoch: 13340 | MAE Train Loss: 0.012862687930464745 | MAE Test Loss: 0.03006727062165737 \n",
      "Epoch: 13350 | MAE Train Loss: 0.012828323058784008 | MAE Test Loss: 0.029991453513503075 \n",
      "Epoch: 13360 | MAE Train Loss: 0.012793943285942078 | MAE Test Loss: 0.029905367642641068 \n",
      "Epoch: 13370 | MAE Train Loss: 0.012759635224938393 | MAE Test Loss: 0.029829537495970726 \n",
      "Epoch: 13380 | MAE Train Loss: 0.012725263833999634 | MAE Test Loss: 0.029746854677796364 \n",
      "Epoch: 13390 | MAE Train Loss: 0.012690859846770763 | MAE Test Loss: 0.029667634516954422 \n",
      "Epoch: 13400 | MAE Train Loss: 0.012656524777412415 | MAE Test Loss: 0.02958495542407036 \n",
      "Epoch: 13410 | MAE Train Loss: 0.012622185051441193 | MAE Test Loss: 0.02950572967529297 \n",
      "Epoch: 13420 | MAE Train Loss: 0.012587790377438068 | MAE Test Loss: 0.029429906979203224 \n",
      "Epoch: 13430 | MAE Train Loss: 0.012553483247756958 | MAE Test Loss: 0.02934722974896431 \n",
      "Epoch: 13440 | MAE Train Loss: 0.012519103474915028 | MAE Test Loss: 0.0292645450681448 \n",
      "Epoch: 13450 | MAE Train Loss: 0.012484761886298656 | MAE Test Loss: 0.02918531373143196 \n",
      "Epoch: 13460 | MAE Train Loss: 0.012450416572391987 | MAE Test Loss: 0.029106086120009422 \n",
      "Epoch: 13470 | MAE Train Loss: 0.012416025623679161 | MAE Test Loss: 0.029023408889770508 \n",
      "Epoch: 13480 | MAE Train Loss: 0.012381638400256634 | MAE Test Loss: 0.028947586193680763 \n",
      "Epoch: 13490 | MAE Train Loss: 0.012347330339252949 | MAE Test Loss: 0.02886490896344185 \n",
      "Epoch: 13500 | MAE Train Loss: 0.012312949635088444 | MAE Test Loss: 0.028785670176148415 \n",
      "Epoch: 13510 | MAE Train Loss: 0.0122786033898592 | MAE Test Loss: 0.028702998533844948 \n",
      "Epoch: 13520 | MAE Train Loss: 0.012244263663887978 | MAE Test Loss: 0.02862377092242241 \n",
      "Epoch: 13530 | MAE Train Loss: 0.012209863401949406 | MAE Test Loss: 0.0285410825163126 \n",
      "Epoch: 13540 | MAE Train Loss: 0.012175487354397774 | MAE Test Loss: 0.02846527099609375 \n",
      "Epoch: 13550 | MAE Train Loss: 0.012141122482717037 | MAE Test Loss: 0.028379177674651146 \n",
      "Epoch: 13560 | MAE Train Loss: 0.012106799520552158 | MAE Test Loss: 0.02830336056649685 \n",
      "Epoch: 13570 | MAE Train Loss: 0.012072416953742504 | MAE Test Loss: 0.028224129229784012 \n",
      "Epoch: 13580 | MAE Train Loss: 0.012038109824061394 | MAE Test Loss: 0.028141450136899948 \n",
      "Epoch: 13590 | MAE Train Loss: 0.012003699317574501 | MAE Test Loss: 0.028058772906661034 \n",
      "Epoch: 13600 | MAE Train Loss: 0.011969337239861488 | MAE Test Loss: 0.027982955798506737 \n",
      "Epoch: 13610 | MAE Train Loss: 0.011934961192309856 | MAE Test Loss: 0.027896851301193237 \n",
      "Epoch: 13620 | MAE Train Loss: 0.011900627985596657 | MAE Test Loss: 0.02781761810183525 \n",
      "Epoch: 13630 | MAE Train Loss: 0.011866260319948196 | MAE Test Loss: 0.027741814032197 \n",
      "Epoch: 13640 | MAE Train Loss: 0.011831946671009064 | MAE Test Loss: 0.027659129351377487 \n",
      "Epoch: 13650 | MAE Train Loss: 0.011797541752457619 | MAE Test Loss: 0.02757643535733223 \n",
      "Epoch: 13660 | MAE Train Loss: 0.011763207614421844 | MAE Test Loss: 0.02749721333384514 \n",
      "Epoch: 13670 | MAE Train Loss: 0.011728804558515549 | MAE Test Loss: 0.027417993173003197 \n",
      "Epoch: 13680 | MAE Train Loss: 0.011694466695189476 | MAE Test Loss: 0.027335304766893387 \n",
      "Epoch: 13690 | MAE Train Loss: 0.011660118587315083 | MAE Test Loss: 0.02725948765873909 \n",
      "Epoch: 13700 | MAE Train Loss: 0.011625783517956734 | MAE Test Loss: 0.02717679738998413 \n",
      "Epoch: 13710 | MAE Train Loss: 0.011591430753469467 | MAE Test Loss: 0.027097588405013084 \n",
      "Epoch: 13720 | MAE Train Loss: 0.011557047255337238 | MAE Test Loss: 0.027014899998903275 \n",
      "Epoch: 13730 | MAE Train Loss: 0.011522707529366016 | MAE Test Loss: 0.026935666799545288 \n",
      "Epoch: 13740 | MAE Train Loss: 0.011488303542137146 | MAE Test Loss: 0.026852989569306374 \n",
      "Epoch: 13750 | MAE Train Loss: 0.011453966610133648 | MAE Test Loss: 0.02677716687321663 \n",
      "Epoch: 13760 | MAE Train Loss: 0.011419623158872128 | MAE Test Loss: 0.026694482192397118 \n",
      "Epoch: 13770 | MAE Train Loss: 0.011385277844965458 | MAE Test Loss: 0.02661525085568428 \n",
      "Epoch: 13780 | MAE Train Loss: 0.011350896209478378 | MAE Test Loss: 0.02653602883219719 \n",
      "Epoch: 13790 | MAE Train Loss: 0.01131654903292656 | MAE Test Loss: 0.026453351601958275 \n",
      "Epoch: 13800 | MAE Train Loss: 0.01128214318305254 | MAE Test Loss: 0.026370663195848465 \n",
      "Epoch: 13810 | MAE Train Loss: 0.011247815564274788 | MAE Test Loss: 0.026294846087694168 \n",
      "Epoch: 13820 | MAE Train Loss: 0.011213429272174835 | MAE Test Loss: 0.02621561288833618 \n",
      "Epoch: 13830 | MAE Train Loss: 0.011179068125784397 | MAE Test Loss: 0.02612953819334507 \n",
      "Epoch: 13840 | MAE Train Loss: 0.011144742369651794 | MAE Test Loss: 0.026053715497255325 \n",
      "Epoch: 13850 | MAE Train Loss: 0.01111038587987423 | MAE Test Loss: 0.02597101964056492 \n",
      "Epoch: 13860 | MAE Train Loss: 0.011075976304709911 | MAE Test Loss: 0.025888342410326004 \n",
      "Epoch: 13870 | MAE Train Loss: 0.011041645891964436 | MAE Test Loss: 0.025809114798903465 \n",
      "Epoch: 13880 | MAE Train Loss: 0.011007281951606274 | MAE Test Loss: 0.02573329769074917 \n",
      "Epoch: 13890 | MAE Train Loss: 0.010972903110086918 | MAE Test Loss: 0.02564721181988716 \n",
      "Epoch: 13900 | MAE Train Loss: 0.010938594117760658 | MAE Test Loss: 0.02557138167321682 \n",
      "Epoch: 13910 | MAE Train Loss: 0.010904221795499325 | MAE Test Loss: 0.025488698855042458 \n",
      "Epoch: 13920 | MAE Train Loss: 0.010869819670915604 | MAE Test Loss: 0.025409478694200516 \n",
      "Epoch: 13930 | MAE Train Loss: 0.010835482738912106 | MAE Test Loss: 0.025326799601316452 \n",
      "Epoch: 13940 | MAE Train Loss: 0.010801143944263458 | MAE Test Loss: 0.025247573852539062 \n",
      "Epoch: 13950 | MAE Train Loss: 0.010766749270260334 | MAE Test Loss: 0.025171751156449318 \n",
      "Epoch: 13960 | MAE Train Loss: 0.010732441209256649 | MAE Test Loss: 0.025089073926210403 \n",
      "Epoch: 13970 | MAE Train Loss: 0.010698061436414719 | MAE Test Loss: 0.025006389245390892 \n",
      "Epoch: 13980 | MAE Train Loss: 0.010663720779120922 | MAE Test Loss: 0.024927157908678055 \n",
      "Epoch: 13990 | MAE Train Loss: 0.010629375465214252 | MAE Test Loss: 0.024847930297255516 \n",
      "Epoch: 14000 | MAE Train Loss: 0.010594984516501427 | MAE Test Loss: 0.024765247479081154 \n",
      "Epoch: 14010 | MAE Train Loss: 0.010560599155724049 | MAE Test Loss: 0.024689430370926857 \n",
      "Epoch: 14020 | MAE Train Loss: 0.010526290163397789 | MAE Test Loss: 0.024606745690107346 \n",
      "Epoch: 14030 | MAE Train Loss: 0.01049190852791071 | MAE Test Loss: 0.02452751435339451 \n",
      "Epoch: 14040 | MAE Train Loss: 0.010457564145326614 | MAE Test Loss: 0.02444484271109104 \n",
      "Epoch: 14050 | MAE Train Loss: 0.010423222556710243 | MAE Test Loss: 0.024365615099668503 \n",
      "Epoch: 14060 | MAE Train Loss: 0.010388822294771671 | MAE Test Loss: 0.024282926693558693 \n",
      "Epoch: 14070 | MAE Train Loss: 0.01035444624722004 | MAE Test Loss: 0.024207115173339844 \n",
      "Epoch: 14080 | MAE Train Loss: 0.010320081375539303 | MAE Test Loss: 0.02412102185189724 \n",
      "Epoch: 14090 | MAE Train Loss: 0.010285758413374424 | MAE Test Loss: 0.024045204743742943 \n",
      "Epoch: 14100 | MAE Train Loss: 0.01025137584656477 | MAE Test Loss: 0.023965973407030106 \n",
      "Epoch: 14110 | MAE Train Loss: 0.01021706685423851 | MAE Test Loss: 0.023883294314146042 \n",
      "Epoch: 14120 | MAE Train Loss: 0.010182658210396767 | MAE Test Loss: 0.023800617083907127 \n",
      "Epoch: 14130 | MAE Train Loss: 0.010148297064006329 | MAE Test Loss: 0.02372479997575283 \n",
      "Epoch: 14140 | MAE Train Loss: 0.010113921947777271 | MAE Test Loss: 0.02363869547843933 \n",
      "Epoch: 14150 | MAE Train Loss: 0.010079586878418922 | MAE Test Loss: 0.023559462279081345 \n",
      "Epoch: 14160 | MAE Train Loss: 0.010045217350125313 | MAE Test Loss: 0.023483658209443092 \n",
      "Epoch: 14170 | MAE Train Loss: 0.01001090556383133 | MAE Test Loss: 0.02340097352862358 \n",
      "Epoch: 14180 | MAE Train Loss: 0.009976500645279884 | MAE Test Loss: 0.023318279534578323 \n",
      "Epoch: 14190 | MAE Train Loss: 0.00994216650724411 | MAE Test Loss: 0.023239057511091232 \n",
      "Epoch: 14200 | MAE Train Loss: 0.009907763451337814 | MAE Test Loss: 0.02315983735024929 \n",
      "Epoch: 14210 | MAE Train Loss: 0.009873425588011742 | MAE Test Loss: 0.02307714894413948 \n",
      "Epoch: 14220 | MAE Train Loss: 0.009839077480137348 | MAE Test Loss: 0.023001331835985184 \n",
      "Epoch: 14230 | MAE Train Loss: 0.009804742410779 | MAE Test Loss: 0.022918641567230225 \n",
      "Epoch: 14240 | MAE Train Loss: 0.009770389646291733 | MAE Test Loss: 0.022839432582259178 \n",
      "Epoch: 14250 | MAE Train Loss: 0.009736006148159504 | MAE Test Loss: 0.02275674417614937 \n",
      "Epoch: 14260 | MAE Train Loss: 0.009701666422188282 | MAE Test Loss: 0.022677510976791382 \n",
      "Epoch: 14270 | MAE Train Loss: 0.009667262434959412 | MAE Test Loss: 0.022594833746552467 \n",
      "Epoch: 14280 | MAE Train Loss: 0.009632925502955914 | MAE Test Loss: 0.022519011050462723 \n",
      "Epoch: 14290 | MAE Train Loss: 0.009598582051694393 | MAE Test Loss: 0.02243632636964321 \n",
      "Epoch: 14300 | MAE Train Loss: 0.009564236737787724 | MAE Test Loss: 0.022357095032930374 \n",
      "Epoch: 14310 | MAE Train Loss: 0.009529855102300644 | MAE Test Loss: 0.022277873009443283 \n",
      "Epoch: 14320 | MAE Train Loss: 0.009495507925748825 | MAE Test Loss: 0.02219519577920437 \n",
      "Epoch: 14330 | MAE Train Loss: 0.009461102075874805 | MAE Test Loss: 0.02211250737309456 \n",
      "Epoch: 14340 | MAE Train Loss: 0.009426774457097054 | MAE Test Loss: 0.022036690264940262 \n",
      "Epoch: 14350 | MAE Train Loss: 0.0093923881649971 | MAE Test Loss: 0.021957457065582275 \n",
      "Epoch: 14360 | MAE Train Loss: 0.009358027018606663 | MAE Test Loss: 0.021871382370591164 \n",
      "Epoch: 14370 | MAE Train Loss: 0.00932370126247406 | MAE Test Loss: 0.02179555967450142 \n",
      "Epoch: 14380 | MAE Train Loss: 0.009289344772696495 | MAE Test Loss: 0.021712863817811012 \n",
      "Epoch: 14390 | MAE Train Loss: 0.009254935197532177 | MAE Test Loss: 0.021630186587572098 \n",
      "Epoch: 14400 | MAE Train Loss: 0.009220604784786701 | MAE Test Loss: 0.02155095897614956 \n",
      "Epoch: 14410 | MAE Train Loss: 0.00918624084442854 | MAE Test Loss: 0.021475141867995262 \n",
      "Epoch: 14420 | MAE Train Loss: 0.009151862002909184 | MAE Test Loss: 0.021389055997133255 \n",
      "Epoch: 14430 | MAE Train Loss: 0.009117553010582924 | MAE Test Loss: 0.021313225850462914 \n",
      "Epoch: 14440 | MAE Train Loss: 0.00908318068832159 | MAE Test Loss: 0.02123054303228855 \n",
      "Epoch: 14450 | MAE Train Loss: 0.00904877856373787 | MAE Test Loss: 0.02115132287144661 \n",
      "Epoch: 14460 | MAE Train Loss: 0.009014441631734371 | MAE Test Loss: 0.021068643778562546 \n",
      "Epoch: 14470 | MAE Train Loss: 0.008980102837085724 | MAE Test Loss: 0.020989418029785156 \n",
      "Epoch: 14480 | MAE Train Loss: 0.0089457081630826 | MAE Test Loss: 0.02091359533369541 \n",
      "Epoch: 14490 | MAE Train Loss: 0.008911400102078915 | MAE Test Loss: 0.020830918103456497 \n",
      "Epoch: 14500 | MAE Train Loss: 0.008877020329236984 | MAE Test Loss: 0.020748233422636986 \n",
      "Epoch: 14510 | MAE Train Loss: 0.008842679671943188 | MAE Test Loss: 0.02066900208592415 \n",
      "Epoch: 14520 | MAE Train Loss: 0.008808334358036518 | MAE Test Loss: 0.02058977447450161 \n",
      "Epoch: 14530 | MAE Train Loss: 0.008773943409323692 | MAE Test Loss: 0.020507091656327248 \n",
      "Epoch: 14540 | MAE Train Loss: 0.008739558048546314 | MAE Test Loss: 0.02043127454817295 \n",
      "Epoch: 14550 | MAE Train Loss: 0.008705249056220055 | MAE Test Loss: 0.02034858986735344 \n",
      "Epoch: 14560 | MAE Train Loss: 0.008670867420732975 | MAE Test Loss: 0.020269358530640602 \n",
      "Epoch: 14570 | MAE Train Loss: 0.00863652303814888 | MAE Test Loss: 0.020186686888337135 \n",
      "Epoch: 14580 | MAE Train Loss: 0.008602181449532509 | MAE Test Loss: 0.020107459276914597 \n",
      "Epoch: 14590 | MAE Train Loss: 0.008567781187593937 | MAE Test Loss: 0.020024770870804787 \n",
      "Epoch: 14600 | MAE Train Loss: 0.008533405140042305 | MAE Test Loss: 0.019948959350585938 \n",
      "Epoch: 14610 | MAE Train Loss: 0.008499040268361568 | MAE Test Loss: 0.019862866029143333 \n",
      "Epoch: 14620 | MAE Train Loss: 0.00846471730619669 | MAE Test Loss: 0.019787048920989037 \n",
      "Epoch: 14630 | MAE Train Loss: 0.008430334739387035 | MAE Test Loss: 0.0197078175842762 \n",
      "Epoch: 14640 | MAE Train Loss: 0.008396025747060776 | MAE Test Loss: 0.019625138491392136 \n",
      "Epoch: 14650 | MAE Train Loss: 0.008361617103219032 | MAE Test Loss: 0.01954246126115322 \n",
      "Epoch: 14660 | MAE Train Loss: 0.008327255956828594 | MAE Test Loss: 0.019466644152998924 \n",
      "Epoch: 14670 | MAE Train Loss: 0.008292880840599537 | MAE Test Loss: 0.019380539655685425 \n",
      "Epoch: 14680 | MAE Train Loss: 0.008258545771241188 | MAE Test Loss: 0.01930130645632744 \n",
      "Epoch: 14690 | MAE Train Loss: 0.008224176242947578 | MAE Test Loss: 0.019225502386689186 \n",
      "Epoch: 14700 | MAE Train Loss: 0.008189864456653595 | MAE Test Loss: 0.019142817705869675 \n",
      "Epoch: 14710 | MAE Train Loss: 0.00815545953810215 | MAE Test Loss: 0.019060123711824417 \n",
      "Epoch: 14720 | MAE Train Loss: 0.008121125400066376 | MAE Test Loss: 0.018980901688337326 \n",
      "Epoch: 14730 | MAE Train Loss: 0.00808672234416008 | MAE Test Loss: 0.018901681527495384 \n",
      "Epoch: 14740 | MAE Train Loss: 0.008052384480834007 | MAE Test Loss: 0.018818993121385574 \n",
      "Epoch: 14750 | MAE Train Loss: 0.008018036372959614 | MAE Test Loss: 0.018743176013231277 \n",
      "Epoch: 14760 | MAE Train Loss: 0.007983701303601265 | MAE Test Loss: 0.01866048574447632 \n",
      "Epoch: 14770 | MAE Train Loss: 0.007949348539113998 | MAE Test Loss: 0.018581276759505272 \n",
      "Epoch: 14780 | MAE Train Loss: 0.00791496504098177 | MAE Test Loss: 0.018498588353395462 \n",
      "Epoch: 14790 | MAE Train Loss: 0.007880625315010548 | MAE Test Loss: 0.018419355154037476 \n",
      "Epoch: 14800 | MAE Train Loss: 0.007846221327781677 | MAE Test Loss: 0.01833667792379856 \n",
      "Epoch: 14810 | MAE Train Loss: 0.007811884395778179 | MAE Test Loss: 0.018260855227708817 \n",
      "Epoch: 14820 | MAE Train Loss: 0.007777539547532797 | MAE Test Loss: 0.018178170546889305 \n",
      "Epoch: 14830 | MAE Train Loss: 0.007743195630609989 | MAE Test Loss: 0.018098939210176468 \n",
      "Epoch: 14840 | MAE Train Loss: 0.0077088139951229095 | MAE Test Loss: 0.018019717186689377 \n",
      "Epoch: 14850 | MAE Train Loss: 0.007674466818571091 | MAE Test Loss: 0.017937039956450462 \n",
      "Epoch: 14860 | MAE Train Loss: 0.007640059106051922 | MAE Test Loss: 0.017854351550340652 \n",
      "Epoch: 14870 | MAE Train Loss: 0.0076057338155806065 | MAE Test Loss: 0.017778534442186356 \n",
      "Epoch: 14880 | MAE Train Loss: 0.0075713470578193665 | MAE Test Loss: 0.01769930124282837 \n",
      "Epoch: 14890 | MAE Train Loss: 0.007536985911428928 | MAE Test Loss: 0.017613226547837257 \n",
      "Epoch: 14900 | MAE Train Loss: 0.007502660155296326 | MAE Test Loss: 0.017537403851747513 \n",
      "Epoch: 14910 | MAE Train Loss: 0.007468304131180048 | MAE Test Loss: 0.017454707995057106 \n",
      "Epoch: 14920 | MAE Train Loss: 0.007433895952999592 | MAE Test Loss: 0.01737203076481819 \n",
      "Epoch: 14930 | MAE Train Loss: 0.007399563677608967 | MAE Test Loss: 0.017292803153395653 \n",
      "Epoch: 14940 | MAE Train Loss: 0.007365201599895954 | MAE Test Loss: 0.017216986045241356 \n",
      "Epoch: 14950 | MAE Train Loss: 0.007330820895731449 | MAE Test Loss: 0.01713090017437935 \n",
      "Epoch: 14960 | MAE Train Loss: 0.007296513766050339 | MAE Test Loss: 0.017055070027709007 \n",
      "Epoch: 14970 | MAE Train Loss: 0.007262139581143856 | MAE Test Loss: 0.016972387209534645 \n",
      "Epoch: 14980 | MAE Train Loss: 0.007227737456560135 | MAE Test Loss: 0.016893167048692703 \n",
      "Epoch: 14990 | MAE Train Loss: 0.007193400524556637 | MAE Test Loss: 0.01681048795580864 \n",
      "Epoch: 15000 | MAE Train Loss: 0.0071590617299079895 | MAE Test Loss: 0.01673126220703125 \n",
      "Epoch: 15010 | MAE Train Loss: 0.007124667055904865 | MAE Test Loss: 0.016655439510941505 \n",
      "Epoch: 15020 | MAE Train Loss: 0.00709035899490118 | MAE Test Loss: 0.01657276228070259 \n",
      "Epoch: 15030 | MAE Train Loss: 0.00705597922205925 | MAE Test Loss: 0.01649007759988308 \n",
      "Epoch: 15040 | MAE Train Loss: 0.007021640427410603 | MAE Test Loss: 0.016410846263170242 \n",
      "Epoch: 15050 | MAE Train Loss: 0.006987293250858784 | MAE Test Loss: 0.016331618651747704 \n",
      "Epoch: 15060 | MAE Train Loss: 0.006952904164791107 | MAE Test Loss: 0.01624893583357334 \n",
      "Epoch: 15070 | MAE Train Loss: 0.00691851694136858 | MAE Test Loss: 0.016173118725419044 \n",
      "Epoch: 15080 | MAE Train Loss: 0.0068842084147036076 | MAE Test Loss: 0.016090434044599533 \n",
      "Epoch: 15090 | MAE Train Loss: 0.006849826313555241 | MAE Test Loss: 0.016011202707886696 \n",
      "Epoch: 15100 | MAE Train Loss: 0.006815481930971146 | MAE Test Loss: 0.01592853106558323 \n",
      "Epoch: 15110 | MAE Train Loss: 0.006781139876693487 | MAE Test Loss: 0.01584930345416069 \n",
      "Epoch: 15120 | MAE Train Loss: 0.0067467400804162025 | MAE Test Loss: 0.01576661504805088 \n",
      "Epoch: 15130 | MAE Train Loss: 0.006712363567203283 | MAE Test Loss: 0.01569080352783203 \n",
      "Epoch: 15140 | MAE Train Loss: 0.006677999161183834 | MAE Test Loss: 0.015604710206389427 \n",
      "Epoch: 15150 | MAE Train Loss: 0.006643676199018955 | MAE Test Loss: 0.01552889309823513 \n",
      "Epoch: 15160 | MAE Train Loss: 0.006609293632209301 | MAE Test Loss: 0.015449660830199718 \n",
      "Epoch: 15170 | MAE Train Loss: 0.006574986036866903 | MAE Test Loss: 0.015366983599960804 \n",
      "Epoch: 15180 | MAE Train Loss: 0.006540576461702585 | MAE Test Loss: 0.015284305438399315 \n",
      "Epoch: 15190 | MAE Train Loss: 0.00650621484965086 | MAE Test Loss: 0.01520848274230957 \n",
      "Epoch: 15200 | MAE Train Loss: 0.0064718397334218025 | MAE Test Loss: 0.015122383832931519 \n",
      "Epoch: 15210 | MAE Train Loss: 0.006437505129724741 | MAE Test Loss: 0.015043151564896107 \n",
      "Epoch: 15220 | MAE Train Loss: 0.006403135601431131 | MAE Test Loss: 0.01496734656393528 \n",
      "Epoch: 15230 | MAE Train Loss: 0.006368823349475861 | MAE Test Loss: 0.014884662814438343 \n",
      "Epoch: 15240 | MAE Train Loss: 0.006334418896585703 | MAE Test Loss: 0.014801966957747936 \n",
      "Epoch: 15250 | MAE Train Loss: 0.006300084292888641 | MAE Test Loss: 0.014722746796905994 \n",
      "Epoch: 15260 | MAE Train Loss: 0.006265681236982346 | MAE Test Loss: 0.014643525704741478 \n",
      "Epoch: 15270 | MAE Train Loss: 0.006231342907994986 | MAE Test Loss: 0.014560836367309093 \n",
      "Epoch: 15280 | MAE Train Loss: 0.006196995265781879 | MAE Test Loss: 0.014485019259154797 \n",
      "Epoch: 15290 | MAE Train Loss: 0.006162659730762243 | MAE Test Loss: 0.014402329921722412 \n",
      "Epoch: 15300 | MAE Train Loss: 0.006128307431936264 | MAE Test Loss: 0.014323120936751366 \n",
      "Epoch: 15310 | MAE Train Loss: 0.006093923933804035 | MAE Test Loss: 0.014240431599318981 \n",
      "Epoch: 15320 | MAE Train Loss: 0.006059584207832813 | MAE Test Loss: 0.01416119933128357 \n",
      "Epoch: 15330 | MAE Train Loss: 0.006025180220603943 | MAE Test Loss: 0.014078522101044655 \n",
      "Epoch: 15340 | MAE Train Loss: 0.005990843288600445 | MAE Test Loss: 0.014002698473632336 \n",
      "Epoch: 15350 | MAE Train Loss: 0.0059564984403550625 | MAE Test Loss: 0.013920014724135399 \n",
      "Epoch: 15360 | MAE Train Loss: 0.005922154523432255 | MAE Test Loss: 0.013840782456099987 \n",
      "Epoch: 15370 | MAE Train Loss: 0.005887772887945175 | MAE Test Loss: 0.013761562295258045 \n",
      "Epoch: 15380 | MAE Train Loss: 0.005853425711393356 | MAE Test Loss: 0.013678884133696556 \n",
      "Epoch: 15390 | MAE Train Loss: 0.0058190179988741875 | MAE Test Loss: 0.013596194796264172 \n",
      "Epoch: 15400 | MAE Train Loss: 0.005784692708402872 | MAE Test Loss: 0.013520377688109875 \n",
      "Epoch: 15410 | MAE Train Loss: 0.005750305950641632 | MAE Test Loss: 0.013441145420074463 \n",
      "Epoch: 15420 | MAE Train Loss: 0.005715944804251194 | MAE Test Loss: 0.013355064205825329 \n",
      "Epoch: 15430 | MAE Train Loss: 0.005681618116796017 | MAE Test Loss: 0.013279247097671032 \n",
      "Epoch: 15440 | MAE Train Loss: 0.005647263024002314 | MAE Test Loss: 0.0131965521723032 \n",
      "Epoch: 15450 | MAE Train Loss: 0.005612855311483145 | MAE Test Loss: 0.01311387401074171 \n",
      "Epoch: 15460 | MAE Train Loss: 0.0055785225704312325 | MAE Test Loss: 0.013034647330641747 \n",
      "Epoch: 15470 | MAE Train Loss: 0.00554416049271822 | MAE Test Loss: 0.01295883022248745 \n",
      "Epoch: 15480 | MAE Train Loss: 0.005509779788553715 | MAE Test Loss: 0.012872743420302868 \n",
      "Epoch: 15490 | MAE Train Loss: 0.005475472658872604 | MAE Test Loss: 0.012796914204955101 \n",
      "Epoch: 15500 | MAE Train Loss: 0.005441098473966122 | MAE Test Loss: 0.012714231386780739 \n",
      "Epoch: 15510 | MAE Train Loss: 0.0054066963493824005 | MAE Test Loss: 0.012635010294616222 \n",
      "Epoch: 15520 | MAE Train Loss: 0.005372360348701477 | MAE Test Loss: 0.012552333064377308 \n",
      "Epoch: 15530 | MAE Train Loss: 0.005338020622730255 | MAE Test Loss: 0.012473106384277344 \n",
      "Epoch: 15540 | MAE Train Loss: 0.005303625948727131 | MAE Test Loss: 0.0123972836881876 \n",
      "Epoch: 15550 | MAE Train Loss: 0.0052693188190460205 | MAE Test Loss: 0.01231460552662611 \n",
      "Epoch: 15560 | MAE Train Loss: 0.0052349381148815155 | MAE Test Loss: 0.012231921777129173 \n",
      "Epoch: 15570 | MAE Train Loss: 0.0052005997858941555 | MAE Test Loss: 0.012152689509093761 \n",
      "Epoch: 15580 | MAE Train Loss: 0.005166252143681049 | MAE Test Loss: 0.012073462828993797 \n",
      "Epoch: 15590 | MAE Train Loss: 0.005131863057613373 | MAE Test Loss: 0.011990780010819435 \n",
      "Epoch: 15600 | MAE Train Loss: 0.0050974758341908455 | MAE Test Loss: 0.011914962902665138 \n",
      "Epoch: 15610 | MAE Train Loss: 0.005063167307525873 | MAE Test Loss: 0.011832279153168201 \n",
      "Epoch: 15620 | MAE Train Loss: 0.0050287856720387936 | MAE Test Loss: 0.01175304688513279 \n",
      "Epoch: 15630 | MAE Train Loss: 0.004994440823793411 | MAE Test Loss: 0.011670375242829323 \n",
      "Epoch: 15640 | MAE Train Loss: 0.004960098769515753 | MAE Test Loss: 0.011591148562729359 \n",
      "Epoch: 15650 | MAE Train Loss: 0.004925698973238468 | MAE Test Loss: 0.011508459225296974 \n",
      "Epoch: 15660 | MAE Train Loss: 0.004891322460025549 | MAE Test Loss: 0.011432647705078125 \n",
      "Epoch: 15670 | MAE Train Loss: 0.004856960382312536 | MAE Test Loss: 0.011346554383635521 \n",
      "Epoch: 15680 | MAE Train Loss: 0.004822633229196072 | MAE Test Loss: 0.011270737275481224 \n",
      "Epoch: 15690 | MAE Train Loss: 0.004788252525031567 | MAE Test Loss: 0.011191505007445812 \n",
      "Epoch: 15700 | MAE Train Loss: 0.004753944929689169 | MAE Test Loss: 0.011108827777206898 \n",
      "Epoch: 15710 | MAE Train Loss: 0.004719535354524851 | MAE Test Loss: 0.011026149615645409 \n",
      "Epoch: 15720 | MAE Train Loss: 0.004685175605118275 | MAE Test Loss: 0.010950326919555664 \n",
      "Epoch: 15730 | MAE Train Loss: 0.004650798626244068 | MAE Test Loss: 0.010864228010177612 \n",
      "Epoch: 15740 | MAE Train Loss: 0.004616464488208294 | MAE Test Loss: 0.0107849957421422 \n",
      "Epoch: 15750 | MAE Train Loss: 0.004582094494253397 | MAE Test Loss: 0.010709190741181374 \n",
      "Epoch: 15760 | MAE Train Loss: 0.004547781310975552 | MAE Test Loss: 0.010626506991684437 \n",
      "Epoch: 15770 | MAE Train Loss: 0.0045133777894079685 | MAE Test Loss: 0.01054381113499403 \n",
      "Epoch: 15780 | MAE Train Loss: 0.004479043185710907 | MAE Test Loss: 0.010464590974152088 \n",
      "Epoch: 15790 | MAE Train Loss: 0.004444638732820749 | MAE Test Loss: 0.010385369881987572 \n",
      "Epoch: 15800 | MAE Train Loss: 0.004410300403833389 | MAE Test Loss: 0.010302680544555187 \n",
      "Epoch: 15810 | MAE Train Loss: 0.004375954624265432 | MAE Test Loss: 0.01022686343640089 \n",
      "Epoch: 15820 | MAE Train Loss: 0.004341618623584509 | MAE Test Loss: 0.010144174098968506 \n",
      "Epoch: 15830 | MAE Train Loss: 0.00430726632475853 | MAE Test Loss: 0.01006496511399746 \n",
      "Epoch: 15840 | MAE Train Loss: 0.004272884223610163 | MAE Test Loss: 0.009982275776565075 \n",
      "Epoch: 15850 | MAE Train Loss: 0.004238543100655079 | MAE Test Loss: 0.009903043508529663 \n",
      "Epoch: 15860 | MAE Train Loss: 0.004204140044748783 | MAE Test Loss: 0.009820366278290749 \n",
      "Epoch: 15870 | MAE Train Loss: 0.00416980218142271 | MAE Test Loss: 0.00974454265087843 \n",
      "Epoch: 15880 | MAE Train Loss: 0.004135458264499903 | MAE Test Loss: 0.009661858901381493 \n",
      "Epoch: 15890 | MAE Train Loss: 0.00410111341625452 | MAE Test Loss: 0.00958262663334608 \n",
      "Epoch: 15900 | MAE Train Loss: 0.004066733177751303 | MAE Test Loss: 0.009503406472504139 \n",
      "Epoch: 15910 | MAE Train Loss: 0.00403238320723176 | MAE Test Loss: 0.00942072831094265 \n",
      "Epoch: 15920 | MAE Train Loss: 0.003997978754341602 | MAE Test Loss: 0.009338038973510265 \n",
      "Epoch: 15930 | MAE Train Loss: 0.003963652066886425 | MAE Test Loss: 0.009262221865355968 \n",
      "Epoch: 15940 | MAE Train Loss: 0.003929264843463898 | MAE Test Loss: 0.009182989597320557 \n",
      "Epoch: 15950 | MAE Train Loss: 0.003894903464242816 | MAE Test Loss: 0.009096908383071423 \n",
      "Epoch: 15960 | MAE Train Loss: 0.003860577242448926 | MAE Test Loss: 0.009021091274917126 \n",
      "Epoch: 15970 | MAE Train Loss: 0.003826223313808441 | MAE Test Loss: 0.008938396349549294 \n",
      "Epoch: 15980 | MAE Train Loss: 0.0037918128073215485 | MAE Test Loss: 0.008855718187987804 \n",
      "Epoch: 15990 | MAE Train Loss: 0.0037574812304228544 | MAE Test Loss: 0.00877649150788784 \n",
      "Epoch: 16000 | MAE Train Loss: 0.0037231191527098417 | MAE Test Loss: 0.008700674399733543 \n",
      "Epoch: 16010 | MAE Train Loss: 0.0036887384485453367 | MAE Test Loss: 0.008614587597548962 \n",
      "Epoch: 16020 | MAE Train Loss: 0.003654430154711008 | MAE Test Loss: 0.008538758382201195 \n",
      "Epoch: 16030 | MAE Train Loss: 0.003620057599619031 | MAE Test Loss: 0.008456075564026833 \n",
      "Epoch: 16040 | MAE Train Loss: 0.003585655242204666 | MAE Test Loss: 0.008376854471862316 \n",
      "Epoch: 16050 | MAE Train Loss: 0.0035513192415237427 | MAE Test Loss: 0.008294177241623402 \n",
      "Epoch: 16060 | MAE Train Loss: 0.0035169795155525208 | MAE Test Loss: 0.008214950561523438 \n",
      "Epoch: 16070 | MAE Train Loss: 0.003482584608718753 | MAE Test Loss: 0.008139127865433693 \n",
      "Epoch: 16080 | MAE Train Loss: 0.003448277711868286 | MAE Test Loss: 0.008056449703872204 \n",
      "Epoch: 16090 | MAE Train Loss: 0.003413895610719919 | MAE Test Loss: 0.007973765954375267 \n",
      "Epoch: 16100 | MAE Train Loss: 0.003379557281732559 | MAE Test Loss: 0.007894533686339855 \n",
      "Epoch: 16110 | MAE Train Loss: 0.0033452108036726713 | MAE Test Loss: 0.007815307006239891 \n",
      "Epoch: 16120 | MAE Train Loss: 0.0033108219504356384 | MAE Test Loss: 0.0077326237224042416 \n",
      "Epoch: 16130 | MAE Train Loss: 0.0032764344941824675 | MAE Test Loss: 0.007656806614249945 \n",
      "Epoch: 16140 | MAE Train Loss: 0.003242126200348139 | MAE Test Loss: 0.007574123330414295 \n",
      "Epoch: 16150 | MAE Train Loss: 0.003207744564861059 | MAE Test Loss: 0.007494890596717596 \n",
      "Epoch: 16160 | MAE Train Loss: 0.003173399716615677 | MAE Test Loss: 0.007412218954414129 \n",
      "Epoch: 16170 | MAE Train Loss: 0.0031390576623380184 | MAE Test Loss: 0.007332992739975452 \n",
      "Epoch: 16180 | MAE Train Loss: 0.00310465763323009 | MAE Test Loss: 0.007250302936881781 \n",
      "Epoch: 16190 | MAE Train Loss: 0.0030702813528478146 | MAE Test Loss: 0.007174491882324219 \n",
      "Epoch: 16200 | MAE Train Loss: 0.0030359209049493074 | MAE Test Loss: 0.007088399026542902 \n",
      "Epoch: 16210 | MAE Train Loss: 0.003001592354848981 | MAE Test Loss: 0.007012581918388605 \n",
      "Epoch: 16220 | MAE Train Loss: 0.002967211650684476 | MAE Test Loss: 0.006933349184691906 \n",
      "Epoch: 16230 | MAE Train Loss: 0.0029329038225114346 | MAE Test Loss: 0.0068506719544529915 \n",
      "Epoch: 16240 | MAE Train Loss: 0.0028984942473471165 | MAE Test Loss: 0.00676799425855279 \n",
      "Epoch: 16250 | MAE Train Loss: 0.0028641342651098967 | MAE Test Loss: 0.006692171096801758 \n",
      "Epoch: 16260 | MAE Train Loss: 0.00282975728623569 | MAE Test Loss: 0.006606072187423706 \n",
      "Epoch: 16270 | MAE Train Loss: 0.002795423613861203 | MAE Test Loss: 0.006526833865791559 \n",
      "Epoch: 16280 | MAE Train Loss: 0.0027610533870756626 | MAE Test Loss: 0.00645103445276618 \n",
      "Epoch: 16290 | MAE Train Loss: 0.002726740436628461 | MAE Test Loss: 0.0063683451153337955 \n",
      "Epoch: 16300 | MAE Train Loss: 0.002692336682230234 | MAE Test Loss: 0.006285655312240124 \n",
      "Epoch: 16310 | MAE Train Loss: 0.0026580020785331726 | MAE Test Loss: 0.006206435151398182 \n",
      "Epoch: 16320 | MAE Train Loss: 0.002623597625643015 | MAE Test Loss: 0.006127214524894953 \n",
      "Epoch: 16330 | MAE Train Loss: 0.002589259296655655 | MAE Test Loss: 0.006044524721801281 \n",
      "Epoch: 16340 | MAE Train Loss: 0.002554913517087698 | MAE Test Loss: 0.005968707613646984 \n",
      "Epoch: 16350 | MAE Train Loss: 0.0025205775164067745 | MAE Test Loss: 0.0058860182762146 \n",
      "Epoch: 16360 | MAE Train Loss: 0.0024862252175807953 | MAE Test Loss: 0.0058068097569048405 \n",
      "Epoch: 16370 | MAE Train Loss: 0.0024518431164324284 | MAE Test Loss: 0.005724119953811169 \n",
      "Epoch: 16380 | MAE Train Loss: 0.002417501760646701 | MAE Test Loss: 0.005644887685775757 \n",
      "Epoch: 16390 | MAE Train Loss: 0.002383098704740405 | MAE Test Loss: 0.00556220393627882 \n",
      "Epoch: 16400 | MAE Train Loss: 0.0023487613070756197 | MAE Test Loss: 0.005486386828124523 \n",
      "Epoch: 16410 | MAE Train Loss: 0.0023144171573221684 | MAE Test Loss: 0.005403703544288874 \n",
      "Epoch: 16420 | MAE Train Loss: 0.0022800720762461424 | MAE Test Loss: 0.0053244708105921745 \n",
      "Epoch: 16430 | MAE Train Loss: 0.0022456920705735683 | MAE Test Loss: 0.005245250649750233 \n",
      "Epoch: 16440 | MAE Train Loss: 0.0022113421000540257 | MAE Test Loss: 0.005162572953850031 \n",
      "Epoch: 16450 | MAE Train Loss: 0.0021769374143332243 | MAE Test Loss: 0.005079883150756359 \n",
      "Epoch: 16460 | MAE Train Loss: 0.0021426111925393343 | MAE Test Loss: 0.005004066042602062 \n",
      "Epoch: 16470 | MAE Train Loss: 0.0021082237362861633 | MAE Test Loss: 0.00492483377456665 \n",
      "Epoch: 16480 | MAE Train Loss: 0.0020738623570650816 | MAE Test Loss: 0.004838752560317516 \n",
      "Epoch: 16490 | MAE Train Loss: 0.0020395361352711916 | MAE Test Loss: 0.0047629354521632195 \n",
      "Epoch: 16500 | MAE Train Loss: 0.002005182206630707 | MAE Test Loss: 0.0046802400611341 \n",
      "Epoch: 16510 | MAE Train Loss: 0.001970771700143814 | MAE Test Loss: 0.004597562365233898 \n",
      "Epoch: 16520 | MAE Train Loss: 0.00193644012324512 | MAE Test Loss: 0.004518336150795221 \n",
      "Epoch: 16530 | MAE Train Loss: 0.0019020780455321074 | MAE Test Loss: 0.0044425190426409245 \n",
      "Epoch: 16540 | MAE Train Loss: 0.0018676973413676023 | MAE Test Loss: 0.004356431774795055 \n",
      "Epoch: 16550 | MAE Train Loss: 0.0018333889311179519 | MAE Test Loss: 0.004280603025108576 \n",
      "Epoch: 16560 | MAE Train Loss: 0.0017990164924412966 | MAE Test Loss: 0.004197919275611639 \n",
      "Epoch: 16570 | MAE Train Loss: 0.0017646141350269318 | MAE Test Loss: 0.00411869864910841 \n",
      "Epoch: 16580 | MAE Train Loss: 0.0017302781343460083 | MAE Test Loss: 0.004036021418869495 \n",
      "Epoch: 16590 | MAE Train Loss: 0.0016959384083747864 | MAE Test Loss: 0.003956794738769531 \n",
      "Epoch: 16600 | MAE Train Loss: 0.0016615435015410185 | MAE Test Loss: 0.0038809715770184994 \n",
      "Epoch: 16610 | MAE Train Loss: 0.0016272366046905518 | MAE Test Loss: 0.0037982941139489412 \n",
      "Epoch: 16620 | MAE Train Loss: 0.001592854387126863 | MAE Test Loss: 0.003715610597282648 \n",
      "Epoch: 16630 | MAE Train Loss: 0.0015585161745548248 | MAE Test Loss: 0.0036363780964165926 \n",
      "Epoch: 16640 | MAE Train Loss: 0.001524169696494937 | MAE Test Loss: 0.003557151649147272 \n",
      "Epoch: 16650 | MAE Train Loss: 0.001489780843257904 | MAE Test Loss: 0.0034744678996503353 \n",
      "Epoch: 16660 | MAE Train Loss: 0.001455393387004733 | MAE Test Loss: 0.0033986507914960384 \n",
      "Epoch: 16670 | MAE Train Loss: 0.0014210849767550826 | MAE Test Loss: 0.0033159672748297453 \n",
      "Epoch: 16680 | MAE Train Loss: 0.0013867035740986466 | MAE Test Loss: 0.00323673477396369 \n",
      "Epoch: 16690 | MAE Train Loss: 0.0013523586094379425 | MAE Test Loss: 0.003154063131660223 \n",
      "Epoch: 16700 | MAE Train Loss: 0.0013180166715756059 | MAE Test Loss: 0.0030748366843909025 \n",
      "Epoch: 16710 | MAE Train Loss: 0.0012836165260523558 | MAE Test Loss: 0.0029921471141278744 \n",
      "Epoch: 16720 | MAE Train Loss: 0.001249240362085402 | MAE Test Loss: 0.0029163360595703125 \n",
      "Epoch: 16730 | MAE Train Loss: 0.001214879797771573 | MAE Test Loss: 0.0028302432037889957 \n",
      "Epoch: 16740 | MAE Train Loss: 0.0011805512476712465 | MAE Test Loss: 0.002754426095634699 \n",
      "Epoch: 16750 | MAE Train Loss: 0.0011461705435067415 | MAE Test Loss: 0.0026751935947686434 \n",
      "Epoch: 16760 | MAE Train Loss: 0.001111863530240953 | MAE Test Loss: 0.0025925158988684416 \n",
      "Epoch: 16770 | MAE Train Loss: 0.0010774530237540603 | MAE Test Loss: 0.0025098384357988834 \n",
      "Epoch: 16780 | MAE Train Loss: 0.0010430931579321623 | MAE Test Loss: 0.0024340152740478516 \n",
      "Epoch: 16790 | MAE Train Loss: 0.0010087169939652085 | MAE Test Loss: 0.0023479163646698 \n",
      "Epoch: 16800 | MAE Train Loss: 0.0009743824484758079 | MAE Test Loss: 0.002268678043037653 \n",
      "Epoch: 16810 | MAE Train Loss: 0.0009400121634826064 | MAE Test Loss: 0.002192878630012274 \n",
      "Epoch: 16820 | MAE Train Loss: 0.0009056992712430656 | MAE Test Loss: 0.0021101892925798893 \n",
      "Epoch: 16830 | MAE Train Loss: 0.0008712954586371779 | MAE Test Loss: 0.002027499722316861 \n",
      "Epoch: 16840 | MAE Train Loss: 0.000836962484754622 | MAE Test Loss: 0.001948279095813632 \n",
      "Epoch: 16850 | MAE Train Loss: 0.0008025564020499587 | MAE Test Loss: 0.0018690585857257247 \n",
      "Epoch: 16860 | MAE Train Loss: 0.0007682197028771043 | MAE Test Loss: 0.0017863691318780184 \n",
      "Epoch: 16870 | MAE Train Loss: 0.0007338725263252854 | MAE Test Loss: 0.0017105520237237215 \n",
      "Epoch: 16880 | MAE Train Loss: 0.0006995379808358848 | MAE Test Loss: 0.0016278624534606934 \n",
      "Epoch: 16890 | MAE Train Loss: 0.000665183353703469 | MAE Test Loss: 0.0015486538177356124 \n",
      "Epoch: 16900 | MAE Train Loss: 0.0006308018928393722 | MAE Test Loss: 0.001465964363887906 \n",
      "Epoch: 16910 | MAE Train Loss: 0.0005964629235677421 | MAE Test Loss: 0.0013867318630218506 \n",
      "Epoch: 16920 | MAE Train Loss: 0.0005620576557703316 | MAE Test Loss: 0.0013040483463555574 \n",
      "Epoch: 16930 | MAE Train Loss: 0.0005277201416902244 | MAE Test Loss: 0.0012282312382012606 \n",
      "Epoch: 16940 | MAE Train Loss: 0.0004933759337291121 | MAE Test Loss: 0.0011455476051196456 \n",
      "Epoch: 16950 | MAE Train Loss: 0.0004590310272760689 | MAE Test Loss: 0.001066315220668912 \n",
      "Epoch: 16960 | MAE Train Loss: 0.00042464956641197205 | MAE Test Loss: 0.0009870945941656828 \n",
      "Epoch: 16970 | MAE Train Loss: 0.0003903009055647999 | MAE Test Loss: 0.0009044170146808028 \n",
      "Epoch: 16980 | MAE Train Loss: 0.00035589709295891225 | MAE Test Loss: 0.0008217275026254356 \n",
      "Epoch: 16990 | MAE Train Loss: 0.000321570027153939 | MAE Test Loss: 0.0007459103944711387 \n",
      "Epoch: 17000 | MAE Train Loss: 0.00028718262910842896 | MAE Test Loss: 0.0006666779518127441 \n",
      "Epoch: 17010 | MAE Train Loss: 0.00025282055139541626 | MAE Test Loss: 0.0005805969121865928 \n",
      "Epoch: 17020 | MAE Train Loss: 0.00021849498443771154 | MAE Test Loss: 0.0005047798040322959 \n",
      "Epoch: 17030 | MAE Train Loss: 0.0001841425837483257 | MAE Test Loss: 0.0004220843256916851 \n",
      "Epoch: 17040 | MAE Train Loss: 0.0001497305929660797 | MAE Test Loss: 0.00033940671710297465 \n",
      "Epoch: 17050 | MAE Train Loss: 0.00011539905972313136 | MAE Test Loss: 0.0002601802407298237 \n",
      "Epoch: 17060 | MAE Train Loss: 8.103847358142957e-05 | MAE Test Loss: 0.00018436313257552683 \n",
      "Epoch: 17070 | MAE Train Loss: 4.6656281483592466e-05 | MAE Test Loss: 9.827614121604711e-05 \n",
      "Epoch: 17080 | MAE Train Loss: 1.7155707610072568e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17090 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17100 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17110 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17120 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17130 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17140 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17150 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17160 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17170 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17180 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17190 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17200 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17210 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17220 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17230 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17240 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17250 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17260 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17270 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17280 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17290 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17300 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17310 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17320 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17330 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17340 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17350 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17360 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17370 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17380 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17390 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17400 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17410 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17420 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17430 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17440 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17450 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17460 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17470 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17480 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17490 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17500 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17510 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17520 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17530 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17540 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17550 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17560 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17570 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17580 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17590 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17600 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17610 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17620 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17630 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17640 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17650 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17660 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17670 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17680 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17690 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17700 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17710 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17720 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17730 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17740 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17750 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17760 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17770 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17780 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17790 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17800 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17810 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17820 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17830 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17840 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17850 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17860 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17870 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17880 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17890 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17900 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17910 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17920 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17930 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17940 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17950 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17960 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17970 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17980 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 17990 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18000 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18010 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18020 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18030 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18040 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18050 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18060 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18070 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18080 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18090 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18100 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18110 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18120 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18130 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18140 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18150 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18160 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18170 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18180 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18190 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18200 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18210 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18220 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18230 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18240 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18250 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18260 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18270 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18280 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18290 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18300 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18310 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18320 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18330 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18340 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18350 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18360 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18370 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18380 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18390 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18400 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18410 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18420 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18430 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18440 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18450 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18460 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18470 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18480 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18490 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18500 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18510 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18520 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18530 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18540 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18550 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18560 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18570 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18580 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18590 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18600 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18610 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18620 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18630 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18640 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18650 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18660 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18670 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18680 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18690 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18700 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18710 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18720 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18730 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18740 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18750 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18760 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18770 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18780 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18790 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18800 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18810 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18820 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18830 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18840 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18850 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18860 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18870 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18880 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18890 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18900 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18910 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18920 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18930 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18940 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18950 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18960 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18970 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18980 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 18990 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19000 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19010 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19020 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19030 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19040 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19050 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19060 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19070 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19080 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19090 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19100 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19110 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19120 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19130 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19140 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19150 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19160 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19170 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19180 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19190 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19200 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19210 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19220 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19230 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19240 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19250 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19260 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19270 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19280 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19290 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19300 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19310 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19320 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19330 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19340 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19350 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19360 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19370 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19380 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19390 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19400 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19410 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19420 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19430 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19440 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19450 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19460 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19470 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19480 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19490 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19500 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19510 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19520 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19530 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19540 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19550 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19560 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19570 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19580 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19590 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19600 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19610 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19620 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19630 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19640 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19650 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19660 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19670 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19680 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19690 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19700 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19710 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19720 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19730 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19740 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19750 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19760 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19770 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19780 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19790 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19800 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19810 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19820 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19830 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19840 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19850 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19860 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19870 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19880 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19890 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19900 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19910 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19920 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19930 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19940 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19950 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19960 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19970 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19980 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n",
      "Epoch: 19990 | MAE Train Loss: 6.57841592328623e-05 | MAE Test Loss: 3.293156623840332e-05 \n"
     ]
    }
   ],
   "source": [
    "# An epoch is one loop through the data ... (this is a hyperparameter because we've set it ourselves)\n",
    "\n",
    "epochs = 20000\n",
    "\n",
    "#track different values \n",
    "epoch_count = [] \n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    #set the model to training mode\n",
    "    model_0.train() # train mode in pytorch sets all parameters that require gradients to require gradient\n",
    "\n",
    "    #1. Forward pass\n",
    "    y_pred = model_0(X_train)  # The current pred res base on the current [params]\n",
    "    \n",
    "    #2. Calculate the loss\n",
    "    loss = loss_fn(y_pred, y_train) # use loss function to get how wrong the res is?\n",
    "    #3. Optimizer zero grad [zero the optimizer gradients] --> they accumulate every epoch, zero them to start fresh each forward pass\n",
    "    optimizer.zero_grad() # \n",
    "\n",
    "    #4. perform backpropagation on the loss with respect to the parameters of the model\n",
    "    loss.backward() # [ Compute the gradient of every parameter with requires_grad = True ]\n",
    "\n",
    "    #5. Step the optimizer (perform gradient descent)\n",
    "    optimizer.step()  # take the step here ? \n",
    "    model_0.eval() \n",
    "    with torch.inference_mode():\n",
    "      # 1. Forward pass on test data\n",
    "      test_pred = model_0(X_test)\n",
    "\n",
    "      # 2. Caculate loss on test data\n",
    "      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n",
    "\n",
    "      # Print out what's happening\n",
    "      if epoch % 10 == 0:\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(loss.detach().numpy())\n",
    "            test_loss_values.append(test_loss.detach().numpy())\n",
    "            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")\n",
    "    # model_0.eval() # turns off gradient tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9d1bb3be-dddf-44e9-b7b7-8142a27671e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([0.6999], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.3000], requires_grad=True)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e8961000-2191-4bd2-8f29-16de18ccde33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    y_preds = model_0(X_test)\n",
    "\n",
    "y_preds, y_test\n",
    "\n",
    "# To visuslize this res for now\n",
    "plot_predictions(predictions=y_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7e40cad-57d1-4777-b193-b6df3e30cf2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABqjUlEQVR4nO3dd3hT9f4H8HeSNkn3ntABBdqyChRaykaqBbkIiIrIlXEVFcHxw8lVWVdFQZErKCheHDgYXgS9siuIDNllU4Z0AJ2U7p18f3+kSRtaSiltTpK+X8+Tp+nJSc7nNKV5811HJoQQICIiIrIScqkLICIiImpKDDdERERkVRhuiIiIyKow3BAREZFVYbghIiIiq8JwQ0RERFaF4YaIiIisCsMNERERWRWGGyIiIrIqDDdEzWzSpEkIDg5u1HPnzJkDmUzWtAWZmaSkJMhkMnz11VdSl9IoMpkMc+bMkboMIqqB4YZaLJlM1qDbrl27pC6VAJw5cwZz5sxBUlJSsx7n008/tdigRUQ6NlIXQCSVVatWGX3/zTffYPv27bW2h4eH39VxVqxYAa1W26jnvvnmm3j99dfv6vjW4syZM5g7dy4GDRrU6Jawhvj000/h6emJSZMmNdsxiKh5MdxQi/X3v//d6Ps///wT27dvr7X9ZsXFxbC3t2/wcWxtbRtVHwDY2NjAxob/TMm8aLValJeXQ61WS10KUZ3YLUVUj0GDBqFz5844cuQIBgwYAHt7e/zzn/8EAGzcuBHDhw+Hv78/VCoVQkJC8K9//QsajcboNW4ec6MfY/LBBx/g888/R0hICFQqFXr16oVDhw4ZPbeuMTcymQzTp0/Hhg0b0LlzZ6hUKnTq1AlbtmypVf+uXbvQs2dPqNVqhISE4LPPPmvwOJ4//vgDDz/8MAIDA6FSqRAQEID/+7//Q0lJSa3zc3R0xNWrVzFq1Cg4OjrCy8sLL7/8cq2fRW5uLiZNmgQXFxe4urpi4sSJyM3NvW0tX331FR5++GEAwODBg+vsMty8eTP69+8PBwcHODk5Yfjw4Th9+rTR66Snp2Py5Mlo3bo1VCoV/Pz8MHLkSENXV3BwME6fPo3ff//dcIxBgwbdtr6bHTt2DMOGDYOzszMcHR0xZMgQ/Pnnn0b7VFRUYO7cuWjfvj3UajU8PDzQr18/bN++vcH11ufcuXN45JFH4OXlBTs7O4SGhuKNN94wPH6rsWD1/c5999136NSpE1QqFX755Re4u7tj8uTJtV4jPz8farUaL7/8smFbWVkZZs+ejXbt2hl+n1599VWUlZUZPXf79u3o168fXF1d4ejoiNDQUMO/OaKG4n8JiW7j+vXrGDZsGB599FH8/e9/h4+PDwDdB66joyNmzJgBR0dH/Pbbb5g1axby8/OxcOHC277u999/j4KCAjz99NOQyWRYsGABHnzwQfz111+3be3Zs2cP1q9fj2effRZOTk74+OOPMWbMGKSkpMDDwwOA7gN26NCh8PPzw9y5c6HRaDBv3jx4eXk16LzXrVuH4uJiTJ06FR4eHjh48CCWLFmCK1euYN26dUb7ajQaxMXFITo6Gh988AF27NiBDz/8ECEhIZg6dSoAQAiBkSNHYs+ePXjmmWcQHh6On376CRMnTrxtLQMGDMDzzz+Pjz/+GP/85z8NXYX6r6tWrcLEiRMRFxeH999/H8XFxVi2bBn69euHY8eOGT7Ex4wZg9OnT+O5555DcHAwMjMzsX37dqSkpCA4OBiLFy/Gc889B0dHR0MQ0L/fDXX69Gn0798fzs7OePXVV2Fra4vPPvsMgwYNwu+//47o6GgAuhAxf/58PPnkk4iKikJ+fj4OHz6Mo0eP4t57721Qvbdy4sQJ9O/fH7a2tnjqqacQHByMS5cu4ZdffsE777xzR+ej99tvv2Ht2rWYPn06PD090b59e4wePRrr16/HZ599BqVSadh3w4YNKCsrw6OPPgpA19LzwAMPYM+ePXjqqacQHh6OkydP4qOPPsL58+exYcMGw8/ub3/7G7p27Yp58+ZBpVLh4sWL2Lt3b6NqphZMEJEQQohp06aJm/9JDBw4UAAQy5cvr7V/cXFxrW1PP/20sLe3F6WlpYZtEydOFEFBQYbvL1++LAAIDw8PkZOTY9i+ceNGAUD88ssvhm2zZ8+uVRMAoVQqxcWLFw3bjh8/LgCIJUuWGLaNGDFC2Nvbi6tXrxq2XbhwQdjY2NR6zbrUdX7z588XMplMJCcnG50fADFv3jyjfbt37y4iIyMN32/YsEEAEAsWLDBsq6ysFP379xcAxJdffllvPevWrRMAxM6dO422FxQUCFdXVzFlyhSj7enp6cLFxcWw/caNGwKAWLhwYb3H6dSpkxg4cGC9+9QEQMyePdvw/ahRo4RSqRSXLl0ybLt27ZpwcnISAwYMMGyLiIgQw4cPv+XrNrTeugwYMEA4OTkZvU9CCKHVag33b/691LvV75xcLhenT5822r5169Zav7NCCHH//feLtm3bGr5ftWqVkMvl4o8//jDab/ny5QKA2Lt3rxBCiI8++kgAEFlZWQ0/WaI6sFuK6DZUKlWdTe92dnaG+wUFBcjOzkb//v1RXFyMc+fO3fZ1x44dCzc3N8P3/fv3BwD89ddft31ubGwsQkJCDN937doVzs7OhudqNBrs2LEDo0aNgr+/v2G/du3aYdiwYbd9fcD4/IqKipCdnY0+ffpACIFjx47V2v+ZZ54x+r5///5G57Jp0ybY2NgYWnIAQKFQ4LnnnmtQPbeyfft25ObmYty4ccjOzjbcFAoFoqOjsXPnTsP5KJVK7Nq1Czdu3LirY96KRqPBtm3bMGrUKLRt29aw3c/PD4899hj27NmD/Px8AICrqytOnz6NCxcu1Plaja03KysLu3fvxj/+8Q8EBgYaPXY3ywoMHDgQHTt2NNp2zz33wNPTE2vWrDFsu3HjBrZv346xY8catq1btw7h4eEICwszeo/uueceADC8R66urgB0Xb6NHYRPBHDMDdFttWrVyqjJXe/06dMYPXo0XFxc4OzsDC8vL8Ng5Ly8vNu+7s0fPPqg05APspufq3++/rmZmZkoKSlBu3btau1X17a6pKSkYNKkSXB3dzeMoxk4cCCA2uenVqtrdXfVrAcAkpOT4efnB0dHR6P9QkNDG1TPrejDwT333AMvLy+j27Zt25CZmQlAF1Lff/99bN68GT4+PhgwYAAWLFiA9PT0uzp+TVlZWSguLq7znMLDw6HVapGamgoAmDdvHnJzc9GhQwd06dIFr7zyCk6cOGHYv7H16gNl586dm+y8AKBNmza1ttnY2GDMmDHYuHGjYezM+vXrUVFRYRRuLly4gNOnT9d6fzp06AAAhvdo7Nix6Nu3L5588kn4+Pjg0Ucfxdq1axl06I5xzA3RbdRswdDLzc3FwIED4ezsjHnz5iEkJARqtRpHjx7Fa6+91qA/xgqFos7tQohmfW5DaDQa3HvvvcjJycFrr72GsLAwODg44OrVq5g0aVKt87tVPaagr2XVqlXw9fWt9XjN2WYvvvgiRowYgQ0bNmDr1q146623MH/+fPz222/o3r27yWoGdOOILl26hI0bN2Lbtm344osv8NFHH2H58uV48sknm73eW7Xi3DwIXK+ufwcA8Oijj+Kzzz7D5s2bMWrUKKxduxZhYWGIiIgw7KPVatGlSxcsWrSoztcICAgwHGP37t3YuXMnfv31V2zZsgVr1qzBPffcg23btkn6e0aWheGGqBF27dqF69evY/369RgwYIBh++XLlyWsqpq3tzfUajUuXrxY67G6tt3s5MmTOH/+PL7++mtMmDDBsL3mTJ47FRQUhPj4eBQWFhq13iQmJjbo+bf6MNZ3z3l7eyM2Nva2rxMSEoKXXnoJL730Ei5cuIBu3brhww8/xLffflvvcRrCy8sL9vb2dZ7TuXPnIJfLDR/kAAyzjSZPnozCwkIMGDAAc+bMMYSbhtR7M3132KlTp+qt1c3Nrc6ZasnJyQ05VYMBAwbAz88Pa9asQb9+/fDbb78ZzcrSn8Px48cxZMiQ2/585XI5hgwZgiFDhmDRokV499138cYbb2Dnzp0Nen+JAHZLETWK/n+QNVtKysvL8emnn0pVkhGFQoHY2Fhs2LAB165dM2y/ePEiNm/e3KDnA8bnJ4TAv//970bXdP/996OyshLLli0zbNNoNFiyZEmDnu/g4AAAtT6Q4+Li4OzsjHfffRcVFRW1npeVlQVAtz5RaWmp0WMhISFwcnIymo7s4ODQoOnpdVEoFLjvvvuwceNGo+naGRkZ+P7779GvXz84OzsD0M3Cq8nR0RHt2rUz1NLQem/m5eWFAQMGYOXKlUhJSTF6rOb7GRISgry8PKOusLS0NPz00093dM5yuRwPPfQQfvnlF6xatQqVlZVGXVIA8Mgjj+Dq1atYsWJFreeXlJSgqKgIAJCTk1Pr8W7dugFAvedMdDO23BA1Qp8+feDm5oaJEyfi+eefh0wmw6pVq5qsW6gpzJkzB9u2bUPfvn0xdepUaDQaLF26FJ07d0ZCQkK9zw0LC0NISAhefvllXL16Fc7Ozvjvf/97VwNxR4wYgb59++L1119HUlISOnbsiPXr1zdofBKg+5BTKBR4//33kZeXB5VKhXvuuQfe3t5YtmwZHn/8cfTo0QOPPvoovLy8kJKSgl9//RV9+/bF0qVLcf78eQwZMgSPPPIIOnbsCBsbG/z000/IyMgwTFkGgMjISCxbtgxvv/022rVrB29vb8PA14Z4++23DWu1PPvss7CxscFnn32GsrIyLFiwwLBfx44dMWjQIERGRsLd3R2HDx/Gjz/+iOnTpwNAg+uty8cff4x+/fqhR48eeOqpp9CmTRskJSXh119/Nbz3jz76KF577TWMHj0azz//vGH6fIcOHXD06NEGny+gGyuzZMkSzJ49G126dKm1qvfjjz+OtWvX4plnnsHOnTvRt29faDQanDt3DmvXrsXWrVvRs2dPzJs3D7t378bw4cMRFBSEzMxMfPrpp2jdujX69et3RzVRCyfdRC0i83KrqeCdOnWqc/+9e/eK3r17Czs7O+Hv7y9effVVw9TYmtOVbzUVvK4pvrhpWvGtpuVOmzat1nODgoLExIkTjbbFx8eL7t27C6VSKUJCQsQXX3whXnrpJaFWq2/xU6h25swZERsbKxwdHYWnp6eYMmWKYcp5zWnbEydOFA4ODrWeX1ft169fF48//rhwdnYWLi4u4vHHHxfHjh1r0FRwIYRYsWKFaNu2rVAoFLV+zjt37hRxcXHCxcVFqNVqERISIiZNmiQOHz4shBAiOztbTJs2TYSFhQkHBwfh4uIioqOjxdq1a42OkZ6eLoYPHy6cnJwEgNtOC7/5PRNCiKNHj4q4uDjh6Ogo7O3txeDBg8W+ffuM9nn77bdFVFSUcHV1FXZ2diIsLEy88847ory8/I7qvZVTp06J0aNHC1dXV6FWq0VoaKh46623jPbZtm2b6Ny5s1AqlSI0NFR8++23d/Q7p6fVakVAQIAAIN5+++069ykvLxfvv/++6NSpk1CpVMLNzU1ERkaKuXPniry8PCGE7vd15MiRwt/fXyiVSuHv7y/GjRsnzp8/36BzJtKTCWFG/9UkomY3atSoeqcgExFZOo65IbJiN18q4cKFC9i0aVOjLilARGQp2HJDZMX8/PwwadIktG3bFsnJyVi2bBnKyspw7NgxtG/fXuryiIiaBQcUE1mxoUOH4ocffkB6ejpUKhViYmLw7rvvMtgQkVVjyw0RERFZFY65ISIiIqvCcENERERWpcWNudFqtbh27RqcnJzuapl1IiIiMh0hBAoKCuDv7w+5vP62mRYXbq5du2Z0bRciIiKyHKmpqWjdunW9+7S4cOPk5ARA98PRX+OFiIiIzFt+fj4CAgIMn+P1aXHhRt8V5ezszHBDRERkYRoypIQDiomIiMiqMNwQERGRVWG4ISIiIqvS4sbcEBGRddNoNKioqJC6DGoEpVJ522neDcFwQ0REVkEIgfT0dOTm5kpdCjWSXC5HmzZtoFQq7+p1GG6IiMgq6IONt7c37O3tuVCrhdEvspuWlobAwMC7ev/MItx88sknWLhwIdLT0xEREYElS5YgKiqqzn2/+uorTJ482WibSqVCaWmpKUolIiIzpNFoDMHGw8ND6nKokby8vHDt2jVUVlbC1ta20a8j+YDiNWvWYMaMGZg9ezaOHj2KiIgIxMXFITMz85bPcXZ2RlpamuGWnJxswoqJiMjc6MfY2NvbS1wJ3Q19d5RGo7mr15E83CxatAhTpkzB5MmT0bFjRyxfvhz29vZYuXLlLZ8jk8ng6+truPn4+JiwYiIiMlfsirJsTfX+SRpuysvLceTIEcTGxhq2yeVyxMbGYv/+/bd8XmFhIYKCghAQEICRI0fi9OnTt9y3rKwM+fn5RjciIiKyXpKGm+zsbGg0mlotLz4+PkhPT6/zOaGhoVi5ciU2btyIb7/9FlqtFn369MGVK1fq3H/+/PlwcXEx3HjRTCIismbBwcFYvHix5K8hJcm7pe5UTEwMJkyYgG7dumHgwIFYv349vLy88Nlnn9W5/8yZM5GXl2e4paammrhiIiKi2mQyWb23OXPmNOp1Dx06hKeeeqppi7Uwks6W8vT0hEKhQEZGhtH2jIwM+Pr6Nug1bG1t0b17d1y8eLHOx1UqFVQq1V3X2iBF2UBhJuDT0TTHIyIii5WWlma4v2bNGsyaNQuJiYmGbY6Ojob7QghoNBrY2Nz+Y9vLy6tpC7VAkrbcKJVKREZGIj4+3rBNq9UiPj4eMTExDXoNjUaDkydPws/Pr7nKbJhzm4CFIcDGZ6Wtg4iILELNiTEuLi5Gk2XOnTsHJycnbN68GZGRkVCpVNizZw8uXbqEkSNHwsfHB46OjujVqxd27Nhh9Lo3dynJZDJ88cUXGD16NOzt7dG+fXv8/PPPd1RrSkoKRo4cCUdHRzg7O+ORRx4xapg4fvw4Bg8eDCcnJzg7OyMyMhKHDx8GACQnJ2PEiBFwc3ODg4MDOnXqhE2bNjX+B9cAkq9zM2PGDEycOBE9e/ZEVFQUFi9ejKKiIsNaNhMmTECrVq0wf/58AMC8efPQu3dvtGvXDrm5uVi4cCGSk5Px5JNPSnkagF+E7mvacaA0H1A7S1sPEVELJoRAScXdTSduLDtbRZPN+nn99dfxwQcfoG3btnBzc0Nqairuv/9+vPPOO1CpVPjmm28wYsQIJCYmIjAw8JavM3fuXCxYsAALFy7EkiVLMH78eCQnJ8Pd3f22NWi1WkOw+f3331FZWYlp06Zh7Nix2LVrFwBg/Pjx6N69O5YtWwaFQoGEhATDOjXTpk1DeXk5du/eDQcHB5w5c8aoVao5SB5uxo4di6ysLMyaNQvp6eno1q0btmzZYhhknJKSYnSdiRs3bmDKlClIT0+Hm5sbIiMjsW/fPnTsKHFXkEsrwC0YuJEEpB4A2t8rbT1ERC1YSYUGHWdtleTYZ+bFwV7ZNB+v8+bNw733Vn+euLu7IyIiwvD9v/71L/z000/4+eefMX369Fu+zqRJkzBu3DgAwLvvvouPP/4YBw8exNChQ29bQ3x8PE6ePInLly8bJuV888036NSpEw4dOoRevXohJSUFr7zyCsLCwgAA7du3Nzw/JSUFY8aMQZcuXQAAbdu2vYOfQONIHm4AYPr06bd8U/SpUO+jjz7CRx99ZIKqGiGony7cJP3BcENERHetZ8+eRt8XFhZizpw5+PXXX5GWlobKykqUlJQgJSWl3tfp2rWr4b6DgwOcnZ3rXSy3prNnzyIgIMBotnHHjh3h6uqKs2fPolevXpgxYwaefPJJrFq1CrGxsXj44YcREhICAHj++ecxdepUbNu2DbGxsRgzZoxRPc3BLMKN1QjuCyR8CyTtlboSIqIWzc5WgTPz4iQ7dlNxcHAw+v7ll1/G9u3b8cEHH6Bdu3aws7PDQw89hPLy8npf5+ZLGchkMmi12iarc86cOXjsscfw66+/YvPmzZg9ezZWr16N0aNH48knn0RcXBx+/fVXbNu2DfPnz8eHH36I5557rsmOfzOGm6YU1Ff39doxoKwQUDVvnyIREdVNJpM1WdeQOdm7dy8mTZqE0aNHA9C15CQlJTXrMcPDw5GamorU1FRD682ZM2eQm5trNCSkQ4cO6NChA/7v//4P48aNw5dffmmoMyAgAM888wyeeeYZzJw5EytWrGjWcGNx69yYNbcgwCUQEBrduBsiIqIm1L59e6xfvx4JCQk4fvw4HnvssSZtgalLbGwsunTpgvHjx+Po0aM4ePAgJkyYgIEDB6Jnz54oKSnB9OnTsWvXLiQnJ2Pv3r04dOgQwsPDAQAvvvgitm7disuXL+Po0aPYuXOn4bHmwnDT1IKrWm+S9khbBxERWZ1FixbBzc0Nffr0wYgRIxAXF4cePXo06zFlMhk2btwINzc3DBgwALGxsWjbti3WrFkDAFAoFLh+/TomTJiADh064JFHHsGwYcMwd+5cALolW6ZNm4bw8HAMHToUHTp0wKefftq8NQshRLMewczk5+fDxcUFeXl5cHZuhunaR1cBP08HAqKBJ7Y1/esTEVEtpaWluHz5Mtq0aQO1Wi11OdRI9b2Pd/L5zZabphbcT/f16lGgvFjaWoiIiFoghpum5hYMOLcCtBXAlYNSV0NERNTiMNw0NZmsetYUp4QTERGZHMNNc+CgYiIiIskw3DSHIP24m8NARYm0tRAREbUwDDfNwSMEcPQFNOXAlcNSV0NERNSiMNw0B5msumsqmeNuiIiITInhprkEcdwNERGRFBhumot+vZsrh4DKMmlrISIiakEYbpqLZwfAwQuoLAWuHpG6GiIiIiNJSUmQyWRISEiQupQmx3DTXLjeDRER1UMmk9V7mzNnzl299oYNG5qsVktjfdeDNyfB/YAzG4DkPQBekboaIiIyI2lpaYb7a9aswaxZs5CYmGjY5ujoKEVZVoEtN81J33KTcgCoLJe2FiIiMiu+vr6Gm4uLC2QymdG21atXIzw8HGq1GmFhYUZX0i4vL8f06dPh5+cHtVqNoKAgzJ8/HwAQHBwMABg9ejRkMpnh+4b4/fffERUVBZVKBT8/P7z++uuorKw0PP7jjz+iS5cusLOzg4eHB2JjY1FUVAQA2LVrF6KiouDg4ABXV1f07dsXycnJd/+DagS23DQnrzDAzh0oyQGuHQMCo6WuiIioZRACqJDo4sW29rqhCXfhu+++w6xZs7B06VJ0794dx44dw5QpU+Dg4ICJEyfi448/xs8//4y1a9ciMDAQqampSE1NBQAcOnQI3t7e+PLLLzF06FAoFIoGHfPq1au4//77MWnSJHzzzTc4d+4cpkyZArVajTlz5iAtLQ3jxo3DggULMHr0aBQUFOCPP/6AEAKVlZUYNWoUpkyZgh9++AHl5eU4ePAgZHf5c2gshpvmJJfr1rs5+4uua4rhhojINCqKgXf9pTn2P68BSoe7eonZs2fjww8/xIMPPggAaNOmDc6cOYPPPvsMEydOREpKCtq3b49+/fpBJpMhKCjI8FwvLy8AgKurK3x9fRt8zE8//RQBAQFYunQpZDIZwsLCcO3aNbz22muYNWsW0tLSUFlZiQcffNBwvC5dugAAcnJykJeXh7/97W8ICQkBAISHh9/Vz+BusFuquekvxcBBxURE1ABFRUW4dOkSnnjiCTg6Ohpub7/9Ni5dugQAmDRpEhISEhAaGornn38e27Ztu+vjnj17FjExMUatLX379kVhYSGuXLmCiIgIDBkyBF26dMHDDz+MFStW4MaNGwAAd3d3TJo0CXFxcRgxYgT+/e9/G40pMjW23DQ3/UrFqQcATQWgsJW2HiKilsDWXteCItWx70JhYSEAYMWKFYiONm7x13cx9ejRA5cvX8bmzZuxY8cOPPLII4iNjcWPP/54V8euj0KhwPbt27Fv3z5s27YNS5YswRtvvIEDBw6gTZs2+PLLL/H8889jy5YtWLNmDd58801s374dvXv3braaboUtN83NuxOgdgXKC4G041JXQ0TUMshkuq4hKW53Oc7Ex8cH/v7++Ouvv9CuXTujW5s2bQz7OTs7Y+zYsVixYgXWrFmD//73v8jJyQEA2NraQqPR3NFxw8PDsX//fgghDNv27t0LJycntG7duurHKkPfvn0xd+5cHDt2DEqlEj/99JNh/+7du2PmzJnYt28fOnfujO+///5ufhSNxpab5iaX62ZNJf6quxRD655SV0RERGZu7ty5eP755+Hi4oKhQ4eirKwMhw8fxo0bNzBjxgwsWrQIfn5+6N69O+RyOdatWwdfX1+4uroC0M2Yio+PR9++faFSqeDm5nbbYz777LNYvHgxnnvuOUyfPh2JiYmYPXs2ZsyYAblcjgMHDiA+Ph733XcfvL29ceDAAWRlZSE8PByXL1/G559/jgceeAD+/v5ITEzEhQsXMGHChGb+SdWN4cYUgqvCTfJeoN+LUldDRERm7sknn4S9vT0WLlyIV155BQ4ODujSpQtefPFFAICTkxMWLFiACxcuQKFQoFevXti0aRPkcl2HzIcffogZM2ZgxYoVaNWqFZKSkm57zFatWmHTpk145ZVXEBERAXd3dzzxxBN48803Aehainbv3o3FixcjPz8fQUFB+PDDDzFs2DBkZGTg3Llz+Prrr3H9+nX4+flh2rRpePrpp5vrR1QvmajZ/tQC5Ofnw8XFBXl5eXB2djbNQa8lAJ8PBFTOwKuXAQUzJRFRUyotLcXly5fRpk0bqNVqqcuhRqrvfbyTz2+OuTEF3y6AygUoywfST0hdDRERkVVjuDEFuQIIrBotnswp4URERM2J4cZUgrneDRERkSkw3JiKfr2blH2A9s6m5xEREVHDMdyYim8EoHQCSvOAjNNSV0NEZJVa2BwZq9NU7x/DjakobKqvLZW0R9paiIisjK2tbvX34mKJLpZJTaK8vBwAGnyxz1vhnGRTCu4HXNyhG1Qc86zU1RARWQ2FQgFXV1dkZmYCAOzt7SW7IjU1jlarRVZWFuzt7WFjc3fxhOHGlPQX0UzeC2i1utWLiYioSeivgK0POGR55HI5AgMD7zqYMtyYkn83wNYBKLkBZJ0FfDpJXRERkdWQyWTw8/ODt7c3KioqpC6HGkGpVBpWWb4bDDempLAFAqKAv3bqxt0w3BARNTmFQnHXYzbIsrFfxNT0U8I5qJiIiKhZMNyYWnB/3dfkfQCnLBIRETU5hhtT8+8B2NgBxdlAVqLU1RAREVkdhhtTs1ECAb1095PZNUVERNTUGG6koJ8SznE3RERETY7hRgo1L6LJcTdERERNiuFGCq0iAYUKKMoErl+UuhoiIiKrwnAjBVs10Lpq3A27poiIiJoUw41UuN4NERFRs2C4kUpQVbhJ5rgbIiKipsRwI5XWvQCFEihIA3L+kroaIiIiq8FwIxWlvW5gMaBrvSEiIqImwXAjJX3XVBLDDRERUVNhuJFSzUHFHHdDRETUJBhupBQQDchtgPwrQG6y1NUQERFZBYYbKSkddBfSBNg1RURE1EQYbqQWXGNKOBEREd01hhupGS6i+Ye0dRAREVkJhhupBUYDMgWQmwLkpkpdDRERkcVjuJGaygnw76a7z64pIiKiu8ZwYw6CeJ0pIiKipmIW4eaTTz5BcHAw1Go1oqOjcfDgwQY9b/Xq1ZDJZBg1alTzFtjcgqvG3bDlhoiI6K5JHm7WrFmDGTNmYPbs2Th69CgiIiIQFxeHzMzMep+XlJSEl19+Gf379zdRpc0osDcgk+uuMZV/TepqiIiILJrk4WbRokWYMmUKJk+ejI4dO2L58uWwt7fHypUrb/kcjUaD8ePHY+7cuWjbtq0Jq20mahfAt4vuPte7ISIiuiuShpvy8nIcOXIEsbGxhm1yuRyxsbHYv3//LZ83b948eHt744knnrjtMcrKypCfn290M0vBVS1QyRx3Q0REdDckDTfZ2dnQaDTw8fEx2u7j44P09PQ6n7Nnzx785z//wYoVKxp0jPnz58PFxcVwCwgIuOu6mwUvoklERNQkJO+WuhMFBQV4/PHHsWLFCnh6ejboOTNnzkReXp7hlppqpmvJBMUAkAHXLwAFdQc7IiIiuj0bKQ/u6ekJhUKBjIwMo+0ZGRnw9fWttf+lS5eQlJSEESNGGLZptVoAgI2NDRITExESEmL0HJVKBZVK1QzVNzE7N8CnM5BxUjdrqvMYqSsiIiKySJK23CiVSkRGRiI+Pt6wTavVIj4+HjExMbX2DwsLw8mTJ5GQkGC4PfDAAxg8eDASEhLMt8upofRTwtk1RURE1GiSttwAwIwZMzBx4kT07NkTUVFRWLx4MYqKijB58mQAwIQJE9CqVSvMnz8farUanTt3Nnq+q6srANTabpGC+wIHlnG9GyIiorsgebgZO3YssrKyMGvWLKSnp6Nbt27YsmWLYZBxSkoK5HKLGhrUeIF9dF+zzgGFWYCjl7T1EBERWSCZEEJIXYQp5efnw8XFBXl5eXB2dpa6nNo+jQEyzwAPfw10GiV1NURERGbhTj6/W0iTiAXRTwln1xQREVGjMNyYGw4qJiIiuisMN+ZG33KTeRoozpG2FiIiIgvEcGNuHL0Az1Dd/eR90tZCRERkgRhuzFGw/lIMvM4UERHRnWK4MUf6cTe8iCYREdEdY7gxR0FV4Sb9FFByQ9paiIiILAzDjTly8gE82gEQQMqfUldDRERkURhuzFUQx90QERE1BsONuTKsd8NwQ0REdCcYbsyVvuUm/QRQmidtLURERBaE4cZcubQC3NoAQgukHJC6GiIiIovBcGPO9OvdcEo4ERFRgzHcmLMgjrshIiK6Uww35kzfcnMtASgrkLQUIiIiS8FwY85cA3U3oQFSOe6GiIioIRhuzJ2ha2qvtHUQERFZCIYbc8eLaBIREd0Rhhtzp1/v5tpRoLxI2lqIiIgsAMONuXMLBpxbA9pKIPWg1NUQERGZPYYbcyeT1VjvhuNuiIiIbofhxhIYLqLJcENERHQ7DDeWQH8RzauHgYoSaWshIiIycww3lsC9LeDkB2jKgSuHpK6GiIjIrDHcWAKZjF1TREREDcRwYyk4qJiIiKhBGG4shX6l4tSDQEWptLUQERGZMYYbS+HZHnDwBjRlwNUjUldDRERkthhuLAXXuyEiImoQhhtLEsTrTBEREd0Ow40lCa4x7qayXNpaiIiIzBTDjSXxCgPsPYDKEt2FNImIiKgWhhtLYrTeDbumiIiI6sJwY2n0XVMcVExERFQnhhtLo2+5STkAaCqkrYWIiMgMMdxYGu+OgJ0bUFEEXEuQuhoiIiKzw3BjaeRyILCP7n4yx90QERHdjOHGEunH3fAimkRERLUw3Fgi/UrFKX8CmkppayEiIjIzDDeWyKczoHIByguA9BNSV0NERGRWGG4skVwBBMXo7nO9GyIiIiMMN5YqiBfRJCIiqgvDjaUyLOa3H9BqpK2FiIjIjDDcWCrfroDSCSjLAzJOSV0NERGR2WC4sVQKGyCwt+4+x90QEREZMNxYMv2UcK53Q0REZMBwY8mC++u+puwDtFppayEiIjITDDeWzC8CsHUASm4AmWekroaIiMgsMNxYMoUtEBitu88p4URERAAYbiyffr2bpD+krYOIiMhMMNxYOsN6N/sAIaSthYiIyAww3DSRE1dy8Y+vDuHF1cdMe2D/HoCNHVB8Hcg6Z9pjExERmSGGmyYigwy/ncvEjrOZKK804cwlGyUQEKW7z/VuiIiIGG6aSid/Z3g6qlBYVonDyTmmPbiha4qDiomIiBhumohcLsOgUC8AwK7ELNMe3DCoeA/H3RARUYvHcNOEBod6AwB2nss07YFbRQI2aqAoC8i+YNpjExERmRmGmybUr70nFHIZLmQWIjWn2HQHtlUDrXvp7idz3A0REbVsZhFuPvnkEwQHB0OtViM6OhoHDx685b7r169Hz5494erqCgcHB3Tr1g2rVq0yYbW35mJni8hANwDArvNSdU1x3A0REbVskoebNWvWYMaMGZg9ezaOHj2KiIgIxMXFITOz7q4dd3d3vPHGG9i/fz9OnDiByZMnY/Lkydi6dauJK6/boLCqcTem7poK5rgbIiIiwAzCzaJFizBlyhRMnjwZHTt2xPLly2Fvb4+VK1fWuf+gQYMwevRohIeHIyQkBC+88AK6du2KPXvMoztGP+5m76VslFZoTHfg1r0AhRIoTAdy/jLdcYmIiMyMpOGmvLwcR44cQWxsrGGbXC5HbGws9u/ff9vnCyEQHx+PxMREDBgwoM59ysrKkJ+fb3RrTmG+TvB1VqO0QosDl004JdzWDmjVU3ef690QEVELJmm4yc7OhkajgY+Pj9F2Hx8fpKen3/J5eXl5cHR0hFKpxPDhw7FkyRLce++9de47f/58uLi4GG4BAQFNeg43k8lkGFzVNWXyWVP6rimud0NERC2Y5N1SjeHk5ISEhAQcOnQI77zzDmbMmIFdu3bVue/MmTORl5dnuKWmpjZ7fYOquqZ2JZo43NQcVMxxN0RE1ELZSHlwT09PKBQKZGRkGG3PyMiAr6/vLZ8nl8vRrl07AEC3bt1w9uxZzJ8/H4MGDaq1r0qlgkqlatK6b6dvO0/YKmRIul6My9lFaOPpYJoDB0QBchsg/wpwIwlwb2Oa4xIREZkRSVtulEolIiMjER8fb9im1WoRHx+PmJiYBr+OVqtFWVlZc5TYKI4qG0S1cQdg4q4ppYNuQT+AXVNERNRiSd4tNWPGDKxYsQJff/01zp49i6lTp6KoqAiTJ08GAEyYMAEzZ8407D9//nxs374df/31F86ePYsPP/wQq1atwt///nepTqFOhtWKpeyaIiIiaoEk7ZYCgLFjxyIrKwuzZs1Ceno6unXrhi1bthgGGaekpEAur85gRUVFePbZZ3HlyhXY2dkhLCwM3377LcaOHSvVKdRpUKg33v71LA78lYPi8krYK030ow7uC+xZxJWKiYioxZIJ0bJGnubn58PFxQV5eXlwdnZutuMIITBg4U6k5pTgiwk9EdvR5/ZPagplBcB7QYDQAC+eBFwDTXNcIiKiZnQnn9+Sd0tZK5lMJk3XlMoJ8O+mu8+uKSIiaoEYbprRYMOU8CyYtIEsuJ/uK7umiIioBWK4aUa923pAZSPH1dwSXMgsNN2Bg6rCDVtuiIioBWK4aUZ2SgViQjwAmHhKeGBvQCYHblwG8q6a7rhERERmgOGmmUky7kbtDPh21d3nejdERNTCMNw0M324OZx0AwWlFaY7sH7cDS+iSURELQzDTTML9LBHWy8HVGoF9l7MNt2BDYOK2XJDREQtC8ONCRi6ps5lme6ggTEAZMD1i0DBra+wTkREZG0Ybkyg5rgbk00Jt3MFfDvr7rNrioiIWhCGGxPo1cYN9koFMgvKcCYt33QHDmLXFBERtTwMNyagslGgT4gnAN2CfiYTzPVuiIio5WG4MZHBYV4ATLzeTVAf3dfsRKDQhKGKiIhIQgw3JjKoatzN0ZQbyC0uN81B7d0B7066++yaIiKiFoLhxkRaudoh1McJWgHsvmDKKeF9dV85qJiIiFoIhhsTGlTVNbXLlF1TXO+GiIhaGIYbEzJcJfx8FrRaE00JD6pquck8AxRdN80xiYiIJMRwY0KRQW5wUtkgp6gcJ67mmeagDp6AV5jufso+0xyTiIhIQo0KN6mpqbhy5Yrh+4MHD+LFF1/E559/3mSFWSNbhRz9O+imhJt21hTH3RARUcvRqHDz2GOPYefOnQCA9PR03HvvvTh48CDeeOMNzJs3r0kLtDb6WVO7THmVcMOgYo67ISIi69eocHPq1ClERUUBANauXYvOnTtj3759+O677/DVV181ZX1WZ1AH3aDi41fykFVQZpqD6lcqzjgFlNwwzTGJiIgk0qhwU1FRAZVKBQDYsWMHHnjgAQBAWFgY0tLSmq46K+TtrEbnVs4AgN3nTbSwnpMP4NEegACS95vmmERERBJpVLjp1KkTli9fjj/++APbt2/H0KFDAQDXrl2Dh4dHkxZojWpeSNNk9F1TnBJORERWrlHh5v3338dnn32GQYMGYdy4cYiIiAAA/Pzzz4buKro1/bib3eezUKnRmuag+q6ppD9MczwiIiKJ2DTmSYMGDUJ2djby8/Ph5uZm2P7UU0/B3t6+yYqzVt0CXOFqb4vc4gocS81Fr2D35j+ovuUm/SRQmgeoXZr/mERERBJoVMtNSUkJysrKDMEmOTkZixcvRmJiIry9vZu0QGukkMswsIOJL6Tp7A+4twWEFkj50zTHJCIikkCjws3IkSPxzTffAAByc3MRHR2NDz/8EKNGjcKyZcuatEBrVT3uxoRX69ZfiuGv3013TCIiIhNrVLg5evQo+vfvDwD48ccf4ePjg+TkZHzzzTf4+OOPm7RAazWggxdkMuBsWj7S80pNc9C2g3VfL8Wb5nhEREQSaFS4KS4uhpOTEwBg27ZtePDBByGXy9G7d28kJyc3aYHWyt1BiW4BrgCA38+bqGuq7SBAJgeyzgF5V01zTCIiIhNrVLhp164dNmzYgNTUVGzduhX33XcfACAzMxPOzs5NWqA1M3RNnTNR15S9O+DfQ3f/0m+mOSYREZGJNSrczJo1Cy+//DKCg4MRFRWFmJgYALpWnO7duzdpgdZMH272XMxGeaWJpoSH3KP7yq4pIiKyUo0KNw899BBSUlJw+PBhbN261bB9yJAh+Oijj5qsOGvXyd8Zno5KFJZV4nByjmkO2m6I7uulnYBWY5pjEhERmVCjwg0A+Pr6onv37rh27ZrhCuFRUVEICwtrsuKsnVwuw8AO+gtpmqhrqlVPQOUClOYC146Z5phEREQm1Khwo9VqMW/ePLi4uCAoKAhBQUFwdXXFv/71L2i1JupesRKDw0y83o3CBmg7QHf/IrumiIjI+jQq3LzxxhtYunQp3nvvPRw7dgzHjh3Du+++iyVLluCtt95q6hqtWv92XlDIZbiQWYjUnGLTHDRE3zXFcENERNanUZdf+Prrr/HFF18YrgYOAF27dkWrVq3w7LPP4p133mmyAq2di70tIgPdcDApB7vOZ+Hx3kHNf1D9oOIrh4GSXMDOtfmPSUREZCKNarnJycmpc2xNWFgYcnJMNDDWigyq6praZaquKbcgwKMdIDTAZa5WTERE1qVR4SYiIgJLly6ttX3p0qXo2rXrXRfV0uinhO+9lI3SChPNYGp3r+7r+a3170dERGRhGtUttWDBAgwfPhw7duwwrHGzf/9+pKamYtOmTU1aYEsQ5usEX2c10vNLceByjuGims0qdChwYJku3Gg1gFzR/MckIiIygUa13AwcOBDnz5/H6NGjkZubi9zcXDz44IM4ffo0Vq1a1dQ1Wj2ZTGb6WVNBfXVTwouzgatHTHNMIiIiE5AJIURTvdjx48fRo0cPaDTmuzhcfn4+XFxckJeXZ1aXith6Oh1PrzqCYA977HplsGkOum4ycHo90G8GEDvbNMckIiJqhDv5/G70In7UtPq284StQoak68W4nF1kmoOGDtN9Pb/FNMcjIiIyAYYbM+GoskFUG3cAJuyaahcLyBRA5hngRpJpjklERNTMGG7MiOEq4YkmCjf27kCgbkA4Etl6Q0RE1uGOZks9+OCD9T6em5t7N7W0eINCvfH2r2dx4K8cFJdXwl7ZqMlsdyZ0KJC8Bzi/Gej9TPMfj4iIqJndUcuNi4tLvbegoCBMmDChuWq1eiFeDghwt0O5Rot9F6+b5qAdqsbdJO0FSvNMc0wiIqJmdEdNA19++WVz1UGomhIe6o1v9idjZ2ImYjv6NP9BPdvpViu+fhG4sB3o8lDzH5OIiKgZccyNmdGPu9mVmIUmnKVfv/ARuq9nNprmeERERM2I4cbM9G7rAZWNHFdzS3Axs9A0B+04Svf1wnag3ETT0ImIiJoJw42ZsVMqEBPiAcCEs6b8IgC3YKCyhNeaIiIii8dwY4YMU8LPZZnmgDJZdevNmQ2mOSYREVEzYbgxQ/pwcygpBwWlFaY5aKdRuq/nt7FrioiILBrDjRkK9LBHW08HVGoF9l7MNs1B/boBrkG6rqkL20xzTCIiombAcGOmBknRNaVvvTm9wTTHJCIiagYMN2ZqcJgXAN2gYpNNCTfMmtoGlBeb5phERERNjOHGTEW1cYedrQKZBWU4k5ZvmoP6dwdcA4GKYnZNERGRxWK4MVMqGwX6tvMEoFvQzyRqzpo6vd40xyQiImpiDDdmzNA1dc5E690AQOcxuq+JW4CSG6Y7LhERURMxi3DzySefIDg4GGq1GtHR0Th48OAt912xYgX69+8PNzc3uLm5ITY2tt79LZl+UPHRlBvILS43zUH9IgDvjoCmDDj9k2mOSURE1IQkDzdr1qzBjBkzMHv2bBw9ehQRERGIi4tDZmbdrRW7du3CuHHjsHPnTuzfvx8BAQG47777cPXqVRNX3vxaudoh1McJWgHsvmCiKeEyGdDtMd39hO9Nc0wiIqImJHm4WbRoEaZMmYLJkyejY8eOWL58Oezt7bFy5co69//uu+/w7LPPolu3bggLC8MXX3wBrVaL+Ph4E1duGoOquqZ2mbJrqssjgEwBXDkEZF8w3XGJiIiagKThpry8HEeOHEFsbKxhm1wuR2xsLPbv39+g1yguLkZFRQXc3d3rfLysrAz5+flGN0tiuEr4+SxotSaaEu7kA7Qbort//AfTHJOIiKiJSBpusrOzodFo4OPjY7Tdx8cH6enpDXqN1157Df7+/kYBqab58+fDxcXFcAsICLjruk0pMsgNTiob5BSV48TVPNMdOGKc7uvx1YBWY7rjEhER3SXJu6XuxnvvvYfVq1fjp59+glqtrnOfmTNnIi8vz3BLTU01cZV3x1YhR/8OuinhJp01FXo/oHYB8q8Cl3eb7rhERER3SdJw4+npCYVCgYyMDKPtGRkZ8PX1rfe5H3zwAd577z1s27YNXbt2veV+KpUKzs7ORjdLo581tSvRhOHGVl09LZxdU0REZEEkDTdKpRKRkZFGg4H1g4NjYmJu+bwFCxbgX//6F7Zs2YKePXuaolRJDeqgG1R8/EoesgrKTHfgiKpZU2d/AcoKTHdcIiKiuyB5t9SMGTOwYsUKfP311zh79iymTp2KoqIiTJ48GQAwYcIEzJw507D/+++/j7feegsrV65EcHAw0tPTkZ6ejsLCQqlOodl5O6vRuZWuxWn3eROtVgwArXsCHu11l2M49V/THZeIiOguSB5uxo4diw8++ACzZs1Ct27dkJCQgC1bthgGGaekpCAtLc2w/7Jly1BeXo6HHnoIfn5+htsHH3wg1SmYhH7W1E5Tdk3JZECPCbr7h/4DmOoCnkRERHdBJkx2yWnzkJ+fDxcXF+Tl5VnU+JsjyTcwZtk+OKttcPSte2GjMFEuLc4BPgzTrVj8ZLyuNYeIiMjE7uTzW/KWG2qYbgGucLW3RX5pJRJSc013YHt3oPODuvuHvjDdcYmIiBqJ4cZCKOQyDKwaWGzSrikA6PmE7uup9bqWHCIiIjPGcGNBDONuzplwUDGg64ry7arrmkr4zrTHJiIiukMMNxZkQAcvyGTAmbR8pOeVmu7AMhnQq6r15uAKQFNpumMTERHdIYYbC+LuoEREa1cAwO/nTdw11eURwM4dyE0Gzv5s2mMTERHdAYYbCyNZ15TSHoh6Snd/7785LZyIiMwWw42FGRymG1S852I2yiu1pj141BTAxg5ISwCS/jDtsYmIiBqI4cbCdPZ3gaejEoVllTicbOKZSw6eQPfxuvt7/23aYxMRETUQw42FkctlGNhBfyFNE3dNAUDMNEAmBy7uANJPmf74REREt8FwY4H0XVM7z5l4UDEAuLcFwh/Q3d+3xPTHJyIiug2GGwvUv50XFHIZLmQWIjWn2PQF9H1e9/XUj0DeFdMfn4iIqB4MNxbIxd4WkYFuAIBdprxKuF6rSCC4P6Ct5NgbIiIyOww3FmpQVdfULim6pgBgwCu6r0e+YusNERGZFYYbC6Vf72bvpWyUVmhMX0CbAUBQP0BTDvyxyPTHJyIiugWGGwsV5usEX2c1Siu0OHBZgotZymTA4Jm6+0e/AXJTTF8DERFRHRhuLJRMJpN21hQABPfTteBoK4Df35emBiIiopsw3FiwQaH69W4kCjcAcM9buq8J3wMZZ6Srg4iIqArDjQXr284TtgoZkq4X43J2kTRFBETp1r0RWmD7LGlqICIiqoHhxoI5qmwQ1cYdgIRdUwAQOweQ2wAXtwOXdkpXBxERERhuLJ7hKuFSdk15hAC9ntTd3/YWoJVg9hYREVEVhhsLpx93c+CvHBSXV0pXyIBXAZULkHESOPatdHUQEVGLx3Bj4UK8HBDgbodyjRb7L12XrhAHD2Dgq7r7O2YDRRLWQkRELRrDjYWTyWTm0TUFANFPA96dgJIbwA4OLiYiImkw3FgBQ7g5lwUhhHSFKGyBv1WtVnzsWyDlT+lqISKiFovhxgr0busBlY0cV3NLcDGzUNpiAnsD3R/X3f/f/wGaCmnrISKiFofhxgrYKRXo3dYDgBl0TQHAvfMAO3cg8wywZ7HU1RARUQvDcGMlBofqL8WQJXElAOzdgWFVl2P4/T0g7bi09RARUYvCcGMl9FPCDyXloKDUDLqCujwMhI8AtJXAT88AlWVSV0RERC0Ew42VCPZ0QFtPB1RqBfZezJa6HN1Vw/+2GLD31HVP7XxX6oqIiKiFYLixIoNqzJoyCw6ewIh/6+7v+xhI3i9tPURE1CIw3FiRwWFV424SM6WdEl5T+N+AiHG6C2v+OBkoNIMBz0REZNUYbqxIVBt32NkqkFlQhjNp+VKXU+3+DwDPUKAgDfjxH4BGwstEEBGR1WO4sSIqGwX6tvMEAOxKNJOuKQBQOQJjVwFKRyDpD+C3eVJXREREVozhxsoYuqbOmVn3j1coMHKp7v7efwNnf5G2HiIisloMN1ZGP6j4aMoN5BaXS1zNTTqNBnpP093/aSqQeVbaeoiIyCox3FiZVq52CPVxglYAuy+YwZTwm907FwjqC5QXAN+OAfKuSF0RERFZGYYbKzSoqmtql7l1TQG6i2uO/VY3wDj/KrDqQaA4R+qqiIjIijDcWCH9VcJ3nc+CVmsmU8JrsncHHl8POPkD2YnA92OB8mKpqyIiIivBcGOFIoPc4KSyQU5ROU5czZO6nLq5tNYFHLULcOUgsG4SryBORERNguHGCtkq5OjfQTcl3OxmTdXkHQ48thawUQMXtgIbpjLgEBHRXWO4sVL6WVO7Es043ABAYG/g4a8AmQI4uQ5Y83d2URER0V1huLFSgzroBhUfv5KH7EIzvyJ36DDg0e91LTjntwCrRgMlN6SuioiILBTDjZXydlajcytnAMDu82a0WvGthA4FHt+gG4OT+ifw5f1AfprUVRERkQViuLFi+llTO83pUgz1CYoBJm8GHH2BzDPAf+4Dsi9KXRUREVkYhhsrph93s/t8Fio1WomraSCfTsAT2wD3ECAvBVgZB1w5LHVVRERkQRhurFi3AFe42tsir6QCCam5UpfTcG5BwD+2An4RQHG2LuDsWQxoLSSgERGRpBhurJhCLsOA9lUX0jT3WVM3c/QCJv4P6DgK0FYCO2YDq0ZyHA4REd0Ww42Vq75KuIWMu6lJ7aybJv7AUsDWHri8G1jWBzi3SerKiIjIjDHcWLkB7b0gkwFn0vKRnlcqdTl3TiYDejwOPL1b101VkgOsHgf8+hJQUSJ1dUREZIYYbqych6MKEa1dAQC/n7ewrqmaPNsDT2wHYqbrvj/0BfD5YCDjtLR1ERGR2WG4aQEMU8ItsWuqJhsVEPcO8Pf1gIM3kHVWF3B2vguU5ktdHRERmQmGmxZAP+5mz8VslFdawYyjdkOAqfuA9vcBmjLg9/eBf0cA+5awq4qIiBhuWoLO/i7wdFSisKwSh5NzpC6naTh66S66+fDXgEd73VicbW8CH/cAjnzFC3ASEbVgDDctgFwuw8AO+gtpWnjXVE0yGdBpFPDsn7oZVc6tgYJrwC8vAJ9EAyd/5No4REQtEMNNC6Hvmtp2Oh1CCImraWIKG92MquePAkPfA+w9gZxLwH+fAD4fAJzfBljbORMR0S0x3LQQg0O9obaVI+l6MU5ezZO6nOZhowJ6TwVeSAAGvwmonIH0k8D3DwNfDgOS90tdIRERmQDDTQvhoLLBvR19AQAbE65JXE0zUzkBA18BXjgO9HkesFEDKfuBL4cC3z4EXD3KlhwiIismebj55JNPEBwcDLVajejoaBw8ePCW+54+fRpjxoxBcHAwZDIZFi9ebLpCrcDICH8AwC/Hr0GjbQEf7vbuwH3/Ap4/BkROBuQ2wMXtwIrBwCdRwG9v61p2GHSIiKyKpOFmzZo1mDFjBmbPno2jR48iIiICcXFxyMyse7G54uJitG3bFu+99x58fX1NXK3lG9DBCy52tsgsKMOBv65LXY7pOPsDIxYD0w4CXccCCiWQfR7YvRBY3g9YEgnsmAtcS2DQISKyAjIh4ejS6Oho9OrVC0uXLgUAaLVaBAQE4LnnnsPrr79e73ODg4Px4osv4sUXX7yjY+bn58PFxQV5eXlwdnZubOkWa+b6k/jhYAoe6dkaCx6KkLocaZTmA+e3Amc2ABd3AJU1LkvhFgx0HKm7+ffQzcgiIiLJ3cnnt2QtN+Xl5Thy5AhiY2Ori5HLERsbi/37m27gZ1lZGfLz841uLdnIbrquqU0n01FUVilxNRJROwNdHwYe/Q545SLw0EpdmLGxA24kAXv/Day4B1jcFdj6BpB6iFPKiYgsiGThJjs7GxqNBj4+PkbbfXx8kJ6e3mTHmT9/PlxcXAy3gICAJnttSxTdxh1tPB1QWFaJX45b+cDihlA5AZ3HAI98A7x6SbcoYKcHAVsHIC8F2L8U+E8ssLgzsPl13YwrBh0iIrMm+YDi5jZz5kzk5eUZbqmpqVKXJCmZTIZxUbqA9/3BFImrMTNKB92igA9/qQs6Y78FujwMKJ2A/KvAgWW6GVeLwoFNrwBJewCtRuqqiYjoJjZSHdjT0xMKhQIZGRlG2zMyMpp0sLBKpYJKpWqy17MGY3q0xgdbz+PElTycupqHzq1cpC7J/NjaAeEjdLeKUuCvncDpDUDiZqAwHTj4ue7m4K3bp+NIIKivbkFBIiKSlGQtN0qlEpGRkYiPjzds02q1iI+PR0xMjFRltQgejirEddYFSLbeNICtGggdBjz4GfDKBeCxdUC3vwNqV6AoEzj8H+CbB4APQ3WXfrj0G69tRUQkIUm7pWbMmIEVK1bg66+/xtmzZzF16lQUFRVh8uTJAIAJEyZg5syZhv3Ly8uRkJCAhIQElJeX4+rVq0hISMDFixelOgWLpe+a2nDsKnKLyyWuxoLYqIAO9wGjPgFevgD8/b9AjwmAnTtQnK27aOeq0cAH7YGN04ALO4BK/nyJiExJ0qngALB06VIsXLgQ6enp6NatGz7++GNER0cDAAYNGoTg4GB89dVXAICkpCS0adOm1msMHDgQu3btatDxWvpUcD0hBIb9+w+cSy/AK3GhmDa4ndQlWTZNJZD0B3BmI3D2F13Q0VO7AKHDdV1XIYN1AYmIiO7InXx+Sx5uTI3hptr6o1cwY+1xeDup8Mdrg6GyUUhdknXQaoDkvbqgc+ZnXdeVnspZ18XVcSQQMkTX5UVERLfFcFMPhptq5ZVa9F/wGzLyy7BgTFc80qtlT5NvFloNkHpANxj57M9AQVr1Y0pHoMNQXdBpFwso7SUrk4jI3DHc1IPhxtiK3X/hnU1nEeBuh99eGgRbhdWvDiAdrRa4cqiqRWcjkH+l+jFbe6D9fbpuq6C+gEc7ro5MRFQDw009GG6MFZdXYsCCncguLGfrjSlptcC1o7pLQJzeqFswsCYHLyCojy7oBPUBvDsBcgZPImq5GG7qwXBTm771prWbHeJfGsixN6YmBHDtmG4NneR9utYdTZnxPmoXIDCmOvD4RQAKW2nqJSKSAMNNPRhuaisp12Dgwp3ILCjDG/eHY8qAtlKX1LJVlgFXj+oGJSfv043ZKS803sfWHgiIqm7ZaRWpW3iQiMhKMdzUg+GmbmsPpeLV/56Ak9oGv78yGO4OSqlLIj1NJZB+AkjZrws7yXuBkhvG+yiUuoAT1Ed3C4jWXTeLiMhKMNzUg+GmbhqtwIgle3AmLR+P9w7Cv0Z1lrokuhWtFshOrG7ZSdqruyRETTK5rutK37ITGAPYu0tTLxFRE2C4qQfDza3tu5SNx1YcgEwGrJ/aB90D3aQuiRpCCODGZV3ISdmvCz03kmrv5xVe3bIT1Bdw9jN5qUREjcVwUw+Gm/rNWJOA9ceuIszXCT9P7welDWfoWKS8q9VBJ3kfkHWu9j5ubapbdoL6AG7BnH5ORGaL4aYeDDf1yykqR+yi35FTVI4XY9vjxdgOUpdETaEo23jMTvpJQGiN93HyN27Z8Qpl2CEis8FwUw+Gm9vbmHAVL6xOgEIuw5qneqNnMMdqWJ3SPCD1YFXLzn7g6hFAe9OVzO09qqafV7Xu+HYB5FwmgIikwXBTD4abhvm/NQn46dhVtHK1w6bn+8PFnmuqWLXyYl3A0bfspB4EKkuM91E6AYG9gaCqwOPfnRcBJSKTYbipB8NNwxSUVuBvS/Yg+XoxBnTwwsqJPWHDSzO0HJXlQNrx6jE7KX8CZXnG+9iogda9qruyWvcClA7S1EtEVo/hph4MNw136moeHl6+HyUVGkzuG4zZIzpJXRJJRasBMk5Xt+wk7wOKs433kdvoWnP0Y3YCogE7V0nKJSLrw3BTD4abO7P5ZBqmfncUADB7REdM7ttG4orILAgBZF+oDjrJe4H8qzftJAN8O9eYkdUXcPCUpFwisnwMN/VguLlzS3+7gA+2nQcAvD+mC8b2CpS4IjI7QgC5KcYtOzmXau/n2cH4gqAurU1fKxFZJIabejDc3DkhBN759Sy+2HMZMhkwf3QXPBrFgEO3UZBeFXaqAk/mmdr7uAYat+y4t+X0cyKqE8NNPRhuGkcIgbc2nsK3f6YAAF4Y0h4vxraHjB9E1FDFOcZr7aQdr73WjqOPccuOVzgg50B2ImK4qRfDTeMJIfDhtvNYuvMiAOChyNZ4e1RnqG259gk1QlmB7orn+tadq0cATbnxPmrXGgsL9gF8IwCFjSTlEpG0GG7qwXBz9747kIy3NpyCVgBhvk5Y+lh3tPPmFajpLlWU1l5rp6LIeB+lIxAQVd26498DsFVLUy8RmRTDTT0YbprG3ovZeGF1ArILy2Bnq8DLcaGYGBPEtXCo6WgqgLQTNdba2adbWbkmhQpo3bPGWjtRgMpRmnqJqFkx3NSD4abpZBaU4v/WJGDvxesAgM6tnPH2qC7oFuAqbWFknbRa3aDkmjOyijKN95EpAP9u1S07gb0BO17dnsgaMNzUg+GmaWm1AqsPpeK9zWeRX1oJAIjr5IMXhnRAR3/+fKkZCQFcv1RjrZ19QF7KTTvJAJ9O1S07gX0AJx9JyiWiu8NwUw+Gm+aRVVCG+ZvP4qdjV6H/jYrr5IOJfYIR09aDs6rINHJTdBcC1Qee6xdq7+PRznhGliuXNSCyBAw39WC4aV4XMgrw8W8X8b8T1wwhp42nAx7tFYDRPVrB24mDP8mECjONp5+nnwJw0588l4AaM7L66sIPwziR2WG4qQfDjWmczyjA1/uSsDHhGgrLdN1VMhkQGeiG+zr54N6OvmjjyYsskomV5Oqmnyft0YWea8cAbaXxPg5eQGBMdcuOTydAzuUOiKTGcFMPhhvTKiqrxC/Hr2H1oVQkpOYaPdbG0wG9gt0Q1cYDUcHuCHC3Y/cVmVZ5EXDlUPWYnSuHgMpS431ULrqByYbp590Aha0k5RK1ZAw39WC4kU5aXgm2n8nAttMZ+POv66jUGv/q+Tqr0S3AFeF+zgj3c0K4nzNauzHwkAlVlulacwzTzw8A5QXG+9jaA617VbfstO4J2NpJUy9RC8JwUw+GG/OQV1KBI8k5OHA5B4cu5+Dk1TxUaGr/KjqpbBDm54RQXycEezjobp72aO1mz5WRqflpKoGMkzWukbUPKMkx3kduC7SKrG7ZCYgC1PzbQtTUGG7qwXBjnkrKNUhIzcXpa3k4k5aPs2kFuJhZUGfgAXTjd/yc1QiqCjtBHg4IcreHn6sd/F3U8HRUQS5niw81Ma0WyD4PJO+pnpVVkGa8j0wO+HatbtkJjAEcPKSpl8iKMNzUg+HGcpRXavFXdiHOpuXjfEYhUq4XIzmnCEnZxYZByrdiq5DB10UNPxdd2NGHHn9XO902VzVc7GzZ5UV3RwjgRpLx1c9vXK69n1e48TWynP1NXiqRpWO4qQfDjeUTQiCnqBxJ14uRfL3I8DUlpxhpuaXILCiFtgG/1Xa2Cvi5qtHK1Q5+VUHI10UNX2c1fJzV8HFWwd1ByQBEdyb/mnE3VtbZ2vu4talu2QnqA7gFc/o50W0w3NSD4cb6VWq0yCgoQ1puCa7lleq+6u/nlSAttxTXi8pv/0IAlAo5vJ1V8HHWhR5vZxV8ndXwdVHD20ltCEN2So7/oVsoun7TWjsnAKE13sfJHwiKqR634xkKyHmdNqKaGG7qwXBDAFBaoUF6Xml16Kn6mpFfivQ8XetPdmHDAhAAOKltbgo9qqrWn+qWIE9HJS8sSkBpvu6K5/oZWVePANoK433s3I27sXy6AAobaeolMhMMN/VguKGGKq/UIrOgFBn5ZcjIrwo++aXIyKvelp5fiuJyTYNeTy4DvJyqQ49PVSuQIQS56L46q23YFdaSVJQAVw5Xt+ykHgQqS4z3UToBgdE11trpDtiopKmXSCIMN/VguKGmJIRAYVllVfgpQ3qeLvBkVgWf9PwyZOaXIrOgDJqGDASCbiyQj7PKKPR4O6mMxgN5O6ugsmFXmFWqLAfSjtdYa+dPoCzPeB8bddVaO1UtO617AUqu+E3WjeGmHgw3JAWNVuB6URky8sp0rT/51V1gGQVlyKgKRXklFbd/sSruDkqj0ONt6AJTGUKRu72SU+ItnVYDZJ6pbtlJ3gcUZRnvI7fRteYY1tqJBuxcJSmXqLkw3NSD4YbMWWmFproVyNAFVlojEOm2l1dqb/9i0E2J93ZSG7UE6YKP8ZggBxXHc1gMIYDrF3VBJ6kq7ORfuWknGeDbucZaO30ARy9JyiVqKgw39WC4IUsnhEBeSYWu2yuvFJlVgadmd1hGfhmyC8vQ0H/dTiob3UwwFzV8nNTwcandCuTpqIItB0Sbp9wU45ad6xdr7+PZobplJ6gP4NLa9HUS3QWGm3ow3FBLUaHRIqug7JZdYPqWoNstiKgnkwGejiqj0GOYDeZSPUCaiyOagYIMIKXGWjsZp2rv4xpYY62dvoB7W661Q2aN4aYeDDdExgwDovNKkVFQivQ849lhmVUzw26+0OmtqGzkda8LVLVNv53XBjOh4hwg9UB1y861BEDcNMvP0ce4ZccrnGvtkFlhuKkHww3RndNqBa4XlRtPic8vqxGIdNtvFDd8QLSrvW2NLrDa6wL5uKjg4aCCggOim15ZQY21dvYDVw8DmpvWdVK7Gq+14xvBtXZIUgw39WC4IWo+pRUaZBVUzwjTLYhYe4p8aUXDBkQr5DJ4Oap03V5VM8N8nKunxusDEdcGuksVpcC1ozWmnx8AKoqM97F1qF5rJ7CP7krotmpp6qUWieGmHgw3RNISQiC/pNKoxceoNajq+6yCsgZdIwwA1LZyo9lg+iB08/R4doU1kKZCd5mImtfIKs013kehBFr1rG7ZCYgCVE6SlEstA8NNPRhuiCxDpUaL60Xl1QGoajC00f2CUuTeQVeYi53tTdPiVYY1gvRdYrxMRh20Wt0FQGvOyCrMMN5HpgD8IqrH7QT2BuzdpamXrBLDTT0YboisS2mFRjfo+aaWoJsvm9HQrrBbzQq7eYaYq30LnhUmBJDzV3XQSd6rm45+M+9OxuN2nHxNXytZDYabejDcELU8QggUlFUargumHxOUeVN32J1cJkN/xfial8Soea0wfRhqMQsk5qbWuPr5PiA7sfY+7iHGM7JcAzn9nBqM4aYeDDdEdCs3zwrTB6GaISgzvxTXi+7givFVCyRWT4+vnh3mXTVN3stRBaWNlXWFFWbVCDt7gfSTAG76uHFuVaNlp69uoUGGHboFhpt6MNwQ0d0qq9TUWCCxrNaaQPr7DV0gEQA8HJTV44Bc1FWXzdBdKsPbyQquFVaSW2P6+T7d7CztTT8fe0/jlh2fToCcg8BJh+GmHgw3RGQq1VeMrz0OSH8V+cyCUlRoGvZn2EYug7eTfmp8VddX1X3fqlWivZ3VcFJZwNT48mLgyqHqlp0rh4DKUuN9VC66gcn6wOMXAdgopamXJMdwUw+GGyIyJ0II3CiuqLo8RlUXWJ5ugLR+RtidXivMXqmocz0gnxrdY15OZrZKdGU5cO1YjbV2/gTKC4z3sbUHWveqbtlp3ROwtZOmXjI5hpt6MNwQkSWq0GiRXVhWNSOsDJmG2WE175civ7ThXWFu9rbVY39qjgOqMU3ew1GiVaK1Gt04nZrTz0tyjPeR2wKtelS37AREA2r+XbdWDDf1YLghImtWXF5Za+xP9eyw6vtllQ2bGi+XAV5ONdcDUtUYGF3dItTsF0wVAshKrDH9fB9QcM14H5kc8O1S3bIT2Adw8Gi+msikGG7qwXBDRC2dfpXo9BrrAGXeNDtM3yLU0FWi9RdMvXk9oJozxXyc1bBTNlFXmBDAjSTjlp0bl2vv5xVmPEjZ2b9pjk8mx3BTD4YbIqKG0WgFrheW1bo0hi4QlVWFoDu7YKqz2sb4Uhk3rxjtooanowq2jVklOv9adatOyn4g80ztfdyCq4NOUB/ArQ2nn1sIhpt6MNwQETUt/QVT67pGWM2ZYSUVmga9nkwGeDio4Ouigo/TTdcIqzFTzN1BWX9XWHGO8Vo7accBcVN3nJNfVRdWjC70eIUBcitbc8hKMNzUg+GGiMj09KtEZ9YIO7rZYdX3M6quIl95B6tEezmpjNYG0k+J93FS64KQsxqO+lWiS/OBKwerW3euHgE0Ny3IaOemG6ujb9nx7QooWsgq02bO4sLNJ598goULFyI9PR0RERFYsmQJoqKibrn/unXr8NZbbyEpKQnt27fH+++/j/vvv79Bx2K4ISIyX1qtQE5xuWENoPQ8/aUxjGeHZRc2fJVoR/0q0VXhR3/f3xFoU3YWvjeOwinjEORXDgIVxcZPVjrqZmHpx+206gHYqJr4rKkhLCrcrFmzBhMmTMDy5csRHR2NxYsXY926dUhMTIS3t3et/fft24cBAwZg/vz5+Nvf/obvv/8e77//Po4ePYrOnTvf9ngMN0RElq+8UousqqnxmTePA6oKQpn5ZSi4g1Wive3l6ONwFTGKc+iiOY22xSeg1hQa7SMUKsha96oKOzFA6yhA5djUp0d1sKhwEx0djV69emHp0qUAAK1Wi4CAADz33HN4/fXXa+0/duxYFBUV4X//+59hW+/evdGtWzcsX778tsdjuCEiajmKDKtEG18l/ubp8uWa2lPj5dAiVJaKKPk5RMnPIkp+Dl6yfKN9NFAg2ykMud5RqGzdG3Z+4VDY2prq9MyWrVIN/4C2Tfqad/L5LWlHYnl5OY4cOYKZM2catsnlcsTGxmL//v11Pmf//v2YMWOG0ba4uDhs2LChzv3LyspQVlZm+D4/P7/O/YiIyPo4qGzQ1ssRbb1u3boihEBucYVharxxGPLF0fyu2JRfiuzCUgQj3RB2ouXn0FqWDZ+C0/ApOA1c+tKEZ2beztmEw//NPyU7vqThJjs7GxqNBj4+PkbbfXx8cO7cuTqfk56eXuf+6enpde4/f/58zJ07t2kKJiIiqyOTyeDmoISbgxLhfrduEajUaJFdWG4IQTvzS1GcdRlOGYfhl3sEbUpOwUubBdnNVz9vgTQKacclWf0Q8JkzZxq19OTn5yMgIEDCioiIyBLZKOTwddENSq4WDGCwRBWZr04SH1/ScOPp6QmFQoGMjAyj7RkZGfD19a3zOb6+vne0v0qlgkrFke1EREQthaQrFSmVSkRGRiI+Pt6wTavVIj4+HjExMXU+JyYmxmh/ANi+ffst9yciIqKWRfJuqRkzZmDixIno2bMnoqKisHjxYhQVFWHy5MkAgAkTJqBVq1aYP38+AOCFF17AwIED8eGHH2L48OFYvXo1Dh8+jM8//1zK0yAiIiIzIXm4GTt2LLKysjBr1iykp6ejW7du2LJli2HQcEpKCuQ1lsLu06cPvv/+e7z55pv45z//ifbt22PDhg0NWuOGiIiIrJ/k69yYGte5ISIisjx38vnNq4MRERGRVWG4ISIiIqvCcENERERWheGGiIiIrArDDREREVkVhhsiIiKyKgw3REREZFUYboiIiMiqMNwQERGRVZH88gumpl+QOT8/X+JKiIiIqKH0n9sNubBCiws3BQUFAICAgACJKyEiIqI7VVBQABcXl3r3aXHXltJqtbh27RqcnJwgk8ma9LXz8/MREBCA1NRUq7xulbWfH2D958jzs3zWfo7Wfn6A9Z9jc52fEAIFBQXw9/c3uqB2XVpcy41cLkfr1q2b9RjOzs5W+QurZ+3nB1j/OfL8LJ+1n6O1nx9g/efYHOd3uxYbPQ4oJiIiIqvCcENERERWheGmCalUKsyePRsqlUrqUpqFtZ8fYP3nyPOzfNZ+jtZ+foD1n6M5nF+LG1BMRERE1o0tN0RERGRVGG6IiIjIqjDcEBERkVVhuCEiIiKrwnDTRD755BMEBwdDrVYjOjoaBw8elLqkOs2fPx+9evWCk5MTvL29MWrUKCQmJhrtM2jQIMhkMqPbM888Y7RPSkoKhg8fDnt7e3h7e+OVV15BZWWl0T67du1Cjx49oFKp0K5dO3z11VfNfXqYM2dOrdrDwsIMj5eWlmLatGnw8PCAo6MjxowZg4yMDIs4N73g4OBa5yiTyTBt2jQAlvf+7d69GyNGjIC/vz9kMhk2bNhg9LgQArNmzYKfnx/s7OwQGxuLCxcuGO2Tk5OD8ePHw9nZGa6urnjiiSdQWFhotM+JEyfQv39/qNVqBAQEYMGCBbVqWbduHcLCwqBWq9GlSxds2rSpWc+voqICr732Grp06QIHBwf4+/tjwoQJuHbtmtFr1PWev/fee2Zxfrc7RwCYNGlSrfqHDh1qtI+lvocA6vz3KJPJsHDhQsM+5vweNuRzwZR/O5vk81TQXVu9erVQKpVi5cqV4vTp02LKlCnC1dVVZGRkSF1aLXFxceLLL78Up06dEgkJCeL+++8XgYGBorCw0LDPwIEDxZQpU0RaWprhlpeXZ3i8srJSdO7cWcTGxopjx46JTZs2CU9PTzFz5kzDPn/99Zewt7cXM2bMEGfOnBFLliwRCoVCbNmypVnPb/bs2aJTp05GtWdlZRkef+aZZ0RAQICIj48Xhw8fFr179xZ9+vSxiHPTy8zMNDq/7du3CwBi586dQgjLe/82bdok3njjDbF+/XoBQPz0009Gj7/33nvCxcVFbNiwQRw/flw88MADok2bNqKkpMSwz9ChQ0VERIT4888/xR9//CHatWsnxo0bZ3g8Ly9P+Pj4iPHjx4tTp06JH374QdjZ2YnPPvvMsM/evXuFQqEQCxYsEGfOnBFvvvmmsLW1FSdPnmy288vNzRWxsbFizZo14ty5c2L//v0iKipKREZGGr1GUFCQmDdvntF7WvPfrJTnd7tzFEKIiRMniqFDhxrVn5OTY7SPpb6HQgij80pLSxMrV64UMplMXLp0ybCPOb+HDflcMNXfzqb6PGW4aQJRUVFi2rRphu81Go3w9/cX8+fPl7CqhsnMzBQAxO+//27YNnDgQPHCCy/c8jmbNm0ScrlcpKenG7YtW7ZMODs7i7KyMiGEEK+++qro1KmT0fPGjh0r4uLimvYEbjJ79mwRERFR52O5ubnC1tZWrFu3zrDt7NmzAoDYv3+/EMK8z+1WXnjhBRESEiK0Wq0QwrLfv5s/OLRarfD19RULFy40bMvNzRUqlUr88MMPQgghzpw5IwCIQ4cOGfbZvHmzkMlk4urVq0IIIT799FPh5uZmOD8hhHjttddEaGio4ftHHnlEDB8+3Kie6Oho8fTTTzfb+dXl4MGDAoBITk42bAsKChIfffTRLZ9jLucnRN3nOHHiRDFy5MhbPsfa3sORI0eKe+65x2ibJb2HN38umPJvZ1N9nrJb6i6Vl5fjyJEjiI2NNWyTy+WIjY3F/v37JaysYfLy8gAA7u7uRtu/++47eHp6onPnzpg5cyaKi4sNj+3fvx9dunSBj4+PYVtcXBzy8/Nx+vRpwz41fyb6fUzxM7lw4QL8/f3Rtm1bjB8/HikpKQCAI0eOoKKiwqiusLAwBAYGGuoy93O7WXl5Ob799lv84x//MLoQrCW/fzVdvnwZ6enpRrW4uLggOjra6D1zdXVFz549DfvExsZCLpfjwIEDhn0GDBgApVJp2CcuLg6JiYm4ceOGYR9zOOe8vDzIZDK4uroabX/vvffg4eGB7t27Y+HChUbN/ZZwfrt27YK3tzdCQ0MxdepUXL9+3ah+a3kPMzIy8Ouvv+KJJ56o9ZilvIc3fy6Y6m9nU36etrgLZza17OxsaDQaozcUAHx8fHDu3DmJqmoYrVaLF198EX379kXnzp0N2x977DEEBQXB398fJ06cwGuvvYbExESsX78eAJCenl7n+eofq2+f/Px8lJSUwM7OrlnOKTo6Gl999RVCQ0ORlpaGuXPnon///jh16hTS09OhVCprfWj4+Pjctm5zOLe6bNiwAbm5uZg0aZJhmyW/fzfT11NXLTVr9fb2NnrcxsYG7u7uRvu0adOm1mvoH3Nzc7vlOetfwxRKS0vx2muvYdy4cUYXHHz++efRo0cPuLu7Y9++fZg5cybS0tKwaNEiwzmY8/kNHToUDz74INq0aYNLly7hn//8J4YNG4b9+/dDoVBY1Xv49ddfw8nJCQ8++KDRdkt5D+v6XDDV384bN2402ecpw00LNm3aNJw6dQp79uwx2v7UU08Z7nfp0gV+fn4YMmQILl26hJCQEFOXeUeGDRtmuN+1a1dER0cjKCgIa9euNWnoMJX//Oc/GDZsGPz9/Q3bLPn9a8kqKirwyCOPQAiBZcuWGT02Y8YMw/2uXbtCqVTi6aefxvz58y1iCf9HH33UcL9Lly7o2rUrQkJCsGvXLgwZMkTCypreypUrMX78eKjVaqPtlvIe3upzwdKwW+oueXp6QqFQ1Bo1npGRAV9fX4mqur3p06fjf//7H3bu3InWrVvXu290dDQA4OLFiwAAX1/fOs9X/1h9+zg7O5s0ZLi6uqJDhw64ePEifH19UV5ejtzc3Fp13a5u/WP17WPqc0tOTsaOHTvw5JNP1rufJb9/+nrq+/fl6+uLzMxMo8crKyuRk5PTJO+rKf4d64NNcnIytm/fbtRqU5fo6GhUVlYiKSkJgPmf383atm0LT09Po99JS38PAeCPP/5AYmLibf9NAub5Ht7qc8FUfzub8vOU4eYuKZVKREZGIj4+3rBNq9UiPj4eMTExElZWNyEEpk+fjp9++gm//fZbrWbQuiQkJAAA/Pz8AAAxMTE4efKk0R8j/R/kjh07Gvap+TPR72Pqn0lhYSEuXboEPz8/REZGwtbW1qiuxMREpKSkGOqypHP78ssv4e3tjeHDh9e7nyW/f23atIGvr69RLfn5+Thw4IDRe5abm4sjR44Y9vntt9+g1WoNwS4mJga7d+9GRUWFYZ/t27cjNDQUbm5uhn2kOGd9sLlw4QJ27NgBDw+P2z4nISEBcrnc0JVjzudXlytXruD69etGv5OW/B7q/ec//0FkZCQiIiJuu685vYe3+1ww1d/OJv08vaPhx1Sn1atXC5VKJb766itx5swZ8dRTTwlXV1ejUePmYurUqcLFxUXs2rXLaEpicXGxEEKIixcvinnz5onDhw+Ly5cvi40bN4q2bduKAQMGGF5DP+XvvvvuEwkJCWLLli3Cy8urzil/r7zyijh79qz45JNPTDJd+qWXXhK7du0Sly9fFnv37hWxsbHC09NTZGZmCiF00xkDAwPFb7/9Jg4fPixiYmJETEyMRZxbTRqNRgQGBorXXnvNaLslvn8FBQXi2LFj4tixYwKAWLRokTh27JhhttB7770nXF1dxcaNG8WJEyfEyJEj65wK3r17d3HgwAGxZ88e0b59e6NpxLm5ucLHx0c8/vjj4tSpU2L16tXC3t6+1jRbGxsb8cEHH4izZ8+K2bNnN8k02/rOr7y8XDzwwAOidevWIiEhwejfpH6Gyb59+8RHH30kEhISxKVLl8S3334rvLy8xIQJE8zi/G53jgUFBeLll18W+/fvF5cvXxY7duwQPXr0EO3btxelpaWG17DU91AvLy9P2Nvbi2XLltV6vrm/h7f7XBDCdH87m+rzlOGmiSxZskQEBgYKpVIpoqKixJ9//il1SXUCUOftyy+/FEIIkZKSIgYMGCDc3d2FSqUS7dq1E6+88orROilCCJGUlCSGDRsm7OzshKenp3jppZdERUWF0T47d+4U3bp1E0qlUrRt29ZwjOY0duxY4efnJ5RKpWjVqpUYO3asuHjxouHxkpIS8eyzzwo3Nzdhb28vRo8eLdLS0izi3GraunWrACASExONtlvi+7dz5846fycnTpwohNBNB3/rrbeEj4+PUKlUYsiQIbXO+/r162LcuHHC0dFRODs7i8mTJ4uCggKjfY4fPy769esnVCqVaNWqlXjvvfdq1bJ27VrRoUMHoVQqRadOncSvv/7arOd3+fLlW/6b1K9bdOTIEREdHS1cXFyEWq0W4eHh4t133zUKBlKe3+3Osbi4WNx3333Cy8tL2NraiqCgIDFlypRaH1aW+h7qffbZZ8LOzk7k5ubWer65v4e3+1wQwrR/O5vi81RWdWJEREREVoFjboiIiMiqMNwQERGRVWG4ISIiIqvCcENERERWheGGiIiIrArDDREREVkVhhsiIiKyKgw3RNQiyWQybNiwQeoyiKgZMNwQkclNmjQJMpms1m3o0KFSl0ZEVsBG6gKIqGUaOnQovvzyS6NtKpVKomqIyJqw5YaIJKFSqeDr62t001/9WCaTYdmyZRg2bBjs7OzQtm1b/Pjjj0bPP3nyJO655x7Y2dnBw8MDTz31FAoLC432WblyJTp16gSVSgU/Pz9Mnz7d6PHs7GyMHj0a9vb2aN++PX7++WfDYzdu3MD48ePh5eUFOzs7tG/fvlYYIyLzxHBDRGbprbfewpgxY3D8+HGMHz8ejz76KM6ePQsAKCoqQlxcHNzc3HDo0CGsW7cOO3bsMAovy5Ytw7Rp0/DUU0/h5MmT+Pnnn9GuXTujY8ydOxePPPIITpw4gfvvvx/jx49HTk6O4fhnzpzB5s2bcfbsWSxbtgyenp6m+wEQUePd8aU2iYju0sSJE4VCoRAODg5Gt3feeUcIobtK8TPPPGP0nOjoaDF16lQhhBCff/65cHNzE4WFhYbHf/31VyGXyw1Xm/b39xdvvPHGLWsAIN58803D94WFhQKA2Lx5sxBCiBEjRojJkyc3zQkTkUlxzA0RSWLw4MFYtmyZ0TZ3d3fD/ZiYGKPHYmJikJCQAAA4e/YsIiIi4ODgYHi8b9++0Gq1SExMhEwmw7Vr1zBkyJB6a+jatavhvoODA5ydnZGZmQkAmDp1KsaMGYOjR4/ivvvuw6hRo9CnT59GnSsRmRbDDRFJwsHBoVY3UVOxs7Nr0H62trZG38tkMmi1WgDAsGHDkJycjE2bNmH79u0YMmQIpk2bhg8++KDJ6yWipsUxN0Rklv78889a34eHhwMAwsPDcfz4cRQVFRke37t3L+RyOUJDQ+Hk5ITg4GDEx8ffVQ1eXl6YOHEivv32WyxevBiff/75Xb0eEZkGW26ISBJlZWVIT0832mZjY2MYtLtu3Tr07NkT/fr1w3fffYeDBw/iP//5DwBg/PjxmD17NiZOnIg5c+YgKysLzz33HB5//HH4+PgAAObMmYNnnnkG3t7eGDZsGAoKCrB3714899xzDapv1qxZiIyMRKdOnVBWVob//e9/hnBFROaN4YaIJLFlyxb4+fkZbQsNDcW5c+cA6GYyrV69Gs8++yz8/Pzwww8/oGPHjgAAe3t7bN26FS+88AJ69eoFe3t7jBkzBosWLTK81sSJE1FaWoqPPvoIL7/8Mjw9PfHQQw81uD6lUomZM2ciKSkJdnZ26N+/P1avXt0EZ05EzU0mhBBSF0FEVJNMJsNPP/2EUaNGSV0KEVkgjrkhIiIiq8JwQ0RERFaFY26IyOywt5yI7gZbboiIiMiqMNwQERGRVWG4ISIiIqvCcENERERWheGGiIiIrArDDREREVkVhhsiIiKyKgw3REREZFUYboiIiMiq/D/jWoXDHatehAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create empty lists to keep track of model progress\n",
    "\n",
    "# Plot the loss curves\n",
    "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb295f-e41b-44e2-8bc9-a7dab4aec8af",
   "metadata": {},
   "source": [
    "## Saving a model in pyTorch\n",
    "\n",
    "There are three main methods you should about for saving and loading models in Pytorch.\n",
    "1. `torch.save()` - allows you save a Pytorch object in Python's pickle format \n",
    "2. `torch.load()` - allow you load a saved PyTorch object \n",
    "3. `torch.nn.Module.load_state_dict()` this allows to laod a model's saved state dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "223d0ab6-1b7e-4e7e-96bc-f2f89f9512af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save the model we have \n",
    "from pathlib import Path\n",
    "\n",
    "MODEL_PATH = Path(\"models\")\n",
    "MODEL_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "#2. Create model save path \n",
    "MODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\n",
    "MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n",
    "\n",
    "## To save the model in the file \n",
    "torch.save(obj=model_0.state_dict(), f=MODEL_SAVE_PATH)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be4cbbf-1193-4d00-83b1-a6038a4118cf",
   "metadata": {},
   "source": [
    "# load our model back into the code \n",
    "|[ Since we saved our model's `state_dict()` rather the entire model, we'll create a new instance of our model class and load the saved `state_dict()` into that]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62e92ec5-9b5c-4811-866e-18763e139140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to load in a saved state_dict we have to instantiate a new instance of our model class\n",
    "loaded_model_0 = LinearRegressionModel()\n",
    "\n",
    "# Load the saved state_dict of model_0 (this will update the new instance with updated parameters)\n",
    "\n",
    "loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "95d8bb02-137c-42c0-b852-41c73ae7e8bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([0.6999])), ('bias', tensor([0.3000]))])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a834e9f7-bd01-461f-b94b-ca3c1dc5b4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1885cbe1-b4fe-4036-be4d-1a0965dd1a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fe3563-01c7-4590-b232-4349790f220c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
