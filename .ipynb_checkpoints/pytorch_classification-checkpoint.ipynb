{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9d7d464-dfd8-4dfa-bb21-2c8344aec5e0",
   "metadata": {},
   "source": [
    "# 1  Neural Network classification with pytroch\n",
    "\n",
    "classification is a problem of predicting weather something is one thing or another ( there can be multiple things as the options )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6572a939-aaa5-4751-98ae-94c9feddfae4",
   "metadata": {},
   "source": [
    "## Make the classification data and get it ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68a0eb33-5996-4a3b-99ec-8fb70bd371de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import make_circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cc2f4e1-a2a3-435f-9e7c-a8193e249cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make 1000 samples\n",
    "n_samples = 1000\n",
    "\n",
    "X,y = make_circles(n_samples, noise=0.03, random_state= 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46349a7f-8bd5-49bd-8705-1cfc42356e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a2f6df1-3cd5-4a9e-9d95-0e55862d1d54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 sample of X :\n",
      " [[ 0.75424625  0.23148074]\n",
      " [-0.75615888  0.15325888]\n",
      " [-0.81539193  0.17328203]\n",
      " [-0.39373073  0.69288277]\n",
      " [ 0.44220765 -0.89672343]]\n",
      "First 5 sample of y :\n",
      " [1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(f\"First 5 sample of X :\\n {X[:5]}\")\n",
    "print(f\"First 5 sample of y :\\n {y[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5368272-6b05-4e4e-9f52-a0712163b857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: <object object at 0x0000021B6C7173E0>\n",
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aad4f6af-6792-469a-84e3-d35e2c3b054d",
   "metadata": {},
   "outputs": [],
   "source": [
    "circles = pd.DataFrame({\"X1\": X[:, 0], \n",
    "                        \"X2\": X[:, 1],\n",
    "                       \"label\": y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee4fc11-97a3-4f75-a0a4-f24293406f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.754246</td>\n",
       "      <td>0.231481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.756159</td>\n",
       "      <td>0.153259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.815392</td>\n",
       "      <td>0.173282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.393731</td>\n",
       "      <td>0.692883</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.442208</td>\n",
       "      <td>-0.896723</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.479646</td>\n",
       "      <td>0.676435</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.013648</td>\n",
       "      <td>0.803349</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.771513</td>\n",
       "      <td>0.147760</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-0.169322</td>\n",
       "      <td>-0.793456</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.121486</td>\n",
       "      <td>1.021509</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         X1        X2  label\n",
       "0  0.754246  0.231481      1\n",
       "1 -0.756159  0.153259      1\n",
       "2 -0.815392  0.173282      1\n",
       "3 -0.393731  0.692883      1\n",
       "4  0.442208 -0.896723      0\n",
       "5 -0.479646  0.676435      1\n",
       "6 -0.013648  0.803349      1\n",
       "7  0.771513  0.147760      1\n",
       "8 -0.169322 -0.793456      1\n",
       "9 -0.121486  1.021509      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "628cb6bb-37c5-4c4c-838e-f1803ee824d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize the data we had\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('classic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a80e9973-be64-4caa-b3d2-7b1a6ebfe51b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x21b73605610>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.scatter(x=X[:, 0],\n",
    "           y=X[:, 1],\n",
    "           c=y,\n",
    "           cmap=plt.cm.RdYlBu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c746559e-d189-4453-bc0d-cf3517e05a51",
   "metadata": {},
   "source": [
    "Note: The data we are working with is often referred to as a toy dataset, a dataset that is small enough to experiment but still sizeable enough to practise the fundamentals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec73b9a-71a2-4cd1-bb3a-0801c612b5ae",
   "metadata": {},
   "source": [
    "## 1.1 Check the input shapes and the output shape. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9cdf2479-5a4b-4a71-a7bb-dda5c3ba8180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 2), (1000,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## for the work with the pytorch, we need to trasfer the data type into the tensor.\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f9ce68-bdc6-47e4-80b6-320db5b845f5",
   "metadata": {},
   "source": [
    "## 1.2 Turn data into tensors and create train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85d5e019-7ac8-4178-b81b-05fddecd2a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1+cu117'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__ ## version > 1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "578d2257-db92-4484-87fe-9cdaa25ac193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X) ## since this is the numpy type array this should be transger into the tensor data type for pytorch to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "669440e4-65df-4e44-8465-6d163bb5fbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.7542,  0.2315],\n",
       "         [-0.7562,  0.1533],\n",
       "         [-0.8154,  0.1733],\n",
       "         [-0.3937,  0.6929],\n",
       "         [ 0.4422, -0.8967]]),\n",
       " tensor([1., 1., 1., 1., 0.]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Turn data into tensors\n",
    "y_tensor = torch.from_numpy(y).type(torch.float)\n",
    "X_tensor = torch.from_numpy(X).type(torch.float)\n",
    "\n",
    "X_tensor[:5], y_tensor[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3344d4f-505c-49ff-89a4-4b57762ef491",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3865ca-516a-4873-a359-acbd052ee2a5",
   "metadata": {},
   "source": [
    "# split data into training and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a08cd79e-6fd1-4807-a02d-263749e95bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "64b70463-4b1b-415b-8c23-772288a00173",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, \n",
    "                                                    y_tensor, \n",
    "                                                    test_size=0.2, # 0.2 means 20% test set, 80% train set.\n",
    "                                                    random_state = RANDOM_SEED )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd62afb4-8a9f-4347-b80c-56ff1aa03434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 200, 800, 200)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train), len(X_test), len(y_train), len(y_test) ## so we could see that the data set have been split into 2:8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd767184-2a4b-43ce-b0a8-b8155289a2bf",
   "metadata": {},
   "source": [
    "## Build a Model\n",
    "\n",
    "Let's build a model to classify our blue and red dots.\n",
    "\n",
    "\n",
    "To do so, we want to: \n",
    "1. set up device agonistic code so our code will run on an accelerator (GPU) if there is one\n",
    "2.  Construct a model (By subClassing `nn.Module`)\n",
    "3.  Define loss function and optimizer\n",
    "4.  create a train and test loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48dca9af-0d6a-4388-ba48-a642d85c0fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pytorch and nn \n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# Make device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b72433-e586-4475-946f-25f022c4039e",
   "metadata": {},
   "source": [
    "## Go for the Model!!!!!\n",
    "1. subclass `nn.Module` (almost all models in pytorch subclass `nn.Module`)\n",
    "2. Create 2 `nn.Linear()` Layers that are capable of handling the shapes of our data\n",
    "3. Defines a `forward()` method that outlines the forward pass (or forward computation) of the model\n",
    "4. Instatiate an instance of our model class and send it to the target `device`\n",
    "5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b8bace59-94e4-4f86-a908-346e84d7c547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CircleModelV0(\n",
       "  (layer_1): Linear(in_features=2, out_features=5, bias=True)\n",
       "  (layer_2): Linear(in_features=5, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. construct a model that subclass nn.Moudle\n",
    "class CircleModelV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #2. Create 2 nn.Linear layers capable of handling the shapes of our data\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features and upscales to 5 features.\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1) # the out put layer out put the final res as 1 output.\n",
    "\n",
    "    #3. define a forward() method that outlines the forward pass \n",
    "    def forward(self, x):\n",
    "        return self.layer_2(self.layer_1(x)) # x -> layer_1 -> layer_2 -> output\n",
    "\n",
    "model_0 = CircleModelV0().to(device)\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b4a488c1-1bb7-4fff-ae59-f2d564f7ce21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model_0.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a94632a-1539-4c5c-b945-c4591627e680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=2, out_features=7, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=7, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#let's replicate the model a \n",
    "model_0 = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=7),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=7, out_features=1)\n",
    ").to(device)\n",
    "\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11183fd0-eb8e-47fc-88db-9c4ddc88373c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('0.weight',\n",
       "              tensor([[-0.0705, -0.0059],\n",
       "                      [-0.1015,  0.0824],\n",
       "                      [ 0.3264, -0.6324],\n",
       "                      [-0.5676,  0.3671],\n",
       "                      [-0.2123,  0.5385],\n",
       "                      [-0.3326,  0.1397],\n",
       "                      [-0.6809, -0.5694]], device='cuda:0')),\n",
       "             ('0.bias',\n",
       "              tensor([-0.5763,  0.0275, -0.3868, -0.5820,  0.3503, -0.0036,  0.2556],\n",
       "                     device='cuda:0')),\n",
       "             ('2.weight',\n",
       "              tensor([[ 0.1779, -0.1933,  0.1380,  0.2553,  0.1412,  0.3132, -0.3193]],\n",
       "                     device='cuda:0')),\n",
       "             ('2.bias', tensor([-0.0179], device='cuda:0'))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16697e85-416b-448f-839c-7f56859eda83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of predictions: 200, shape: torch.Size([200, 1])\n",
      "Length of test samples: 200, Shape: torch.Size([200, 2])\n",
      "\n",
      "First 10 predictions: \n",
      "tensor([[ 0.0999],\n",
      "        [ 0.1231],\n",
      "        [-0.2186],\n",
      "        [ 0.1582],\n",
      "        [-0.1175],\n",
      "        [-0.0409],\n",
      "        [ 0.0477],\n",
      "        [ 0.0160],\n",
      "        [-0.2178],\n",
      "        [ 0.1201]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "\n",
      "First 1- labels: \n",
      "tensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 0.])\n"
     ]
    }
   ],
   "source": [
    "# make some predictions \n",
    "untrained_preds = model_0(X_test.to(device))\n",
    "print(f\"Length of predictions: {len(untrained_preds)}, shape: {untrained_preds.shape}\")\n",
    "print(f\"Length of test samples: {len(X_test)}, Shape: {X_test.shape}\")\n",
    "print(f\"\\nFirst 10 predictions: \\n{untrained_preds[:10]}\")\n",
    "print(f\"\\nFirst 1- labels: \\n{y_test[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a629079-bb0b-47f1-bccf-7a3e7437da12",
   "metadata": {},
   "source": [
    "### 2.1 set up loss function and optimizer\n",
    "Which loss function or optimizer should you use?\n",
    "\n",
    "Again... this is problem specific. \n",
    "\n",
    "For example for regression you might want MAE pr MSE (Mean absolute error or mean squared error).\n",
    "\n",
    "For classification you might want binary cross entropy or categorical cross entropy(cross entropy)\n",
    "\n",
    "As a reminder, the loss function measures how *wrong* your models predictions are. \n",
    "\n",
    "and for optimizers, two of the most common and useful are SGD and Adam, however pytorch has many built-in options.\n",
    "\n",
    "* For the loss function we're going to use `torch.nn.BECWithLogitsLoss()`, for more on what binary cross entropy (BCE) is.\n",
    "* for different optimizers see `torch.optim`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3be74688-139e-451c-92b9-2e7d83dc4b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET UP the loss function\n",
    "## loss_fn = nn.BCELoss() # BCELoss = requires inputs to have gone through the sigmoid activation function prior to input to BCELoss\n",
    "loss_fn = nn.BCEWithLogitsLoss()  ## sigmoid activation function built-in\n",
    "## BCEWithLogitsLoss is smth like\n",
    "## nn.Sequential(\n",
    "##   nn.Sigmoid(),\n",
    "##   nn.BCELoss()\n",
    "## )\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)  ## SGD: stochastic gradient descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "23e24951-dbc7-45fc-9318-6ad2bf0a5161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy at the same time     [ Accuracy = (True Positive) / (True Positive + True Negative) * 100 ]\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ba9e4-6c4d-46dd-a168-f8c5a41a057f",
   "metadata": {},
   "source": [
    "# 3. Train the model\n",
    "- 1. Forward pass \n",
    "- 2. Calculate the loss \n",
    "- 3. Zero gradients\n",
    "- 4. Perform backpropagation on the loss\n",
    "- 5. step the optimizer (gradient descent) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a069220-5c85-4de0-8c14-1a2e400b8bba",
   "metadata": {},
   "source": [
    "## 3.1 Going from raw logits -> prediction probabilities -> prediction labels\n",
    "\n",
    "Our Model outputs are going to be raw logits. \n",
    "\n",
    "We can convert these logits into prediction probabilities by passing them to some kind of activation function(sigmoid for binary classification and softmax for multiclass classification)\n",
    "\n",
    "then we can convert our model's prediction probabilities to prediction labels by either rounding them or taking the argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab8e0220-8719-4b3e-bb23-4f2fb83cf795",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0999],\n",
       "        [ 0.1231],\n",
       "        [-0.2186],\n",
       "        [ 0.1582],\n",
       "        [-0.1175]], device='cuda:0', grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_logits = model_0(X_test.to(device))[:5]\n",
    "y_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82f85fee-f080-4d2d-8da5-bdd4f2f81827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5250],\n",
       "        [0.5307],\n",
       "        [0.4456],\n",
       "        [0.5395],\n",
       "        [0.4707]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## for our prediction probaility valuesm, we need to perform a range-style rounding on them: \n",
    "# * `y_pred_probs >= 0.5, y=1 ( class 1 )`\n",
    "# * `y_pred_probs < 0.5, y=0 ( class 0 )`\n",
    "\n",
    "y_pred_probs = torch.sigmoid(y_logits)\n",
    "y_pred_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "126c1b55-7175-47ba-90c6-99285b9c45a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.],\n",
       "        [1.],\n",
       "        [0.],\n",
       "        [1.],\n",
       "        [0.]], device='cuda:0', grad_fn=<RoundBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_preds = torch.round(y_pred_probs)\n",
    "y_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c618dc6f-7e10-4973-9208-24c684405b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([True, True, True, True, True], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0., 1., 0.], device='cuda:0', grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in some what y preds we will call it y_pred_labels\n",
    "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n",
    "## check for the quility \n",
    "print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n",
    "\n",
    "# squeeze is use for get rid of the extra dimension\n",
    "y_pred_labels.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750b9ab8-3aa3-4f2b-820d-250ac3127ad2",
   "metadata": {},
   "source": [
    "#### 3.2 Building a training  and testing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8b53cd8-db73-4527-82bc-72c7cdc2b772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Loss: 0.69459, Acc: 48.12% | Test Loss: 0.70286, Test acc: 44.00%\n",
      "Epoch: 100 | Loss: 0.69071, Acc: 51.00% | Test Loss: 0.69866, Test acc: 47.50%\n",
      "Epoch: 200 | Loss: 0.68837, Acc: 51.25% | Test Loss: 0.69658, Test acc: 46.50%\n",
      "Epoch: 300 | Loss: 0.68598, Acc: 52.25% | Test Loss: 0.69465, Test acc: 47.00%\n",
      "Epoch: 400 | Loss: 0.68308, Acc: 53.12% | Test Loss: 0.69224, Test acc: 47.00%\n",
      "Epoch: 500 | Loss: 0.67920, Acc: 53.87% | Test Loss: 0.68898, Test acc: 48.50%\n",
      "Epoch: 600 | Loss: 0.67369, Acc: 60.62% | Test Loss: 0.68435, Test acc: 56.00%\n",
      "Epoch: 700 | Loss: 0.66726, Acc: 67.62% | Test Loss: 0.67872, Test acc: 63.00%\n",
      "Epoch: 800 | Loss: 0.65972, Acc: 69.88% | Test Loss: 0.67225, Test acc: 64.50%\n",
      "Epoch: 900 | Loss: 0.65078, Acc: 71.75% | Test Loss: 0.66413, Test acc: 66.00%\n",
      "Epoch: 1000 | Loss: 0.64001, Acc: 74.88% | Test Loss: 0.65478, Test acc: 69.00%\n",
      "Epoch: 1100 | Loss: 0.62726, Acc: 77.62% | Test Loss: 0.64452, Test acc: 71.50%\n",
      "Epoch: 1200 | Loss: 0.61217, Acc: 81.50% | Test Loss: 0.63263, Test acc: 74.50%\n",
      "Epoch: 1300 | Loss: 0.59518, Acc: 84.38% | Test Loss: 0.61833, Test acc: 77.50%\n",
      "Epoch: 1400 | Loss: 0.57628, Acc: 85.62% | Test Loss: 0.60270, Test acc: 78.50%\n",
      "Epoch: 1500 | Loss: 0.55546, Acc: 88.12% | Test Loss: 0.58543, Test acc: 80.00%\n",
      "Epoch: 1600 | Loss: 0.53307, Acc: 88.50% | Test Loss: 0.56640, Test acc: 82.00%\n",
      "Epoch: 1700 | Loss: 0.50919, Acc: 90.00% | Test Loss: 0.54588, Test acc: 85.50%\n",
      "Epoch: 1800 | Loss: 0.48402, Acc: 91.38% | Test Loss: 0.52354, Test acc: 86.00%\n",
      "Epoch: 1900 | Loss: 0.45772, Acc: 92.50% | Test Loss: 0.49957, Test acc: 88.50%\n",
      "Epoch: 2000 | Loss: 0.43060, Acc: 93.12% | Test Loss: 0.47476, Test acc: 90.00%\n",
      "Epoch: 2100 | Loss: 0.40342, Acc: 94.12% | Test Loss: 0.44929, Test acc: 92.00%\n",
      "Epoch: 2200 | Loss: 0.37658, Acc: 94.75% | Test Loss: 0.42481, Test acc: 93.00%\n",
      "Epoch: 2300 | Loss: 0.35094, Acc: 95.38% | Test Loss: 0.40104, Test acc: 93.00%\n",
      "Epoch: 2400 | Loss: 0.32586, Acc: 96.62% | Test Loss: 0.37594, Test acc: 95.00%\n",
      "Epoch: 2500 | Loss: 0.30204, Acc: 97.25% | Test Loss: 0.35176, Test acc: 95.50%\n",
      "Epoch: 2600 | Loss: 0.27895, Acc: 97.88% | Test Loss: 0.32940, Test acc: 95.00%\n",
      "Epoch: 2700 | Loss: 0.25728, Acc: 98.25% | Test Loss: 0.30871, Test acc: 96.00%\n",
      "Epoch: 2800 | Loss: 0.23760, Acc: 98.38% | Test Loss: 0.28982, Test acc: 96.00%\n",
      "Epoch: 2900 | Loss: 0.21916, Acc: 98.62% | Test Loss: 0.27214, Test acc: 97.00%\n",
      "Epoch: 3000 | Loss: 0.20278, Acc: 98.75% | Test Loss: 0.25667, Test acc: 97.50%\n",
      "Epoch: 3100 | Loss: 0.18806, Acc: 98.75% | Test Loss: 0.24381, Test acc: 97.50%\n",
      "Epoch: 3200 | Loss: 0.17466, Acc: 98.62% | Test Loss: 0.23204, Test acc: 97.50%\n",
      "Epoch: 3300 | Loss: 0.16241, Acc: 98.88% | Test Loss: 0.21988, Test acc: 97.00%\n",
      "Epoch: 3400 | Loss: 0.15137, Acc: 99.00% | Test Loss: 0.20856, Test acc: 97.00%\n",
      "Epoch: 3500 | Loss: 0.14181, Acc: 99.12% | Test Loss: 0.19839, Test acc: 97.00%\n",
      "Epoch: 3600 | Loss: 0.13326, Acc: 99.12% | Test Loss: 0.18867, Test acc: 98.00%\n",
      "Epoch: 3700 | Loss: 0.12551, Acc: 99.25% | Test Loss: 0.18011, Test acc: 98.00%\n",
      "Epoch: 3800 | Loss: 0.11855, Acc: 99.38% | Test Loss: 0.17251, Test acc: 98.00%\n",
      "Epoch: 3900 | Loss: 0.11230, Acc: 99.38% | Test Loss: 0.16535, Test acc: 98.50%\n",
      "Epoch: 4000 | Loss: 0.10661, Acc: 99.38% | Test Loss: 0.15883, Test acc: 98.50%\n",
      "Epoch: 4100 | Loss: 0.10136, Acc: 99.38% | Test Loss: 0.15317, Test acc: 98.50%\n",
      "Epoch: 4200 | Loss: 0.09661, Acc: 99.50% | Test Loss: 0.14779, Test acc: 98.50%\n",
      "Epoch: 4300 | Loss: 0.09225, Acc: 99.50% | Test Loss: 0.14280, Test acc: 98.50%\n",
      "Epoch: 4400 | Loss: 0.08817, Acc: 99.50% | Test Loss: 0.13766, Test acc: 98.50%\n",
      "Epoch: 4500 | Loss: 0.08446, Acc: 99.50% | Test Loss: 0.13313, Test acc: 98.50%\n",
      "Epoch: 4600 | Loss: 0.08097, Acc: 99.50% | Test Loss: 0.12924, Test acc: 98.50%\n",
      "Epoch: 4700 | Loss: 0.07776, Acc: 99.50% | Test Loss: 0.12551, Test acc: 98.50%\n",
      "Epoch: 4800 | Loss: 0.07478, Acc: 99.50% | Test Loss: 0.12154, Test acc: 98.50%\n",
      "Epoch: 4900 | Loss: 0.07198, Acc: 99.50% | Test Loss: 0.11772, Test acc: 98.50%\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "## torch.cuda.manual_seed(42)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 5000\n",
    "\n",
    "# Put data to target device\n",
    "X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "X_test, y_test = X_test.to(device), y_test.to(device)\n",
    "\n",
    "## Build training and evaluation loop\n",
    "for epoch in range(epochs):\n",
    "    ### Training\n",
    "    model_0.train()\n",
    "    # 1. Forward pass\n",
    "    y_logits = model_0(X_train).squeeze()\n",
    "    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -> pred probs -> pred labels\n",
    "    # 2. Calculate loss/accuracy\n",
    "    loss = loss_fn(y_logits, y_train) # nn.BCEWithLogitsLoss expects raw logits as input --------- so we dont have to wrap the logits with the sigmoid func\n",
    "    acc = accuracy_fn(y_true = y_train, y_pred = y_pred)\n",
    "    # 3. optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "    # 4. loss back ward \n",
    "    loss.backward()\n",
    "\n",
    "    #5. Optimizer step(Gradient descent)\n",
    "    optimizer.step()\n",
    "\n",
    "    ### Testing\n",
    "    model_0.eval()\n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass \n",
    "        test_logits = model_0(X_test).squeeze()\n",
    "        test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "\n",
    "        # 2. Calculate test loss/ acc\n",
    "        test_loss = loss_fn(test_logits, y_test) \n",
    "        test_acc = accuracy_fn(y_true = y_test, y_pred = test_pred)\n",
    "\n",
    "    #Print out what's happen\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e7dddc1e-811d-4516-bcaf-950a50f680f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## why the model is doesnt seems like is studying anything, --- \"Visualize\"!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cf5c6379-9086-4e04-a202-7720f2314c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "helper_functions.py already exit, skipping download\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "# Download helper functions from learn Pytorch repo (if it's not already downloaded)\n",
    "if Path(\"helper_functions.py\").is_file():\n",
    "    print(\"helper_functions.py already exit, skipping download\")\n",
    "else: \n",
    "    print(\"Downloading helper_functions.py\")\n",
    "    request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "    with open(\"helper_functions.py\", \"wb\") as f:\n",
    "        f.write(request.content)\n",
    "\n",
    "from helper_functions import plot_predictions, plot_decision_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9947b368-a982-4bf1-abb7-abb091f08b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize(12, 6))\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.title(\"Train\")\n",
    "# plot_decision_boundary(model_0, X_train, y_train)\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.title(\"Test\")\n",
    "# plot_decision_boundary(model_0, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cbca21-8856-4fa0-8f83-0ac8dc66dcbe",
   "metadata": {},
   "source": [
    "## 5. how to improing a model (from model perspective)\n",
    "* add more layers - give the model more chance to learn about patterns in the data\n",
    "* add more hidden units - go from 5 hidden units to 10 hidden units\n",
    "* fit for longer\n",
    "* changing the activation functions\n",
    "* change the learning rate\n",
    "* change the loss function\n",
    "* \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb319cc-61fe-4ccb-b079-9f1c34b122ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
